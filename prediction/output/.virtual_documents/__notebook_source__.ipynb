# ============================================================
# COMPREHENSIVE CROP RECOMMENDATION SYSTEM WITH NOVEL RANDOM FOREST VARIANTS
# ============================================================

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# Data Visualization
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import cm

# Machine Learning Models
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans
from sklearn.base import clone, BaseEstimator, ClassifierMixin
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, confusion_matrix, classification_report)
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted

# Additional utilities
import time
from tqdm import tqdm
import random
from collections import defaultdict

# Set random seeds for reproducibility
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# ============================================================
# 1. CUSTOM RANDOM FOREST VARIANTS
# ============================================================

print("="*80)
print("CUSTOM RANDOM FOREST VARIANTS - IMPLEMENTATION")
print("="*80)

# ------------------------------------------------------------
# 1.1 CASCADE RANDOM FOREST
# ------------------------------------------------------------
class CascadeRandomForest(ClassifierMixin, BaseEstimator):
    """
    Cascade Random Forest: Trees arranged in cascade where each layer 
    focuses on instances misclassified by previous layer
    """
    
    def __init__(self, n_layers=3, n_estimators_per_layer=50, 
                 max_depth=15, min_samples_split=5, random_state=42):
        self.n_layers = n_layers
        self.n_estimators_per_layer = n_estimators_per_layer
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.random_state = random_state
        self.layers = []
        self.feature_importances_ = None
        
    def fit(self, X, y):
        X, y = check_X_y(X, y)
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        
        # Initialize feature importances
        n_features = X.shape[1]
        self.feature_importances_ = np.zeros(n_features)
        
        # First layer: Train on all data
        print(f"\n[Layer 1] Training Random Forest on all data...")
        rf_layer1 = RandomForestClassifier(
            n_estimators=self.n_estimators_per_layer,
            max_depth=self.max_depth,
            min_samples_split=self.min_samples_split,
            random_state=self.random_state,
            n_jobs=-1
        )
        rf_layer1.fit(X, y)
        self.layers.append(rf_layer1)
        
        # Update feature importances
        self.feature_importances_ += rf_layer1.feature_importances_
        
        # Get predictions from first layer
        y_pred_layer1 = rf_layer1.predict(X)
        
        # Identify misclassified instances
        misclassified_mask = (y_pred_layer1 != y)
        
        if misclassified_mask.sum() == 0:
            print("‚úì All instances correctly classified in first layer!")
            return self
        
        X_misclassified = X[misclassified_mask]
        y_misclassified = y[misclassified_mask]
        
        print(f"  Misclassified instances: {len(X_misclassified)}/{len(X)} "
              f"({len(X_misclassified)/len(X)*100:.1f}%)")
        
        # Subsequent layers: Focus on misclassified instances
        for layer_idx in range(1, self.n_layers):
            if len(X_misclassified) < 10:  # Stop if too few instances
                print(f"\nStopping early: Only {len(X_misclassified)} misclassified instances remain")
                break
            
            print(f"\n[Layer {layer_idx+1}] Training Random Forest on misclassified instances...")
            print(f"  Training on {len(X_misclassified)} instances")
            
            # Train new RF on misclassified instances
            rf_layer = RandomForestClassifier(
                n_estimators=self.n_estimators_per_layer,
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                random_state=self.random_state + layer_idx,
                n_jobs=-1
            )
            rf_layer.fit(X_misclassified, y_misclassified)
            self.layers.append(rf_layer)
            
            # Update feature importances
            self.feature_importances_ += rf_layer.feature_importances_
            
            # Get predictions for current misclassified set
            y_pred_layer = rf_layer.predict(X_misclassified)
            
            # Update misclassified mask
            new_misclassified_mask = (y_pred_layer != y_misclassified)
            
            if new_misclassified_mask.sum() == 0:
                print(f"‚úì All remaining instances correctly classified!")
                break
            
            X_misclassified = X_misclassified[new_misclassified_mask]
            y_misclassified = y_misclassified[new_misclassified_mask]
            
            print(f"  Still misclassified: {len(X_misclassified)} instances")
        
        # Normalize feature importances
        self.feature_importances_ /= len(self.layers)
        
        return self
    
    def predict_proba(self, X):
        check_is_fitted(self)
        X = check_array(X)
        
        # Initialize probability matrix
        proba = np.zeros((X.shape[0], self.n_classes_))
        
        # Get predictions from all layers with adaptive exponential weights
        total_weight = sum([2.0 ** (len(self.layers) - i - 1) for i in range(len(self.layers))])
        for i, layer in enumerate(self.layers):
            # Exponential weighting: first layer gets most weight
            layer_weight = (2.0 ** (len(self.layers) - i - 1)) / total_weight
            layer_proba = layer.predict_proba(X)
            
            # Ensure consistent class ordering
            layer_proba_aligned = np.zeros_like(proba)
            for cls_idx, cls in enumerate(self.classes_):
                if cls in layer.classes_:
                    class_idx_in_layer = np.where(layer.classes_ == cls)[0][0]
                    layer_proba_aligned[:, cls_idx] = layer_proba[:, class_idx_in_layer]
            
            proba += layer_weight * layer_proba_aligned
        
        # Normalize probabilities
        proba_sum = proba.sum(axis=1, keepdims=True)
        proba_sum[proba_sum == 0] = 1  # Avoid division by zero
        proba = proba / proba_sum
        
        return proba
    
    def predict(self, X):
        proba = self.predict_proba(X)
        return self.classes_[np.argmax(proba, axis=1)]

# ------------------------------------------------------------
# 1.2 HIERARCHICAL RANDOM FOREST
# ------------------------------------------------------------
class HierarchicalRandomForest(ClassifierMixin, BaseEstimator):
    """
    Hierarchical Random Forest: First layer clusters data, 
    second layer builds specialized forests per cluster
    """
    
    def __init__(self, n_clusters=3, n_estimators_global=50, 
                 n_estimators_local=30, max_depth=12, random_state=42):
        self.n_clusters = n_clusters
        self.n_estimators_global = n_estimators_global
        self.n_estimators_local = n_estimators_local
        self.max_depth = max_depth
        self.random_state = random_state
        self.global_rf = None
        self.cluster_models = {}
        self.kmeans = None
        self.feature_importances_ = None
        
    def fit(self, X, y):
        X, y = check_X_y(X, y)
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        
        print(f"\n[Hierarchical RF] Training with {self.n_clusters} clusters...")
        
        # Step 1: Cluster the feature space
        print("  Step 1: Clustering feature space...")
        self.kmeans = KMeans(
            n_clusters=self.n_clusters,
            random_state=self.random_state,
            n_init=10
        )
        clusters = self.kmeans.fit_predict(X)
        
        print(f"  Cluster distribution: {np.bincount(clusters)}")
        
        # Step 2: Train global Random Forest on all data
        print("  Step 2: Training global Random Forest...")
        self.global_rf = RandomForestClassifier(
            n_estimators=self.n_estimators_global,
            max_depth=self.max_depth,
            random_state=self.random_state,
            n_jobs=-1
        )
        self.global_rf.fit(X, y)
        
        # Initialize feature importances from global model
        self.feature_importances_ = self.global_rf.feature_importances_.copy()
        
        # Step 3: Train specialized Random Forest for each cluster
        print("  Step 3: Training specialized forests per cluster...")
        for cluster_id in range(self.n_clusters):
            cluster_mask = (clusters == cluster_id)
            
            if cluster_mask.sum() < 10:  # Skip if too few samples
                print(f"    Cluster {cluster_id}: Skipped (only {cluster_mask.sum()} samples)")
                continue
            
            X_cluster = X[cluster_mask]
            y_cluster = y[cluster_mask]
            
            print(f"    Cluster {cluster_id}: Training on {len(X_cluster)} samples "
                  f"({len(np.unique(y_cluster))} classes)")
            
            # Train specialized RF for this cluster
            cluster_rf = RandomForestClassifier(
                n_estimators=self.n_estimators_local,
                max_depth=self.max_depth,
                random_state=self.random_state + cluster_id,
                n_jobs=-1
            )
            cluster_rf.fit(X_cluster, y_cluster)
            self.cluster_models[cluster_id] = cluster_rf
            
            # Aggregate feature importances
            self.feature_importances_ += cluster_rf.feature_importances_
        
        # Average feature importances
        self.feature_importances_ /= (1 + len(self.cluster_models))
        
        return self
    
    def predict_proba(self, X):
        check_is_fitted(self)
        X = check_array(X)
        
        # Get cluster assignments
        clusters = self.kmeans.predict(X)
        
        # Initialize probability matrix
        proba = np.zeros((X.shape[0], self.n_classes_))
        
        # Global predictions
        global_proba = self.global_rf.predict_proba(X)
        
        # Align global probabilities with class ordering
        global_proba_aligned = np.zeros_like(proba)
        for cls_idx, cls in enumerate(self.classes_):
            if cls in self.global_rf.classes_:
                class_idx_in_global = np.where(self.global_rf.classes_ == cls)[0][0]
                global_proba_aligned[:, cls_idx] = global_proba[:, class_idx_in_global]
        
        # Weight for global vs local predictions (optimized for better accuracy)
        global_weight = 0.25  # 25% weight to global predictions
        local_weight = 0.75   # 75% weight to local predictions
        
        proba = global_weight * global_proba_aligned
        
        # Add local predictions for each cluster
        for cluster_id, cluster_model in self.cluster_models.items():
            cluster_mask = (clusters == cluster_id)
            
            if cluster_mask.sum() == 0:
                continue
            
            X_cluster = X[cluster_mask]
            cluster_proba = cluster_model.predict_proba(X_cluster)
            
            # Align cluster probabilities with class ordering
            cluster_proba_aligned = np.zeros((len(X_cluster), self.n_classes_))
            for cls_idx, cls in enumerate(self.classes_):
                if cls in cluster_model.classes_:
                    class_idx_in_cluster = np.where(cluster_model.classes_ == cls)[0][0]
                    cluster_proba_aligned[:, cls_idx] = cluster_proba[:, class_idx_in_cluster]
            
            proba[cluster_mask] += local_weight * cluster_proba_aligned
        
        # Normalize probabilities
        proba_sum = proba.sum(axis=1, keepdims=True)
        proba_sum[proba_sum == 0] = 1
        proba = proba / proba_sum
        
        return proba
    
    def predict(self, X):
        proba = self.predict_proba(X)
        return self.classes_[np.argmax(proba, axis=1)]

# ------------------------------------------------------------
# 1.3 TEMPORAL RANDOM FOREST (Season-aware)
# ------------------------------------------------------------
class TemporalRandomForest(ClassifierMixin, BaseEstimator):
    """
    Temporal Random Forest: Incorporates temporal/seasonal patterns
    Creates specialized forests for different seasons/time periods
    """
    
    def __init__(self, temporal_feature='Season', n_estimators_base=50, 
                 n_estimators_temporal=30, max_depth=12, random_state=42):
        self.temporal_feature = temporal_feature
        self.n_estimators_base = n_estimators_base
        self.n_estimators_temporal = n_estimators_temporal
        self.max_depth = max_depth
        self.random_state = random_state
        self.base_rf = None
        self.temporal_models = {}
        self.temporal_categories_ = None
        self.feature_importances_ = None
        
    def fit(self, X, y, temporal_data=None):
        """
        temporal_data: Series or array containing temporal information
                      (e.g., season, month, time period)
        """
        X, y = check_X_y(X, y)
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        
        # Use provided temporal data or extract from X if column exists
        if temporal_data is None:
            # Check if temporal feature is in column names
            if hasattr(X, 'columns') and self.temporal_feature in X.columns:
                temporal_data = X[self.temporal_feature]
                X = X.drop(columns=[self.temporal_feature])
            else:
                # If no temporal data provided, fall back to base RF
                print("Warning: No temporal data provided. Using base Random Forest.")
                self.base_rf = RandomForestClassifier(
                    n_estimators=self.n_estimators_base,
                    max_depth=self.max_depth,
                    random_state=self.random_state,
                    n_jobs=-1
                )
                self.base_rf.fit(X, y)
                self.feature_importances_ = self.base_rf.feature_importances_
                return self
        
        # Convert temporal data to array if needed
        if hasattr(temporal_data, 'values'):
            temporal_data = temporal_data.values
        
        print(f"\n[Temporal RF] Training with temporal feature '{self.temporal_feature}'...")
        
        # Get unique temporal categories
        self.temporal_categories_ = np.unique(temporal_data)
        print(f"  Temporal categories: {self.temporal_categories_}")
        
        # Step 1: Train base Random Forest on all data
        print("  Step 1: Training base Random Forest...")
        self.base_rf = RandomForestClassifier(
            n_estimators=self.n_estimators_base,
            max_depth=self.max_depth,
            random_state=self.random_state,
            n_jobs=-1
        )
        self.base_rf.fit(X, y)
        
        # Initialize feature importances
        self.feature_importances_ = self.base_rf.feature_importances_.copy()
        
        # Step 2: Train specialized Random Forest for each temporal category
        print("  Step 2: Training specialized forests per temporal category...")
        for category in self.temporal_categories_:
            category_mask = (temporal_data == category)
            
            if category_mask.sum() < 10:  # Skip if too few samples
                print(f"    {self.temporal_feature} '{category}': "
                      f"Skipped (only {category_mask.sum()} samples)")
                continue
            
            X_category = X[category_mask]
            y_category = y[category_mask]
            
            print(f"    {self.temporal_feature} '{category}': "
                  f"Training on {len(X_category)} samples "
                  f"({len(np.unique(y_category))} classes)")
            
            # Train specialized RF for this temporal category
            category_rf = RandomForestClassifier(
                n_estimators=self.n_estimators_temporal,
                max_depth=self.max_depth,
                random_state=self.random_state + hash(category) % 1000,
                n_jobs=-1
            )
            category_rf.fit(X_category, y_category)
            self.temporal_models[category] = category_rf
            
            # Aggregate feature importances
            self.feature_importances_ += category_rf.feature_importances_
        
        # Average feature importances
        self.feature_importances_ /= (1 + len(self.temporal_models))
        
        return self
    
    def predict_proba(self, X, temporal_data=None):
        check_is_fitted(self)
        X = check_array(X)
        
        # If no temporal models trained (fallback case)
        if not self.temporal_models:
            return self.base_rf.predict_proba(X)
        
        # Get temporal data for predictions
        if temporal_data is None:
            if hasattr(X, 'columns') and self.temporal_feature in X.columns:
                temporal_data = X[self.temporal_feature].values
                X = X.drop(columns=[self.temporal_feature])
            else:
                # If no temporal data for prediction, use base model only
                return self.base_rf.predict_proba(X)
        
        # Initialize probability matrix
        proba = np.zeros((X.shape[0], self.n_classes_))
        
        # Base predictions
        base_proba = self.base_rf.predict_proba(X)
        
        # Align base probabilities with class ordering
        base_proba_aligned = np.zeros_like(proba)
        for cls_idx, cls in enumerate(self.classes_):
            if cls in self.base_rf.classes_:
                class_idx_in_base = np.where(self.base_rf.classes_ == cls)[0][0]
                base_proba_aligned[:, cls_idx] = base_proba[:, class_idx_in_base]
        
        # Weight for base vs temporal predictions (optimized for better accuracy)
        base_weight = 0.3  # 30% weight to base predictions
        temporal_weight = 0.7  # 70% weight to temporal predictions
        
        proba = base_weight * base_proba_aligned
        
        # Group by temporal categories
        unique_categories = np.unique(temporal_data)
        
        for category in unique_categories:
            category_mask = (temporal_data == category)
            
            if category_mask.sum() == 0 or category not in self.temporal_models:
                continue
            
            X_category = X[category_mask]
            category_model = self.temporal_models[category]
            
            if len(X_category) == 0:
                continue
            
            category_proba = category_model.predict_proba(X_category)
            
            # Align category probabilities with class ordering
            category_proba_aligned = np.zeros((len(X_category), self.n_classes_))
            for cls_idx, cls in enumerate(self.classes_):
                if cls in category_model.classes_:
                    class_idx_in_category = np.where(category_model.classes_ == cls)[0][0]
                    category_proba_aligned[:, cls_idx] = category_proba[:, class_idx_in_category]
            
            proba[category_mask] += temporal_weight * category_proba_aligned
        
        # Normalize probabilities
        proba_sum = proba.sum(axis=1, keepdims=True)
        proba_sum[proba_sum == 0] = 1
        proba = proba / proba_sum
        
        return proba
    
    def predict(self, X, temporal_data=None):
        proba = self.predict_proba(X, temporal_data)
        return self.classes_[np.argmax(proba, axis=1)]

# ============================================================
# 2. DATA LOADING AND PREPROCESSING
# ============================================================

print("\n" + "="*80)
print("[2] DATA LOADING AND PREPROCESSING")
print("="*80)

# Load dataset
try:
    data = pd.read_csv('/kaggle/input/crop-dataset/crop_dataset.csv')
    print(f"‚úì Dataset loaded successfully")
    print(f"  Shape: {data.shape[0]} rows √ó {data.shape[1]} columns")
    
    # Display basic info
    print("\nDataset info:")
    print(data.info())
    print(f"\nCrop distribution:")
    print(data['Crop Name'].value_counts())
    
except FileNotFoundError:
    # Create synthetic data for testing if file not found
    print("‚úó File not found. Creating synthetic data for testing...")
    n_samples = 1000
    data = pd.DataFrame({
        'Area': np.random.uniform(10000, 500000, n_samples),
        'AP Ratio': np.random.uniform(0.5, 1.5, n_samples),
        'District': np.random.choice(['District_A', 'District_B', 'District_C'], n_samples),
        'Season': np.random.choice(['Kharif', 'Rabi', 'Summer'], n_samples),
        'Avg Temp': np.random.uniform(20, 35, n_samples),
        'Avg Humidity': np.random.uniform(50, 90, n_samples),
        'Max Temp': np.random.uniform(30, 45, n_samples),
        'Min Temp': np.random.uniform(10, 25, n_samples),
        'Max Relative Humidity': np.random.uniform(60, 95, n_samples),
        'Min Relative Humidity': np.random.uniform(40, 80, n_samples),
        'Transplant': np.random.choice(['Jan', 'Feb', 'Mar', 'Apr'], n_samples),
        'Growth': np.random.choice(['Spring', 'Summer', 'Fall'], n_samples),
        'Harvest': np.random.choice(['Winter', 'Spring'], n_samples),
        'Crop Name': np.random.choice(['Rice', 'Wheat', 'Maize', 'Cotton', 'Sugarcane'], n_samples)
    })

# Data preprocessing
print("\n[2.1] Preprocessing data...")

# Clean data - replace Excel error strings with NaN
print("  Cleaning error values...")
error_values = ['#DIV/0!', '#N/A', '#VALUE!', '#REF!', '#NAME?', '#NUM!', '#NULL!']
for col in data.columns:
    if data[col].dtype == 'object':
        # Replace error strings with NaN
        data[col] = data[col].replace(error_values, np.nan)
        # Try to convert to numeric if possible
        try:
            data[col] = pd.to_numeric(data[col], errors='ignore')
        except:
            pass

# Drop rows with missing values
data_cleaned = data.dropna()
if len(data) != len(data_cleaned):
    print(f"  Removed {len(data) - len(data_cleaned)} rows with missing values/errors")
    data = data_cleaned

# Encode categorical variables
label_encoders = {}
categorical_cols = ['District', 'Season', 'Crop Name', 'Transplant', 'Growth', 'Harvest']

for col in categorical_cols:
    if col in data.columns:
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col])
        label_encoders[col] = le
        print(f"  Encoded '{col}' with {len(le.classes_)} classes")

# Feature engineering
print("\n[2.2] Feature engineering...")
if 'Max Temp' in data.columns and 'Min Temp' in data.columns:
    data['Temp_Range'] = data['Max Temp'] - data['Min Temp']
    print("  Added: Temp_Range")

if 'Max Relative Humidity' in data.columns and 'Min Relative Humidity' in data.columns:
    data['Humidity_Range'] = data['Max Relative Humidity'] - data['Min Relative Humidity']
    print("  Added: Humidity_Range")

if 'Avg Temp' in data.columns and 'Avg Humidity' in data.columns:
    data['Temp_Humidity_Index'] = data['Avg Temp'] * data['Avg Humidity'] / 100
    print("  Added: Temp_Humidity_Index")

# Prepare features and target
X = data.drop(['Crop Name'], axis=1)
y = data['Crop Name']

# Store season data before splitting (for Temporal RF)
season_data = None
if 'Season' in data.columns:
    season_data = data['Season'].values.copy()

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data (including season data if available)
if season_data is not None:
    X_train, X_test, y_train, y_test, season_train, season_test = train_test_split(
        X_scaled, y, season_data, test_size=0.2, random_state=SEED, stratify=y
    )
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=SEED, stratify=y
    )
    season_train, season_test = None, None

print(f"\n[2.3] Data split:")
print(f"  Training set: {X_train.shape[0]} samples")
print(f"  Test set: {X_test.shape[0]} samples")
print(f"  Features: {X_train.shape[1]}")
print(f"  Classes: {len(np.unique(y))}")

# ============================================================
# 3. MODEL TRAINING AND EVALUATION
# ============================================================

print("\n" + "="*80)
print("[3] TRAINING AND EVALUATING MODELS")
print("="*80)

# Define models to compare
models = {
    'Standard Random Forest': RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        random_state=SEED,
        n_jobs=-1,
        verbose=0
    ),
    'Cascade Random Forest': CascadeRandomForest(
        n_layers=4,
        n_estimators_per_layer=80,
        max_depth=18,
        random_state=SEED
    ),
    'Hierarchical Random Forest': HierarchicalRandomForest(
        n_clusters=5,
        n_estimators_global=80,
        n_estimators_local=60,
        max_depth=18,
        random_state=SEED
    ),
    'Temporal Random Forest': TemporalRandomForest(
        temporal_feature='Season',
        n_estimators_base=80,
        n_estimators_temporal=70,
        max_depth=18,
        random_state=SEED
    )
}

# For Temporal RF, we need temporal data (already split above)
temporal_data_train = season_train
temporal_data_test = season_test

# Train and evaluate models
results = {}
predictions = {}
training_times = {}

print("\n[3.1] Training models...")
print("-" * 60)

for name, model in tqdm(models.items(), desc="Models"):
    print(f"\n{'='*50}")
    print(f"Training: {name}")
    print(f"{'='*50}")
    
    start_time = time.time()
    
    try:
        # Special handling for Temporal RF
        if name == 'Temporal Random Forest' and temporal_data_train is not None:
            model.fit(X_train, y_train, temporal_data=temporal_data_train)
        else:
            model.fit(X_train, y_train)
        
        train_time = time.time() - start_time
        training_times[name] = train_time
        
        # Make predictions
        if name == 'Temporal Random Forest' and temporal_data_test is not None:
            y_pred = model.predict(X_test, temporal_data=temporal_data_test)
            y_pred_proba = model.predict_proba(X_test, temporal_data=temporal_data_test)
        else:
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)
        
        predictions[name] = (y_pred, y_pred_proba)
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # Cross-validation
        print(f"  Performing cross-validation...")
        
        # Special CV for Temporal RF
        if name == 'Temporal Random Forest':
            # Custom CV that preserves temporal structure
            cv_scores = []
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)
            
            # Use season_data for CV if available
            if season_data is not None:
                for train_idx, val_idx in skf.split(X_scaled, y):
                    X_cv_train, X_cv_val = X_scaled[train_idx], X_scaled[val_idx]
                    y_cv_train, y_cv_val = y.iloc[train_idx], y.iloc[val_idx]
                    
                    # Get temporal data for CV fold
                    temporal_cv_train = season_data[train_idx]
                    temporal_cv_val = season_data[val_idx]
                    
                    # Train and evaluate
                    model_cv = clone(model)
                    model_cv.fit(X_cv_train, y_cv_train, temporal_data=temporal_cv_train)
                    y_cv_pred = model_cv.predict(X_cv_val, temporal_data=temporal_cv_val)
                    
                    cv_scores.append(accuracy_score(y_cv_val, y_cv_pred))
            else:
                # Fallback to standard CV if no season data
                cv_scores = cross_val_score(
                    model, X_scaled, y, cv=3, scoring='accuracy', n_jobs=-1
                )
            
            cv_accuracy_mean = np.mean(cv_scores)
            cv_accuracy_std = np.std(cv_scores)
            
        else:
            # Standard CV for other models
            cv_scores = cross_val_score(
                model, X_scaled, y, cv=3, scoring='accuracy', n_jobs=-1
            )
            cv_accuracy_mean = cv_scores.mean()
            cv_accuracy_std = cv_scores.std()
                
        # Store results
        results[name] = {
            'Test Accuracy': accuracy,
            'Test Precision': precision,
            'Test Recall': recall,
            'Test F1': f1,
            'CV Accuracy Mean': cv_accuracy_mean,
            'CV Accuracy Std': cv_accuracy_std,
            'Training Time': train_time
        }
        
        print(f"  ‚úì Training completed in {train_time:.2f}s")
        print(f"  Test Accuracy: {accuracy:.4f}")
        print(f"  CV Accuracy: {cv_accuracy_mean:.4f} ¬± {cv_accuracy_std:.4f}")
        
        # Print feature importance if available
        if hasattr(model, 'feature_importances_'):
            top_features = np.argsort(model.feature_importances_)[-5:][::-1]
            print(f"  Top 5 features by importance: {top_features}")
        
    except Exception as e:
        print(f"  ‚úó Error training {name}: {str(e)}")
        results[name] = {
            'Test Accuracy': 0,
            'Test Precision': 0,
            'Test Recall': 0,
            'Test F1': 0,
            'CV Accuracy Mean': 0,
            'CV Accuracy Std': 0,
            'Training Time': 0
        }

# ============================================================
# 4. RESULTS COMPARISON
# ============================================================

print("\n" + "="*80)
print("[4] RESULTS COMPARISON")
print("="*80)

# Create results DataFrame
results_df = pd.DataFrame(results).T
results_df = results_df.sort_values('Test Accuracy', ascending=False)

print("\n[4.1] Performance Comparison:")
print("-" * 80)
print(results_df.round(4).to_string())

# Visualize results with research-worthy styling
sns.set_style("whitegrid")
research_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E', '#BC4B51']
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.patch.set_facecolor('white')

# 1. Test Accuracy Comparison
ax1 = axes[0, 0]
models_sorted = results_df.index.tolist()
accuracies = results_df['Test Accuracy'].values
bars1 = ax1.barh(models_sorted, accuracies, color=research_colors[:len(models_sorted)], edgecolor='black', linewidth=1.2)
ax1.set_xlabel('Accuracy', fontsize=12, fontweight='bold')
ax1.set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')
ax1.axvline(x=accuracies.max(), color='#C73E1D', linestyle='--', alpha=0.7, linewidth=2)
ax1.grid(False)
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)

# 2. CV Accuracy with Error Bars
ax2 = axes[0, 1]
cv_means = results_df['CV Accuracy Mean'].values
cv_stds = results_df['CV Accuracy Std'].values
y_pos = np.arange(len(models_sorted))
bars2 = ax2.barh(y_pos, cv_means, xerr=cv_stds, color=research_colors[:len(models_sorted)], 
                 capsize=5, edgecolor='black', linewidth=1.2, ecolor='#333333', error_kw={'linewidth': 2})
ax2.set_yticks(y_pos)
ax2.set_yticklabels(models_sorted)
ax2.set_xlabel('Accuracy', fontsize=12, fontweight='bold')
ax2.set_title('Cross-Validation Accuracy (¬± std)', fontsize=14, fontweight='bold')
ax2.grid(False)
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)

# 3. Training Time Comparison
ax3 = axes[0, 2]
training_times_vals = results_df['Training Time'].values
bars3 = ax3.barh(models_sorted, training_times_vals, color=research_colors[:len(models_sorted)], 
                 edgecolor='black', linewidth=1.2)
ax3.set_xlabel('Time (seconds)', fontsize=12, fontweight='bold')
ax3.set_title('Training Time Comparison', fontsize=14, fontweight='bold')
ax3.grid(False)
ax3.spines['top'].set_visible(False)
ax3.spines['right'].set_visible(False)

# 4. F1-Score Comparison
ax4 = axes[1, 0]
f1_scores = results_df['Test F1'].values
bars4 = ax4.barh(models_sorted, f1_scores, color=research_colors[:len(models_sorted)], 
                 edgecolor='black', linewidth=1.2)
ax4.set_xlabel('F1-Score', fontsize=12, fontweight='bold')
ax4.set_title('Test F1-Score Comparison', fontsize=14, fontweight='bold')
ax4.axvline(x=f1_scores.max(), color='#C73E1D', linestyle='--', alpha=0.7, linewidth=2)
ax4.grid(False)
ax4.spines['top'].set_visible(False)
ax4.spines['right'].set_visible(False)

# 5. Precision-Recall Tradeoff
ax5 = axes[1, 1]
precision = results_df['Test Precision'].values
recall = results_df['Test Recall'].values
for i, model in enumerate(models_sorted):
    ax5.scatter(precision[i], recall[i], s=200, alpha=0.8, 
                color=research_colors[i % len(research_colors)], 
                edgecolor='black', linewidth=1.5, zorder=3)
    ax5.annotate(model, (precision[i], recall[i]), xytext=(8, 8), 
                 textcoords='offset points', fontsize=9, fontweight='bold',
                 bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='gray', alpha=0.8))
ax5.set_xlabel('Precision', fontsize=12, fontweight='bold')
ax5.set_ylabel('Recall', fontsize=12, fontweight='bold')
ax5.set_title('Precision-Recall Tradeoff', fontsize=14, fontweight='bold')
ax5.grid(False)
ax5.spines['top'].set_visible(False)
ax5.spines['right'].set_visible(False)

# 6. Accuracy vs Training Time
ax6 = axes[1, 2]
for i, model in enumerate(models_sorted):
    ax6.scatter(training_times_vals[i], accuracies[i], s=200, alpha=0.8,
                color=research_colors[i % len(research_colors)], 
                edgecolor='black', linewidth=1.5, zorder=3)
    ax6.annotate(model, (training_times_vals[i], accuracies[i]), 
                 xytext=(8, 8), textcoords='offset points', fontsize=9, fontweight='bold',
                 bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='gray', alpha=0.8))
ax6.set_xlabel('Training Time (seconds)', fontsize=12, fontweight='bold')
ax6.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')
ax6.set_title('Accuracy vs Training Time', fontsize=14, fontweight='bold')
ax6.grid(False)
ax6.spines['top'].set_visible(False)
ax6.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig('model_comparison.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# ============================================================
# 5. DETAILED ANALYSIS
# ============================================================

print("\n" + "="*80)
print("[5] DETAILED ANALYSIS")
print("="*80)

# Best model analysis
best_model_name = results_df.index[0]
print(f"\n[5.1] Best Model: {best_model_name}")
print(f"  Test Accuracy: {results_df.loc[best_model_name, 'Test Accuracy']:.4f}")
print(f"  CV Accuracy: {results_df.loc[best_model_name, 'CV Accuracy Mean']:.4f} "
      f"¬± {results_df.loc[best_model_name, 'CV Accuracy Std']:.4f}")
print(f"  F1-Score: {results_df.loc[best_model_name, 'Test F1']:.4f}")
print(f"  Training Time: {results_df.loc[best_model_name, 'Training Time']:.2f}s")

# Improvement over baseline
if 'Standard Random Forest' in results_df.index:
    baseline_acc = results_df.loc['Standard Random Forest', 'Test Accuracy']
    best_acc = results_df.loc[best_model_name, 'Test Accuracy']
    improvement = ((best_acc - baseline_acc) / baseline_acc) * 100
    
    print(f"\n[5.2] Improvement over Standard Random Forest:")
    print(f"  Baseline (Standard RF): {baseline_acc:.4f}")
    print(f"  Best Model ({best_model_name}): {best_acc:.4f}")
    print(f"  Improvement: {improvement:+.2f}%")

# Feature importance analysis
print("\n[5.3] Feature Importance Analysis:")
for model_name in models_sorted:
    if model_name in models and hasattr(models[model_name], 'feature_importances_'):
        importances = models[model_name].feature_importances_
        top_3_idx = np.argsort(importances)[-3:][::-1]
        
        # Get feature names if available
        if hasattr(X, 'columns'):
            feature_names = X.columns.tolist()
            top_features = [feature_names[i] for i in top_3_idx]
        else:
            top_features = top_3_idx.tolist()
        
        print(f"  {model_name}:")
        print(f"    Top 3 features: {top_features}")
        print(f"    Importance values: {importances[top_3_idx].round(4)}")

# Confusion matrix for best model
if best_model_name in predictions:
    print(f"\n[5.4] Confusion Matrix for {best_model_name}:")
    y_pred_best, _ = predictions[best_model_name]
    
    cm = confusion_matrix(y_test, y_pred_best)
    
    # Create custom colormap for research paper
    from matplotlib.colors import LinearSegmentedColormap
    colors_cm = ['#FFFFFF', '#E8F4F8', '#2E86AB']
    n_bins = 100
    cmap_custom = LinearSegmentedColormap.from_list('research', colors_cm, N=n_bins)
    
    plt.figure(figsize=(12, 10), facecolor='white')
    sns.set_style("whitegrid")
    ax = sns.heatmap(cm, annot=True, fmt='d', cmap=cmap_custom, 
                     cbar=True, linewidths=0.5, linecolor='gray',
                     annot_kws={'fontsize': 11, 'fontweight': 'bold'},
                     cbar_kws={'label': 'Count', 'shrink': 0.8})
    ax.set_facecolor('white')
    plt.title(f'Confusion Matrix - {best_model_name}', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('Predicted Label', fontsize=13, fontweight='bold')
    plt.ylabel('True Label', fontsize=13, fontweight='bold')
    ax.grid(False)
    plt.tight_layout()
    plt.savefig('confusion_matrix_best_model.png', dpi=600, bbox_inches='tight', facecolor='white')
    plt.show()
    
    # Classification report
    print("\n  Classification Report:")
    print(classification_report(y_test, y_pred_best))

# ============================================================
# 6. ENSEMBLE OF NOVEL MODELS
# ============================================================

print("\n" + "="*80)
print("[6] ENSEMBLE OF NOVEL MODELS")
print("="*80)

# Create an ensemble of the three novel models
print("\n[6.1] Creating Ensemble of Novel Models...")

# Get predictions from novel models
novel_model_names = ['Cascade Random Forest', 'Hierarchical Random Forest', 'Temporal Random Forest']
novel_predictions = []

for model_name in novel_model_names:
    if model_name in predictions:
        _, y_pred_proba = predictions[model_name]
        novel_predictions.append(y_pred_proba)

if len(novel_predictions) >= 2:
    # Average predictions
    ensemble_proba = np.mean(novel_predictions, axis=0)
    ensemble_pred = np.argmax(ensemble_proba, axis=1)
    
    # Calculate ensemble metrics
    ensemble_accuracy = accuracy_score(y_test, ensemble_pred)
    ensemble_f1 = f1_score(y_test, ensemble_pred, average='weighted', zero_division=0)
    
    print(f"  Ensemble of {len(novel_predictions)} novel models:")
    print(f"    Test Accuracy: {ensemble_accuracy:.4f}")
    print(f"    F1-Score: {ensemble_f1:.4f}")
    
    # Compare with individual models
    print(f"\n  Comparison with individual models:")
    for model_name in novel_model_names:
        if model_name in results_df.index:
            model_acc = results_df.loc[model_name, 'Test Accuracy']
            diff = ensemble_accuracy - model_acc
            print(f"    vs {model_name}: {ensemble_accuracy:.4f} vs {model_acc:.4f} "
                  f"(Œî = {diff:+.4f})")

# ============================================================
# 7. FINAL CONCLUSIONS
# ============================================================

print("\n" + "="*80)
print("[7] FINAL CONCLUSIONS")
print("="*80)

print(f"\nüìä SUMMARY OF NOVEL RANDOM FOREST VARIANTS:")
print(f"   Models tested: {len(models)}")
print(f"   Best performing model: {best_model_name}")
print(f"   Best test accuracy: {results_df['Test Accuracy'].max():.4f}")
print(f"   Most stable model (lowest CV std): "
      f"{results_df['CV Accuracy Std'].idxmin()} "
      f"({results_df['CV Accuracy Std'].min():.4f})")

print(f"\nüîç KEY INSIGHTS:")
print(f"   1. Cascade RF: Focuses on misclassified instances iteratively")
print(f"   2. Hierarchical RF: Creates specialized models for data clusters")
print(f"   3. Temporal RF: Incorporates seasonal/temporal patterns")
print(f"   4. Standard RF: Serves as baseline comparison")

print(f"\nüéØ RECOMMENDATIONS:")
print(f"   1. Use {best_model_name} for deployment")
print(f"   2. Consider ensemble of novel models for robustness")
print(f"   3. Monitor model performance on new data")
print(f"   4. Regularly retrain models with updated data")

print(f"\nüìà PERFORMANCE METRICS:")
for model_name in models_sorted:
    acc = results_df.loc[model_name, 'Test Accuracy']
    cv_mean = results_df.loc[model_name, 'CV Accuracy Mean']
    cv_std = results_df.loc[model_name, 'CV Accuracy Std']
    time_val = results_df.loc[model_name, 'Training Time']
    
    print(f"   {model_name:30s}: "
          f"Test={acc:.4f}, CV={cv_mean:.4f}¬±{cv_std:.4f}, Time={time_val:.1f}s")

print("\n" + "="*80)
print("EXPERIMENT COMPLETE - NOVEL RANDOM FOREST VARIANTS IMPLEMENTED")
print("="*80)


# ============================================================
# COMPREHENSIVE CROP RECOMMENDATION SYSTEM WITH NOVEL RANDOM FOREST VARIANTS
# ============================================================

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# Data Visualization
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import cm

# Machine Learning Models
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans
from sklearn.base import clone, BaseEstimator, ClassifierMixin
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, confusion_matrix, classification_report,
                           roc_curve, auc, roc_auc_score, precision_recall_curve,
                           average_precision_score, matthews_corrcoef, cohen_kappa_score,
                           balanced_accuracy_score, jaccard_score, log_loss)
from sklearn.calibration import calibration_curve
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from sklearn.preprocessing import label_binarize
from itertools import cycle
from scipy import stats
from sklearn.model_selection import cross_validate, RepeatedStratifiedKFold, LeaveOneOut
from sklearn.utils import resample

# Additional utilities
import time
from tqdm import tqdm
import random
from collections import defaultdict

# Set random seeds for reproducibility
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# ============================================================
# 1. CUSTOM RANDOM FOREST VARIANTS
# ============================================================

print("="*80)
print("CUSTOM RANDOM FOREST VARIANTS - IMPLEMENTATION")
print("="*80)

# ------------------------------------------------------------
# 1.1 CASCADE RANDOM FOREST
# ------------------------------------------------------------
class CascadeRandomForest(ClassifierMixin, BaseEstimator):
    """
    Cascade Random Forest: Trees arranged in cascade where each layer 
    focuses on instances misclassified by previous layer
    """
    
    def __init__(self, n_layers=3, n_estimators_per_layer=50, 
                 max_depth=15, min_samples_split=5, random_state=42):
        self.n_layers = n_layers
        self.n_estimators_per_layer = n_estimators_per_layer
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.random_state = random_state
        self.layers = []
        self.feature_importances_ = None
        
    def fit(self, X, y):
        X, y = check_X_y(X, y)
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        
        # Initialize feature importances
        n_features = X.shape[1]
        self.feature_importances_ = np.zeros(n_features)
        
        # First layer: Train on all data
        print(f"\n[Layer 1] Training Random Forest on all data...")
        rf_layer1 = RandomForestClassifier(
            n_estimators=self.n_estimators_per_layer,
            max_depth=self.max_depth,
            min_samples_split=self.min_samples_split,
            random_state=self.random_state,
            n_jobs=-1
        )
        rf_layer1.fit(X, y)
        self.layers.append(rf_layer1)
        
        # Update feature importances
        self.feature_importances_ += rf_layer1.feature_importances_
        
        # Get predictions from first layer
        y_pred_layer1 = rf_layer1.predict(X)
        
        # Identify misclassified instances
        misclassified_mask = (y_pred_layer1 != y)
        
        if misclassified_mask.sum() == 0:
            print("‚úì All instances correctly classified in first layer!")
            return self
        
        X_misclassified = X[misclassified_mask]
        y_misclassified = y[misclassified_mask]
        
        print(f"  Misclassified instances: {len(X_misclassified)}/{len(X)} "
              f"({len(X_misclassified)/len(X)*100:.1f}%)")
        
        # Subsequent layers: Focus on misclassified instances
        for layer_idx in range(1, self.n_layers):
            if len(X_misclassified) < 10:  # Stop if too few instances
                print(f"\nStopping early: Only {len(X_misclassified)} misclassified instances remain")
                break
            
            print(f"\n[Layer {layer_idx+1}] Training Random Forest on misclassified instances...")
            print(f"  Training on {len(X_misclassified)} instances")
            
            # Train new RF on misclassified instances
            rf_layer = RandomForestClassifier(
                n_estimators=self.n_estimators_per_layer,
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                random_state=self.random_state + layer_idx,
                n_jobs=-1
            )
            rf_layer.fit(X_misclassified, y_misclassified)
            self.layers.append(rf_layer)
            
            # Update feature importances
            self.feature_importances_ += rf_layer.feature_importances_
            
            # Get predictions for current misclassified set
            y_pred_layer = rf_layer.predict(X_misclassified)
            
            # Update misclassified mask
            new_misclassified_mask = (y_pred_layer != y_misclassified)
            
            if new_misclassified_mask.sum() == 0:
                print(f"‚úì All remaining instances correctly classified!")
                break
            
            X_misclassified = X_misclassified[new_misclassified_mask]
            y_misclassified = y_misclassified[new_misclassified_mask]
            
            print(f"  Still misclassified: {len(X_misclassified)} instances")
        
        # Normalize feature importances
        self.feature_importances_ /= len(self.layers)
        
        return self
    
    def predict_proba(self, X):
        check_is_fitted(self)
        X = check_array(X)
        
        # Initialize probability matrix
        proba = np.zeros((X.shape[0], self.n_classes_))
        
        # Get predictions from all layers with adaptive exponential weights
        total_weight = sum([2.0 ** (len(self.layers) - i - 1) for i in range(len(self.layers))])
        for i, layer in enumerate(self.layers):
            # Exponential weighting: first layer gets most weight
            layer_weight = (2.0 ** (len(self.layers) - i - 1)) / total_weight
            layer_proba = layer.predict_proba(X)
            
            # Ensure consistent class ordering
            layer_proba_aligned = np.zeros_like(proba)
            for cls_idx, cls in enumerate(self.classes_):
                if cls in layer.classes_:
                    class_idx_in_layer = np.where(layer.classes_ == cls)[0][0]
                    layer_proba_aligned[:, cls_idx] = layer_proba[:, class_idx_in_layer]
            
            proba += layer_weight * layer_proba_aligned
        
        # Normalize probabilities
        proba_sum = proba.sum(axis=1, keepdims=True)
        proba_sum[proba_sum == 0] = 1  # Avoid division by zero
        proba = proba / proba_sum
        
        return proba
    
    def predict(self, X):
        proba = self.predict_proba(X)
        return self.classes_[np.argmax(proba, axis=1)]

# ------------------------------------------------------------
# 1.2 HIERARCHICAL RANDOM FOREST
# ------------------------------------------------------------
class HierarchicalRandomForest(ClassifierMixin, BaseEstimator):
    """
    Hierarchical Random Forest: First layer clusters data, 
    second layer builds specialized forests per cluster
    """
    
    def __init__(self, n_clusters=3, n_estimators_global=50, 
                 n_estimators_local=30, max_depth=12, random_state=42):
        self.n_clusters = n_clusters
        self.n_estimators_global = n_estimators_global
        self.n_estimators_local = n_estimators_local
        self.max_depth = max_depth
        self.random_state = random_state
        self.global_rf = None
        self.cluster_models = {}
        self.kmeans = None
        self.feature_importances_ = None
        
    def fit(self, X, y):
        X, y = check_X_y(X, y)
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        
        print(f"\n[Hierarchical RF] Training with {self.n_clusters} clusters...")
        
        # Step 1: Cluster the feature space
        print("  Step 1: Clustering feature space...")
        self.kmeans = KMeans(
            n_clusters=self.n_clusters,
            random_state=self.random_state,
            n_init=10
        )
        clusters = self.kmeans.fit_predict(X)
        
        print(f"  Cluster distribution: {np.bincount(clusters)}")
        
        # Step 2: Train global Random Forest on all data
        print("  Step 2: Training global Random Forest...")
        self.global_rf = RandomForestClassifier(
            n_estimators=self.n_estimators_global,
            max_depth=self.max_depth,
            random_state=self.random_state,
            n_jobs=-1
        )
        self.global_rf.fit(X, y)
        
        # Initialize feature importances from global model
        self.feature_importances_ = self.global_rf.feature_importances_.copy()
        
        # Step 3: Train specialized Random Forest for each cluster
        print("  Step 3: Training specialized forests per cluster...")
        for cluster_id in range(self.n_clusters):
            cluster_mask = (clusters == cluster_id)
            
            if cluster_mask.sum() < 10:  # Skip if too few samples
                print(f"    Cluster {cluster_id}: Skipped (only {cluster_mask.sum()} samples)")
                continue
            
            X_cluster = X[cluster_mask]
            y_cluster = y[cluster_mask]
            
            print(f"    Cluster {cluster_id}: Training on {len(X_cluster)} samples "
                  f"({len(np.unique(y_cluster))} classes)")
            
            # Train specialized RF for this cluster
            cluster_rf = RandomForestClassifier(
                n_estimators=self.n_estimators_local,
                max_depth=self.max_depth,
                random_state=self.random_state + cluster_id,
                n_jobs=-1
            )
            cluster_rf.fit(X_cluster, y_cluster)
            self.cluster_models[cluster_id] = cluster_rf
            
            # Aggregate feature importances
            self.feature_importances_ += cluster_rf.feature_importances_
        
        # Average feature importances
        self.feature_importances_ /= (1 + len(self.cluster_models))
        
        return self
    
    def predict_proba(self, X):
        check_is_fitted(self)
        X = check_array(X)
        
        # Get cluster assignments
        clusters = self.kmeans.predict(X)
        
        # Initialize probability matrix
        proba = np.zeros((X.shape[0], self.n_classes_))
        
        # Global predictions
        global_proba = self.global_rf.predict_proba(X)
        
        # Align global probabilities with class ordering
        global_proba_aligned = np.zeros_like(proba)
        for cls_idx, cls in enumerate(self.classes_):
            if cls in self.global_rf.classes_:
                class_idx_in_global = np.where(self.global_rf.classes_ == cls)[0][0]
                global_proba_aligned[:, cls_idx] = global_proba[:, class_idx_in_global]
        
        # Weight for global vs local predictions (optimized for better accuracy)
        global_weight = 0.25  # 25% weight to global predictions
        local_weight = 0.75   # 75% weight to local predictions
        
        proba = global_weight * global_proba_aligned
        
        # Add local predictions for each cluster
        for cluster_id, cluster_model in self.cluster_models.items():
            cluster_mask = (clusters == cluster_id)
            
            if cluster_mask.sum() == 0:
                continue
            
            X_cluster = X[cluster_mask]
            cluster_proba = cluster_model.predict_proba(X_cluster)
            
            # Align cluster probabilities with class ordering
            cluster_proba_aligned = np.zeros((len(X_cluster), self.n_classes_))
            for cls_idx, cls in enumerate(self.classes_):
                if cls in cluster_model.classes_:
                    class_idx_in_cluster = np.where(cluster_model.classes_ == cls)[0][0]
                    cluster_proba_aligned[:, cls_idx] = cluster_proba[:, class_idx_in_cluster]
            
            proba[cluster_mask] += local_weight * cluster_proba_aligned
        
        # Normalize probabilities
        proba_sum = proba.sum(axis=1, keepdims=True)
        proba_sum[proba_sum == 0] = 1
        proba = proba / proba_sum
        
        return proba
    
    def predict(self, X):
        proba = self.predict_proba(X)
        return self.classes_[np.argmax(proba, axis=1)]

# ------------------------------------------------------------
# 1.3 TEMPORAL RANDOM FOREST (Season-aware)
# ------------------------------------------------------------
class TemporalRandomForest(ClassifierMixin, BaseEstimator):
    """
    Temporal Random Forest: Incorporates temporal/seasonal patterns
    Creates specialized forests for different seasons/time periods
    """
    
    def __init__(self, temporal_feature='Season', n_estimators_base=50, 
                 n_estimators_temporal=30, max_depth=12, random_state=42):
        self.temporal_feature = temporal_feature
        self.n_estimators_base = n_estimators_base
        self.n_estimators_temporal = n_estimators_temporal
        self.max_depth = max_depth
        self.random_state = random_state
        self.base_rf = None
        self.temporal_models = {}
        self.temporal_categories_ = None
        self.feature_importances_ = None
        
    def fit(self, X, y, temporal_data=None):
        """
        temporal_data: Series or array containing temporal information
                      (e.g., season, month, time period)
        """
        X, y = check_X_y(X, y)
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        
        # Use provided temporal data or extract from X if column exists
        if temporal_data is None:
            # Check if temporal feature is in column names
            if hasattr(X, 'columns') and self.temporal_feature in X.columns:
                temporal_data = X[self.temporal_feature]
                X = X.drop(columns=[self.temporal_feature])
            else:
                # If no temporal data provided, fall back to base RF
                print("Warning: No temporal data provided. Using base Random Forest.")
                self.base_rf = RandomForestClassifier(
                    n_estimators=self.n_estimators_base,
                    max_depth=self.max_depth,
                    random_state=self.random_state,
                    n_jobs=-1
                )
                self.base_rf.fit(X, y)
                self.feature_importances_ = self.base_rf.feature_importances_
                return self
        
        # Convert temporal data to array if needed
        if hasattr(temporal_data, 'values'):
            temporal_data = temporal_data.values
        
        print(f"\n[Temporal RF] Training with temporal feature '{self.temporal_feature}'...")
        
        # Get unique temporal categories
        self.temporal_categories_ = np.unique(temporal_data)
        print(f"  Temporal categories: {self.temporal_categories_}")
        
        # Step 1: Train base Random Forest on all data
        print("  Step 1: Training base Random Forest...")
        self.base_rf = RandomForestClassifier(
            n_estimators=self.n_estimators_base,
            max_depth=self.max_depth,
            random_state=self.random_state,
            n_jobs=-1
        )
        self.base_rf.fit(X, y)
        
        # Initialize feature importances
        self.feature_importances_ = self.base_rf.feature_importances_.copy()
        
        # Step 2: Train specialized Random Forest for each temporal category
        print("  Step 2: Training specialized forests per temporal category...")
        for category in self.temporal_categories_:
            category_mask = (temporal_data == category)
            
            if category_mask.sum() < 10:  # Skip if too few samples
                print(f"    {self.temporal_feature} '{category}': "
                      f"Skipped (only {category_mask.sum()} samples)")
                continue
            
            X_category = X[category_mask]
            y_category = y[category_mask]
            
            print(f"    {self.temporal_feature} '{category}': "
                  f"Training on {len(X_category)} samples "
                  f"({len(np.unique(y_category))} classes)")
            
            # Train specialized RF for this temporal category
            category_rf = RandomForestClassifier(
                n_estimators=self.n_estimators_temporal,
                max_depth=self.max_depth,
                random_state=self.random_state + hash(category) % 1000,
                n_jobs=-1
            )
            category_rf.fit(X_category, y_category)
            self.temporal_models[category] = category_rf
            
            # Aggregate feature importances
            self.feature_importances_ += category_rf.feature_importances_
        
        # Average feature importances
        self.feature_importances_ /= (1 + len(self.temporal_models))
        
        return self
    
    def predict_proba(self, X, temporal_data=None):
        check_is_fitted(self)
        X = check_array(X)
        
        # If no temporal models trained (fallback case)
        if not self.temporal_models:
            return self.base_rf.predict_proba(X)
        
        # Get temporal data for predictions
        if temporal_data is None:
            if hasattr(X, 'columns') and self.temporal_feature in X.columns:
                temporal_data = X[self.temporal_feature].values
                X = X.drop(columns=[self.temporal_feature])
            else:
                # If no temporal data for prediction, use base model only
                return self.base_rf.predict_proba(X)
        
        # Initialize probability matrix
        proba = np.zeros((X.shape[0], self.n_classes_))
        
        # Base predictions
        base_proba = self.base_rf.predict_proba(X)
        
        # Align base probabilities with class ordering
        base_proba_aligned = np.zeros_like(proba)
        for cls_idx, cls in enumerate(self.classes_):
            if cls in self.base_rf.classes_:
                class_idx_in_base = np.where(self.base_rf.classes_ == cls)[0][0]
                base_proba_aligned[:, cls_idx] = base_proba[:, class_idx_in_base]
        
        # Weight for base vs temporal predictions (optimized for better accuracy)
        base_weight = 0.3  # 30% weight to base predictions
        temporal_weight = 0.7  # 70% weight to temporal predictions
        
        proba = base_weight * base_proba_aligned
        
        # Group by temporal categories
        unique_categories = np.unique(temporal_data)
        
        for category in unique_categories:
            category_mask = (temporal_data == category)
            
            if category_mask.sum() == 0 or category not in self.temporal_models:
                continue
            
            X_category = X[category_mask]
            category_model = self.temporal_models[category]
            
            if len(X_category) == 0:
                continue
            
            category_proba = category_model.predict_proba(X_category)
            
            # Align category probabilities with class ordering
            category_proba_aligned = np.zeros((len(X_category), self.n_classes_))
            for cls_idx, cls in enumerate(self.classes_):
                if cls in category_model.classes_:
                    class_idx_in_category = np.where(category_model.classes_ == cls)[0][0]
                    category_proba_aligned[:, cls_idx] = category_proba[:, class_idx_in_category]
            
            proba[category_mask] += temporal_weight * category_proba_aligned
        
        # Normalize probabilities
        proba_sum = proba.sum(axis=1, keepdims=True)
        proba_sum[proba_sum == 0] = 1
        proba = proba / proba_sum
        
        return proba
    
    def predict(self, X, temporal_data=None):
        proba = self.predict_proba(X, temporal_data)
        return self.classes_[np.argmax(proba, axis=1)]

# ============================================================
# 2. DATA LOADING AND PREPROCESSING
# ============================================================

print("\n" + "="*80)
print("[2] DATA LOADING AND PREPROCESSING")
print("="*80)

# Load dataset
try:
    data = pd.read_csv('/kaggle/input/crop-dataset/crop_dataset.csv')
    print(f"‚úì Dataset loaded successfully")
    print(f"  Shape: {data.shape[0]} rows √ó {data.shape[1]} columns")
    
    # Display basic info
    print("\nDataset info:")
    print(data.info())
    print(f"\nCrop distribution:")
    print(data['Crop Name'].value_counts())
    
except FileNotFoundError:
    # Create synthetic data for testing if file not found
    print("‚úó File not found. Creating synthetic data for testing...")
    n_samples = 1000
    data = pd.DataFrame({
        'Area': np.random.uniform(10000, 500000, n_samples),
        'AP Ratio': np.random.uniform(0.5, 1.5, n_samples),
        'District': np.random.choice(['District_A', 'District_B', 'District_C'], n_samples),
        'Season': np.random.choice(['Kharif', 'Rabi', 'Summer'], n_samples),
        'Avg Temp': np.random.uniform(20, 35, n_samples),
        'Avg Humidity': np.random.uniform(50, 90, n_samples),
        'Max Temp': np.random.uniform(30, 45, n_samples),
        'Min Temp': np.random.uniform(10, 25, n_samples),
        'Max Relative Humidity': np.random.uniform(60, 95, n_samples),
        'Min Relative Humidity': np.random.uniform(40, 80, n_samples),
        'Transplant': np.random.choice(['Jan', 'Feb', 'Mar', 'Apr'], n_samples),
        'Growth': np.random.choice(['Spring', 'Summer', 'Fall'], n_samples),
        'Harvest': np.random.choice(['Winter', 'Spring'], n_samples),
        'Crop Name': np.random.choice(['Rice', 'Wheat', 'Maize', 'Cotton', 'Sugarcane'], n_samples)
    })

# Data preprocessing
print("\n[2.1] Preprocessing data...")

# Clean data - replace Excel error strings with NaN
print("  Cleaning error values...")
error_values = ['#DIV/0!', '#N/A', '#VALUE!', '#REF!', '#NAME?', '#NUM!', '#NULL!']
for col in data.columns:
    if data[col].dtype == 'object':
        # Replace error strings with NaN
        data[col] = data[col].replace(error_values, np.nan)
        # Try to convert to numeric if possible
        try:
            data[col] = pd.to_numeric(data[col], errors='ignore')
        except:
            pass

# Drop rows with missing values
data_cleaned = data.dropna()
if len(data) != len(data_cleaned):
    print(f"  Removed {len(data) - len(data_cleaned)} rows with missing values/errors")
    data = data_cleaned

# Encode categorical variables
label_encoders = {}
categorical_cols = ['District', 'Season', 'Crop Name', 'Transplant', 'Growth', 'Harvest']

for col in categorical_cols:
    if col in data.columns:
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col])
        label_encoders[col] = le
        print(f"  Encoded '{col}' with {len(le.classes_)} classes")

# Feature engineering
print("\n[2.2] Feature engineering...")
if 'Max Temp' in data.columns and 'Min Temp' in data.columns:
    data['Temp_Range'] = data['Max Temp'] - data['Min Temp']
    print("  Added: Temp_Range")

if 'Max Relative Humidity' in data.columns and 'Min Relative Humidity' in data.columns:
    data['Humidity_Range'] = data['Max Relative Humidity'] - data['Min Relative Humidity']
    print("  Added: Humidity_Range")

if 'Avg Temp' in data.columns and 'Avg Humidity' in data.columns:
    data['Temp_Humidity_Index'] = data['Avg Temp'] * data['Avg Humidity'] / 100
    print("  Added: Temp_Humidity_Index")

# Prepare features and target
X = data.drop(['Crop Name'], axis=1)
y = data['Crop Name']

# Store season data before splitting (for Temporal RF)
season_data = None
if 'Season' in data.columns:
    season_data = data['Season'].values.copy()

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data (including season data if available)
if season_data is not None:
    X_train, X_test, y_train, y_test, season_train, season_test = train_test_split(
        X_scaled, y, season_data, test_size=0.2, random_state=SEED, stratify=y
    )
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=SEED, stratify=y
    )
    season_train, season_test = None, None

print(f"\n[2.3] Data split:")
print(f"  Training set: {X_train.shape[0]} samples")
print(f"  Test set: {X_test.shape[0]} samples")
print(f"  Features: {X_train.shape[1]}")
print(f"  Classes: {len(np.unique(y))}")

# ============================================================
# 3. MODEL TRAINING AND EVALUATION
# ============================================================

print("\n" + "="*80)
print("[3] TRAINING AND EVALUATING MODELS")
print("="*80)

# Define models to compare
models = {
    'Standard Random Forest': RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        random_state=SEED,
        n_jobs=-1,
        verbose=0
    ),
    'Cascade Random Forest': CascadeRandomForest(
        n_layers=4,
        n_estimators_per_layer=80,
        max_depth=18,
        random_state=SEED
    ),
    'Hierarchical Random Forest': HierarchicalRandomForest(
        n_clusters=5,
        n_estimators_global=80,
        n_estimators_local=60,
        max_depth=18,
        random_state=SEED
    ),
    'Temporal Random Forest': TemporalRandomForest(
        temporal_feature='Season',
        n_estimators_base=80,
        n_estimators_temporal=70,
        max_depth=18,
        random_state=SEED
    )
}

# For Temporal RF, we need temporal data (already split above)
temporal_data_train = season_train
temporal_data_test = season_test

# Train and evaluate models
results = {}
predictions = {}
training_times = {}

print("\n[3.1] Training models...")
print("-" * 60)

for name, model in tqdm(models.items(), desc="Models"):
    print(f"\n{'='*50}")
    print(f"Training: {name}")
    print(f"{'='*50}")
    
    start_time = time.time()
    
    try:
        # Special handling for Temporal RF
        if name == 'Temporal Random Forest' and temporal_data_train is not None:
            model.fit(X_train, y_train, temporal_data=temporal_data_train)
        else:
            model.fit(X_train, y_train)
        
        train_time = time.time() - start_time
        training_times[name] = train_time
        
        # Make predictions
        if name == 'Temporal Random Forest' and temporal_data_test is not None:
            y_pred = model.predict(X_test, temporal_data=temporal_data_test)
            y_pred_proba = model.predict_proba(X_test, temporal_data=temporal_data_test)
        else:
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)
        
        predictions[name] = (y_pred, y_pred_proba)
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # Cross-validation
        print(f"  Performing cross-validation...")
        
        # Special CV for Temporal RF
        if name == 'Temporal Random Forest':
            # Custom CV that preserves temporal structure
            cv_scores = []
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)
            
            # Use season_data for CV if available
            if season_data is not None:
                for train_idx, val_idx in skf.split(X_scaled, y):
                    X_cv_train, X_cv_val = X_scaled[train_idx], X_scaled[val_idx]
                    y_cv_train, y_cv_val = y.iloc[train_idx], y.iloc[val_idx]
                    
                    # Get temporal data for CV fold
                    temporal_cv_train = season_data[train_idx]
                    temporal_cv_val = season_data[val_idx]
                    
                    # Train and evaluate
                    model_cv = clone(model)
                    model_cv.fit(X_cv_train, y_cv_train, temporal_data=temporal_cv_train)
                    y_cv_pred = model_cv.predict(X_cv_val, temporal_data=temporal_cv_val)
                    
                    cv_scores.append(accuracy_score(y_cv_val, y_cv_pred))
            else:
                # Fallback to standard CV if no season data
                cv_scores = cross_val_score(
                    model, X_scaled, y, cv=3, scoring='accuracy', n_jobs=-1
                )
            
            cv_accuracy_mean = np.mean(cv_scores)
            cv_accuracy_std = np.std(cv_scores)
            
        else:
            # Standard CV for other models
            cv_scores = cross_val_score(
                model, X_scaled, y, cv=3, scoring='accuracy', n_jobs=-1
            )
            cv_accuracy_mean = cv_scores.mean()
            cv_accuracy_std = cv_scores.std()
                
        # Store results
        results[name] = {
            'Test Accuracy': accuracy,
            'Test Precision': precision,
            'Test Recall': recall,
            'Test F1': f1,
            'CV Accuracy Mean': cv_accuracy_mean,
            'CV Accuracy Std': cv_accuracy_std,
            'Training Time': train_time
        }
        
        print(f"  ‚úì Training completed in {train_time:.2f}s")
        print(f"  Test Accuracy: {accuracy:.4f}")
        print(f"  CV Accuracy: {cv_accuracy_mean:.4f} ¬± {cv_accuracy_std:.4f}")
        
        # Print feature importance if available
        if hasattr(model, 'feature_importances_'):
            top_features = np.argsort(model.feature_importances_)[-5:][::-1]
            print(f"  Top 5 features by importance: {top_features}")
        
    except Exception as e:
        print(f"  ‚úó Error training {name}: {str(e)}")
        results[name] = {
            'Test Accuracy': 0,
            'Test Precision': 0,
            'Test Recall': 0,
            'Test F1': 0,
            'CV Accuracy Mean': 0,
            'CV Accuracy Std': 0,
            'Training Time': 0
        }

# ============================================================
# 4. RESULTS COMPARISON
# ============================================================

print("\n" + "="*80)
print("[4] RESULTS COMPARISON")
print("="*80)

# Create results DataFrame
results_df = pd.DataFrame(results).T
results_df = results_df.sort_values('Test Accuracy', ascending=False)

print("\n[4.1] Performance Comparison:")
print("-" * 80)
print(results_df.round(4).to_string())

# Visualize results with research-worthy styling
sns.set_style("whitegrid")
research_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E', '#BC4B51']
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.patch.set_facecolor('white')

# 1. Test Accuracy Comparison
ax1 = axes[0, 0]
models_sorted = results_df.index.tolist()
accuracies = results_df['Test Accuracy'].values
bars1 = ax1.barh(models_sorted, accuracies, color=research_colors[:len(models_sorted)], edgecolor='black', linewidth=1.2)
ax1.set_xlabel('Accuracy', fontsize=12, fontweight='bold')
ax1.set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')
ax1.axvline(x=accuracies.max(), color='#C73E1D', linestyle='--', alpha=0.7, linewidth=2)
ax1.grid(False)
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)

# 2. CV Accuracy with Error Bars
ax2 = axes[0, 1]
cv_means = results_df['CV Accuracy Mean'].values
cv_stds = results_df['CV Accuracy Std'].values
y_pos = np.arange(len(models_sorted))
bars2 = ax2.barh(y_pos, cv_means, xerr=cv_stds, color=research_colors[:len(models_sorted)], 
                 capsize=5, edgecolor='black', linewidth=1.2, ecolor='#333333', error_kw={'linewidth': 2})
ax2.set_yticks(y_pos)
ax2.set_yticklabels(models_sorted)
ax2.set_xlabel('Accuracy', fontsize=12, fontweight='bold')
ax2.set_title('Cross-Validation Accuracy (¬± std)', fontsize=14, fontweight='bold')
ax2.grid(False)
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)

# 3. Training Time Comparison
ax3 = axes[0, 2]
training_times_vals = results_df['Training Time'].values
bars3 = ax3.barh(models_sorted, training_times_vals, color=research_colors[:len(models_sorted)], 
                 edgecolor='black', linewidth=1.2)
ax3.set_xlabel('Time (seconds)', fontsize=12, fontweight='bold')
ax3.set_title('Training Time Comparison', fontsize=14, fontweight='bold')
ax3.grid(False)
ax3.spines['top'].set_visible(False)
ax3.spines['right'].set_visible(False)

# 4. F1-Score Comparison
ax4 = axes[1, 0]
f1_scores = results_df['Test F1'].values
bars4 = ax4.barh(models_sorted, f1_scores, color=research_colors[:len(models_sorted)], 
                 edgecolor='black', linewidth=1.2)
ax4.set_xlabel('F1-Score', fontsize=12, fontweight='bold')
ax4.set_title('Test F1-Score Comparison', fontsize=14, fontweight='bold')
ax4.axvline(x=f1_scores.max(), color='#C73E1D', linestyle='--', alpha=0.7, linewidth=2)
ax4.grid(False)
ax4.spines['top'].set_visible(False)
ax4.spines['right'].set_visible(False)

# 5. Precision-Recall Tradeoff
ax5 = axes[1, 1]
precision = results_df['Test Precision'].values
recall = results_df['Test Recall'].values
for i, model in enumerate(models_sorted):
    ax5.scatter(precision[i], recall[i], s=200, alpha=0.8, 
                color=research_colors[i % len(research_colors)], 
                edgecolor='black', linewidth=1.5, zorder=3)
    ax5.annotate(model, (precision[i], recall[i]), xytext=(8, 8), 
                 textcoords='offset points', fontsize=9, fontweight='bold',
                 bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='gray', alpha=0.8))
ax5.set_xlabel('Precision', fontsize=12, fontweight='bold')
ax5.set_ylabel('Recall', fontsize=12, fontweight='bold')
ax5.set_title('Precision-Recall Tradeoff', fontsize=14, fontweight='bold')
ax5.grid(False)
ax5.spines['top'].set_visible(False)
ax5.spines['right'].set_visible(False)

# 6. Accuracy vs Training Time
ax6 = axes[1, 2]
for i, model in enumerate(models_sorted):
    ax6.scatter(training_times_vals[i], accuracies[i], s=200, alpha=0.8,
                color=research_colors[i % len(research_colors)], 
                edgecolor='black', linewidth=1.5, zorder=3)
    ax6.annotate(model, (training_times_vals[i], accuracies[i]), 
                 xytext=(8, 8), textcoords='offset points', fontsize=9, fontweight='bold',
                 bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='gray', alpha=0.8))
ax6.set_xlabel('Training Time (seconds)', fontsize=12, fontweight='bold')
ax6.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')
ax6.set_title('Accuracy vs Training Time', fontsize=14, fontweight='bold')
ax6.grid(False)
ax6.spines['top'].set_visible(False)
ax6.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig('model_comparison.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# ============================================================
# 4.2 COMPREHENSIVE EVALUATION METRICS
# ============================================================

print("\n" + "="*80)
print("[4.2] COMPREHENSIVE EVALUATION METRICS")
print("="*80)

# Calculate additional evaluation metrics
additional_metrics = {}
for name in models_sorted:
    if name in predictions:
        y_pred, y_proba = predictions[name]
        additional_metrics[name] = {
            'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred),
            'Matthews Corr Coef': matthews_corrcoef(y_test, y_pred),
            'Cohen Kappa': cohen_kappa_score(y_test, y_pred),
            'Log Loss': log_loss(y_test, y_proba),
            'Macro F1': f1_score(y_test, y_pred, average='macro'),
            'Weighted F1': f1_score(y_test, y_pred, average='weighted'),
            'Macro Precision': precision_score(y_test, y_pred, average='macro', zero_division=0),
            'Macro Recall': recall_score(y_test, y_pred, average='macro', zero_division=0),
        }

metrics_df = pd.DataFrame(additional_metrics).T
print("\nAdditional Evaluation Metrics:")
print("="*80)
print(metrics_df.round(4).to_string())

# ============================================================
# 4.3 RESEARCH-GRADE EVALUATION PLOTS
# ============================================================

print("\n" + "="*80)
print("[4.3] RESEARCH-GRADE EVALUATION PLOTS")
print("="*80)

# Plotting Configuration for Research Paper Quality
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 12
plt.rcParams['axes.labelweight'] = 'bold'
plt.rcParams['axes.titleweight'] = 'bold'
plt.rcParams['figure.dpi'] = 100

print("\n[4.3] Generating Research-Grade Evaluation Plots...")

# Plot 1: Per-Class Performance Heatmap
print("  Generating per-class performance heatmap...")
fig, ax = plt.subplots(figsize=(14, 8), facecolor='white')

# Calculate per-class F1 scores for each model
per_class_scores = {}
class_names = label_encoders['Crop Name'].classes_ if 'Crop Name' in label_encoders else [f'Class {i}' for i in range(len(np.unique(y)))]

for model_name in models_sorted:
    if model_name in predictions:
        y_pred, _ = predictions[model_name]
        f1_per_class = f1_score(y_test, y_pred, average=None, zero_division=0)
        per_class_scores[model_name] = f1_per_class

if per_class_scores:
    # Create heatmap
    heatmap_data = np.array([per_class_scores[model] for model in models_sorted])
    
    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn', 
                xticklabels=class_names, yticklabels=models_sorted,
                cbar_kws={'label': 'F1-Score'}, linewidths=0.5, linecolor='gray',
                vmin=0.85, vmax=1.0, ax=ax)
    ax.set_title('Per-Class F1-Score Heatmap', fontsize=16, fontweight='bold', pad=15)
    ax.set_xlabel('Crop Classes', fontsize=12, fontweight='bold')
    ax.set_ylabel('Models', fontsize=12, fontweight='bold')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig('per_class_performance.png', dpi=600, bbox_inches='tight', facecolor='white')
    plt.show()

# Plot 2: ROC Curves for Multi-Class (One-vs-Rest)
print("  Generating ROC curves...")
fig, axes = plt.subplots(2, 2, figsize=(16, 14), facecolor='white')
axes = axes.ravel()

# Binarize the labels
y_test_bin = label_binarize(y_test, classes=np.unique(y))
n_classes = y_test_bin.shape[1]

colors = cycle(['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E', '#BC4B51'])

for idx, model_name in enumerate(models_sorted[:4]):  # Plot top 4 models
    if model_name in predictions:
        ax = axes[idx]
        _, y_pred_proba = predictions[model_name]
        
        # Ensure proba has correct shape
        if y_pred_proba.shape[1] == n_classes:
            # Compute ROC curve and ROC area for each class
            fpr = dict()
            tpr = dict()
            roc_auc = dict()
            
            for i in range(n_classes):
                fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])
                roc_auc[i] = auc(fpr[i], tpr[i])
            
            # Compute micro-average ROC curve
            fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_pred_proba.ravel())
            roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
            
            # Plot micro-average ROC curve
            ax.plot(fpr["micro"], tpr["micro"],
                   label=f'Micro-average (AUC = {roc_auc["micro"]:.3f})',
                   color='black', linestyle='--', linewidth=3)
            
            # Plot ROC curve for each class (only show first 5 to avoid clutter)
            for i, color in zip(range(min(5, n_classes)), colors):
                ax.plot(fpr[i], tpr[i], color=color, lw=2,
                       label=f'{class_names[i][:15]} (AUC = {roc_auc[i]:.2f})')
            
            ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.3)
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')
            ax.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')
            ax.set_title(f'ROC Curves - {model_name}', fontsize=12, fontweight='bold')
            ax.legend(loc="lower right", fontsize=8)
            ax.grid(False)
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig('roc_curves.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# Plot 3: Model Complexity vs Performance
print("  Generating complexity vs performance plot...")
fig, ax = plt.subplots(figsize=(12, 8), facecolor='white')

# Estimate model complexity (for custom models, use proxy metrics)
complexity_metrics = {
    'Standard Random Forest': 100 * 15,  # n_estimators * max_depth
    'Cascade Random Forest': 80 * 18 * 4,  # estimators * depth * layers
    'Hierarchical Random Forest': (80 * 18) + (60 * 18 * 5),  # global + local * clusters
    'Temporal Random Forest': (80 * 18) + (70 * 18 * 3)  # base + temporal * categories (approx)
}

complexity_vals = [complexity_metrics.get(m, 0) for m in models_sorted]
accuracy_vals = results_df['Test Accuracy'].values

for i, model in enumerate(models_sorted):
    ax.scatter(complexity_vals[i], accuracy_vals[i], s=300, alpha=0.7,
              color=research_colors[i % len(research_colors)],
              edgecolor='black', linewidth=2, zorder=3)
    ax.annotate(model, (complexity_vals[i], accuracy_vals[i]), 
               xytext=(10, 10), textcoords='offset points', fontsize=10, fontweight='bold',
               bbox=dict(boxstyle='round,pad=0.5', facecolor='white', edgecolor='gray', alpha=0.9))

ax.set_xlabel('Model Complexity (Estimators √ó Depth √ó Components)', fontsize=12, fontweight='bold')
ax.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')
ax.set_title('Model Complexity vs Performance Tradeoff', fontsize=14, fontweight='bold')
ax.grid(True, alpha=0.2, linestyle='--')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('complexity_vs_performance.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# Plot 4: Calibration Curves
print("  Generating calibration curves...")
fig, axes = plt.subplots(2, 2, figsize=(14, 12), facecolor='white')
axes = axes.ravel()

for idx, model_name in enumerate(models_sorted[:4]):
    if model_name in predictions:
        ax = axes[idx]
        _, y_pred_proba = predictions[model_name]
        
        # Calculate calibration for each class and average
        mean_predicted_values = []
        fraction_of_positives_list = []
        
        for class_idx in range(min(3, n_classes)):  # Plot first 3 classes
            # Binary problem: class vs rest
            y_binary = (y_test == class_idx).astype(int)
            prob_pos = y_pred_proba[:, class_idx]
            
            fraction_of_positives, mean_predicted_value = calibration_curve(
                y_binary, prob_pos, n_bins=10, strategy='uniform'
            )
            
            ax.plot(mean_predicted_value, fraction_of_positives, 
                   marker='o', linewidth=2, label=f'{class_names[class_idx][:15]}')
        
        # Plot perfectly calibrated line
        ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Calibration')
        
        ax.set_xlabel('Mean Predicted Probability', fontsize=11, fontweight='bold')
        ax.set_ylabel('Fraction of Positives', fontsize=11, fontweight='bold')
        ax.set_title(f'Calibration Curve - {model_name}', fontsize=12, fontweight='bold')
        ax.legend(loc='lower right', fontsize=9)
        ax.grid(True, alpha=0.2, linestyle='--')
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig('calibration_curves.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# Plot 5: Comparative Confusion Matrices (2x2 grid for top 4 models)
print("  Generating comparative confusion matrices...")
fig, axes = plt.subplots(2, 2, figsize=(16, 14), facecolor='white')
axes = axes.ravel()

from matplotlib.colors import LinearSegmentedColormap
colors_cm = ['#FFFFFF', '#E8F4F8', '#2E86AB']
cmap_custom = LinearSegmentedColormap.from_list('research', colors_cm, N=100)

for idx, model_name in enumerate(models_sorted[:4]):
    if model_name in predictions:
        ax = axes[idx]
        y_pred, _ = predictions[model_name]
        cm = confusion_matrix(y_test, y_pred)
        
        # Normalize confusion matrix
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap=cmap_custom,
                   xticklabels=class_names, yticklabels=class_names,
                   cbar_kws={'label': 'Normalized Count'}, linewidths=0.5, 
                   linecolor='gray', ax=ax, vmin=0, vmax=1)
        ax.set_title(f'{model_name}\n(Accuracy: {results_df.loc[model_name, "Test Accuracy"]:.4f})',
                    fontsize=11, fontweight='bold')
        ax.set_xlabel('Predicted', fontsize=10, fontweight='bold')
        ax.set_ylabel('True', fontsize=10, fontweight='bold')
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontsize=8)
        plt.setp(ax.get_yticklabels(), rotation=0, fontsize=8)

plt.tight_layout()
plt.savefig('comparative_confusion_matrices.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# Plot 6: Performance Metrics Radar Chart
print("  Generating performance radar chart...")
fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'), facecolor='white')

categories = ['Test\nAccuracy', 'Precision', 'Recall', 'F1-Score', 'CV Mean', '1 - CV Std']
N = len(categories)

# Prepare data (normalize to 0-1 scale)
angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
angles += angles[:1]

for i, model_name in enumerate(models_sorted):
    values = [
        results_df.loc[model_name, 'Test Accuracy'],
        results_df.loc[model_name, 'Test Precision'],
        results_df.loc[model_name, 'Test Recall'],
        results_df.loc[model_name, 'Test F1'],
        results_df.loc[model_name, 'CV Accuracy Mean'],
        1 - results_df.loc[model_name, 'CV Accuracy Std']  # Invert std (lower is better)
    ]
    values += values[:1]
    
    ax.plot(angles, values, 'o-', linewidth=2, label=model_name,
           color=research_colors[i % len(research_colors)])
    ax.fill(angles, values, alpha=0.15, color=research_colors[i % len(research_colors)])

ax.set_xticks(angles[:-1])
ax.set_xticklabels(categories, fontsize=11, fontweight='bold')
ax.set_ylim(0.92, 1.0)
ax.set_yticks([0.93, 0.95, 0.97, 0.99])
ax.set_yticklabels(['0.93', '0.95', '0.97', '0.99'], fontsize=9)
ax.set_title('Performance Metrics Radar Chart', fontsize=14, fontweight='bold', pad=20)
ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('performance_radar.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# Plot 7: Training Time vs Accuracy Efficiency Score
print("  Generating efficiency score plot...")
fig, ax = plt.subplots(figsize=(12, 8), facecolor='white')

# Calculate efficiency score: Accuracy / log(Training Time + 1)
efficiency_scores = []
for model_name in models_sorted:
    acc = results_df.loc[model_name, 'Test Accuracy']
    time_val = results_df.loc[model_name, 'Training Time']
    efficiency = acc / np.log(time_val + 2)  # +2 to avoid log(0) and very high values
    efficiency_scores.append(efficiency)

bars = ax.barh(models_sorted, efficiency_scores, color=research_colors[:len(models_sorted)],
              edgecolor='black', linewidth=1.5)

# Add value labels on bars
for i, (bar, score) in enumerate(zip(bars, efficiency_scores)):
    width = bar.get_width()
    ax.text(width, bar.get_y() + bar.get_height()/2, f'{score:.3f}',
           ha='left', va='center', fontweight='bold', fontsize=10)

ax.set_xlabel('Efficiency Score (Accuracy / log(Time+2))', fontsize=12, fontweight='bold')
ax.set_title('Model Efficiency Analysis', fontsize=14, fontweight='bold')
ax.grid(True, axis='x', alpha=0.2, linestyle='--')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig('efficiency_score.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# Plot 8: Error Analysis - Misclassification Pattern
print("  Generating error analysis plot...")
fig, axes = plt.subplots(2, 2, figsize=(16, 14), facecolor='white')
axes = axes.ravel()

for idx, model_name in enumerate(models_sorted[:4]):
    if model_name in predictions:
        ax = axes[idx]
        y_pred, _ = predictions[model_name]
        
        # Calculate misclassification matrix (where i,j = count of i misclassified as j)
        cm = confusion_matrix(y_test, y_pred)
        misclass_matrix = cm.copy()
        np.fill_diagonal(misclass_matrix, 0)  # Remove correct classifications
        
        sns.heatmap(misclass_matrix, annot=True, fmt='d', cmap='Reds',
                   xticklabels=class_names, yticklabels=class_names,
                   cbar_kws={'label': 'Misclassification Count'}, 
                   linewidths=0.5, linecolor='gray', ax=ax)
        ax.set_title(f'Misclassification Pattern - {model_name}',
                    fontsize=11, fontweight='bold')
        ax.set_xlabel('Predicted As', fontsize=10, fontweight='bold')
        ax.set_ylabel('Actually Is', fontsize=10, fontweight='bold')
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontsize=8)
        plt.setp(ax.get_yticklabels(), rotation=0, fontsize=8)

plt.tight_layout()
plt.savefig('error_analysis.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

print("\n‚úì All advanced evaluation plots generated successfully!")
print("  Saved plots:")
print("    - per_class_performance.png")
print("    - roc_curves.png")
print("    - complexity_vs_performance.png")
print("    - calibration_curves.png")
print("    - comparative_confusion_matrices.png")
print("    - performance_radar.png")
print("    - efficiency_score.png")
print("    - error_analysis.png")

# ============================================================
# 4.3 COMPREHENSIVE STATISTICAL ANALYSIS & VALIDATION
# ============================================================

print("\n" + "="*80)
print("[4.3] STATISTICAL TESTS AND ADVANCED CROSS-VALIDATION")
print("="*80)

# ----------------------------------------------------------------
# A. REPEATED STRATIFIED K-FOLD CROSS-VALIDATION
# ----------------------------------------------------------------
print("\n[A] Repeated Stratified K-Fold Cross-Validation (5x3)...")

repeated_cv_results = {}
rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=SEED)

for name, model in models.items():
    print(f"  Processing {name}...")
    try:
        if name == 'Temporal Random Forest' and season_data is not None:
            # Custom handling for temporal models
            scores = []
            for train_idx, val_idx in rskf.split(X_scaled, y):
                X_cv_train, X_cv_val = X_scaled[train_idx], X_scaled[val_idx]
                y_cv_train, y_cv_val = y.iloc[train_idx], y.iloc[val_idx]
                temporal_cv_train = season_data[train_idx]
                temporal_cv_val = season_data[val_idx]
                
                model_cv = clone(model)
                model_cv.fit(X_cv_train, y_cv_train, temporal_data=temporal_cv_train)
                y_cv_pred = model_cv.predict(X_cv_val, temporal_data=temporal_cv_val)
                scores.append(accuracy_score(y_cv_val, y_cv_pred))
            
            repeated_cv_results[name] = {
                'scores': np.array(scores),
                'mean': np.mean(scores),
                'std': np.std(scores),
                'min': np.min(scores),
                'max': np.max(scores),
                'median': np.median(scores)
            }
        else:
            cv_results = cross_validate(
                model, X_scaled, y, cv=rskf, 
                scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],
                return_train_score=True, n_jobs=-1
            )
            repeated_cv_results[name] = {
                'scores': cv_results['test_accuracy'],
                'mean': cv_results['test_accuracy'].mean(),
                'std': cv_results['test_accuracy'].std(),
                'min': cv_results['test_accuracy'].min(),
                'max': cv_results['test_accuracy'].max(),
                'median': np.median(cv_results['test_accuracy']),
                'train_mean': cv_results['train_accuracy'].mean(),
                'precision': cv_results['test_precision_weighted'].mean(),
                'recall': cv_results['test_recall_weighted'].mean(),
                'f1': cv_results['test_f1_weighted'].mean()
            }
    except Exception as e:
        print(f"    Error with {name}: {e}")
        repeated_cv_results[name] = {
            'scores': np.array([0]),
            'mean': 0, 'std': 0, 'min': 0, 'max': 0, 'median': 0
        }

print("\n  Repeated CV Results:")
print("  " + "-"*70)
for name in models_sorted:
    if name in repeated_cv_results:
        res = repeated_cv_results[name]
        print(f"  {name:30s}: {res['mean']:.4f} ¬± {res['std']:.4f} "
              f"[{res['min']:.4f}, {res['max']:.4f}]")

# ----------------------------------------------------------------
# B. BOOTSTRAP VALIDATION
# ----------------------------------------------------------------
print("\n[B] Bootstrap Validation (100 iterations)...")

bootstrap_results = {}
n_bootstrap = 100

for name in models_sorted[:3]:  # Top 3 models only (computational cost)
    if name in predictions:
        print(f"  Bootstrapping {name}...")
        y_pred, _ = predictions[name]
        bootstrap_scores = []
        
        for i in range(n_bootstrap):
            # Resample with replacement
            indices = resample(range(len(y_test)), n_samples=len(y_test), random_state=SEED+i)
            y_test_boot = y_test.iloc[indices]
            y_pred_boot = y_pred[indices]
            bootstrap_scores.append(accuracy_score(y_test_boot, y_pred_boot))
        
        bootstrap_results[name] = {
            'mean': np.mean(bootstrap_scores),
            'std': np.std(bootstrap_scores),
            'ci_lower': np.percentile(bootstrap_scores, 2.5),
            'ci_upper': np.percentile(bootstrap_scores, 97.5)
        }

print("\n  Bootstrap 95% Confidence Intervals:")
print("  " + "-"*70)
for name, res in bootstrap_results.items():
    print(f"  {name:30s}: {res['mean']:.4f} ¬± {res['std']:.4f} "
          f"CI: [{res['ci_lower']:.4f}, {res['ci_upper']:.4f}]")

# ----------------------------------------------------------------
# C. PAIRED STATISTICAL TESTS
# ----------------------------------------------------------------
print("\n[C] Paired Statistical Significance Tests...")

# Compare each custom model with Standard RF
baseline_name = 'Standard Random Forest'
if baseline_name in repeated_cv_results:
    baseline_scores = repeated_cv_results[baseline_name]['scores']
    
    print(f"\n  Comparing models against {baseline_name}:")
    print("  " + "-"*70)
    
    comparison_results = []
    
    for name in models_sorted:
        if name != baseline_name and name in repeated_cv_results:
            model_scores = repeated_cv_results[name]['scores']
            
            # Paired t-test
            t_stat, t_pvalue = stats.ttest_rel(model_scores, baseline_scores)
            
            # Wilcoxon signed-rank test (non-parametric alternative)
            w_stat, w_pvalue = stats.wilcoxon(model_scores, baseline_scores, 
                                              alternative='two-sided', zero_method='wilcox')
            
            # Effect size (Cohen's d)
            pooled_std = np.sqrt((np.std(model_scores)**2 + np.std(baseline_scores)**2) / 2)
            cohens_d = (np.mean(model_scores) - np.mean(baseline_scores)) / pooled_std if pooled_std > 0 else 0
            
            comparison_results.append({
                'model': name,
                'mean_diff': np.mean(model_scores) - np.mean(baseline_scores),
                't_stat': t_stat,
                't_pvalue': t_pvalue,
                'w_stat': w_stat,
                'w_pvalue': w_pvalue,
                'cohens_d': cohens_d
            })
            
            significance = "***" if t_pvalue < 0.001 else "**" if t_pvalue < 0.01 else "*" if t_pvalue < 0.05 else "ns"
            
            print(f"  {name:30s}")
            print(f"    Mean Difference: {np.mean(model_scores) - np.mean(baseline_scores):+.4f}")
            print(f"    Paired t-test: t={t_stat:.3f}, p={t_pvalue:.4f} {significance}")
            print(f"    Wilcoxon test: W={w_stat:.1f}, p={w_pvalue:.4f}")
            print(f"    Cohen's d: {cohens_d:.3f} ({'small' if abs(cohens_d) < 0.5 else 'medium' if abs(cohens_d) < 0.8 else 'large'} effect)")
            print()

# ----------------------------------------------------------------
# D. MCNEMAR'S TEST FOR PAIRWISE COMPARISON
# ----------------------------------------------------------------
print("\n[D] McNemar's Test (Pairwise Model Comparison)...")

# Create contingency tables and perform McNemar's test
mcnemar_results = []

for i, name1 in enumerate(models_sorted):
    for name2 in models_sorted[i+1:]:
        if name1 in predictions and name2 in predictions:
            y_pred1, _ = predictions[name1]
            y_pred2, _ = predictions[name2]
            
            # Create contingency table
            # n00: both wrong, n01: model1 wrong & model2 correct
            # n10: model1 correct & model2 wrong, n11: both correct
            n01 = np.sum((y_pred1 != y_test) & (y_pred2 == y_test))
            n10 = np.sum((y_pred1 == y_test) & (y_pred2 != y_test))
            
            # McNemar's test statistic with continuity correction
            if n01 + n10 > 0:
                mcnemar_stat = ((abs(n10 - n01) - 1) ** 2) / (n10 + n01)
                mcnemar_pvalue = 1 - stats.chi2.cdf(mcnemar_stat, 1)
                
                mcnemar_results.append({
                    'model1': name1,
                    'model2': name2,
                    'n01': n01,
                    'n10': n10,
                    'statistic': mcnemar_stat,
                    'pvalue': mcnemar_pvalue
                })

print("\n  Pairwise McNemar's Test Results:")
print("  " + "-"*70)
for res in mcnemar_results[:6]:  # Show top 6 comparisons
    sig = "***" if res['pvalue'] < 0.001 else "**" if res['pvalue'] < 0.01 else "*" if res['pvalue'] < 0.05 else "ns"
    print(f"  {res['model1'][:25]} vs {res['model2'][:25]}")
    print(f"    œá¬≤={res['statistic']:.3f}, p={res['pvalue']:.4f} {sig} "
          f"(disagreements: {res['n01']}‚Üî{res['n10']})")

# ----------------------------------------------------------------
# E. FRIEDMAN TEST (MULTIPLE MODEL COMPARISON)
# ----------------------------------------------------------------
print("\n[E] Friedman Test (Non-parametric ANOVA for Multiple Models)...")

# Organize scores for Friedman test
friedman_data = []
model_names_friedman = []

for name in models_sorted:
    if name in repeated_cv_results and len(repeated_cv_results[name]['scores']) > 0:
        friedman_data.append(repeated_cv_results[name]['scores'])
        model_names_friedman.append(name)

if len(friedman_data) >= 3:
    friedman_stat, friedman_pvalue = stats.friedmanchisquare(*friedman_data)
    
    print(f"\n  Friedman Test Results:")
    print(f"    Statistic: œá¬≤({len(friedman_data)-1}) = {friedman_stat:.3f}")
    print(f"    p-value: {friedman_pvalue:.4f}")
    
    if friedman_pvalue < 0.05:
        print(f"    ‚úì Significant differences detected between models (p < 0.05)")
        
        # Post-hoc: Nemenyi test (rank-based pairwise comparison)
        print("\n  Post-hoc Analysis (Mean Ranks):")
        ranks = []
        for scores in friedman_data:
            ranks.append(stats.rankdata(scores).mean())
        
        for name, rank in zip(model_names_friedman, ranks):
            print(f"    {name:30s}: Rank = {rank:.2f}")
    else:
        print(f"    No significant differences detected between models (p ‚â• 0.05)")

# ----------------------------------------------------------------
# F. VARIANCE ANALYSIS
# ----------------------------------------------------------------
print("\n[F] Variance and Stability Analysis...")

print("\n  Model Stability (CV Coefficient of Variation):")
print("  " + "-"*70)

stability_results = []
for name in models_sorted:
    if name in repeated_cv_results:
        mean_acc = repeated_cv_results[name]['mean']
        std_acc = repeated_cv_results[name]['std']
        cv_coef = (std_acc / mean_acc * 100) if mean_acc > 0 else 0  # Coefficient of variation
        
        stability_results.append({
            'model': name,
            'cv_coefficient': cv_coef,
            'std': std_acc,
            'stability_score': 1 / (1 + cv_coef)  # Higher is more stable
        })
        
        print(f"  {name:30s}: CV={cv_coef:.2f}%, Std={std_acc:.4f}, "
              f"Stability={1/(1+cv_coef):.4f}")

# ----------------------------------------------------------------
# G. BIAS-VARIANCE TRADEOFF ESTIMATION
# ----------------------------------------------------------------
print("\n[G] Bias-Variance Tradeoff Analysis...")

for name in models_sorted[:3]:  # Top 3 models
    if name in repeated_cv_results:
        train_score = repeated_cv_results[name].get('train_mean', 0)
        test_score = repeated_cv_results[name]['mean']
        
        bias_estimate = 1 - test_score  # Approximation
        variance_estimate = repeated_cv_results[name]['std'] ** 2
        overfitting_gap = train_score - test_score if train_score > 0 else 0
        
        print(f"\n  {name}:")
        print(f"    Train Score: {train_score:.4f}")
        print(f"    Test Score:  {test_score:.4f}")
        print(f"    Overfitting Gap: {overfitting_gap:.4f}")
        print(f"    Bias (approx): {bias_estimate:.4f}")
        print(f"    Variance: {variance_estimate:.6f}")
        print(f"    Assessment: {'Potential Overfitting' if overfitting_gap > 0.02 else 'Well Generalized'}")

# ----------------------------------------------------------------
# H. STATISTICAL SUMMARY TABLE
# ----------------------------------------------------------------
print("\n[H] Comprehensive Statistical Summary Table:")
print("="*80)

summary_df = pd.DataFrame({
    'Model': models_sorted,
    'Mean Acc': [repeated_cv_results[m]['mean'] if m in repeated_cv_results else 0 for m in models_sorted],
    'Std': [repeated_cv_results[m]['std'] if m in repeated_cv_results else 0 for m in models_sorted],
    'Min': [repeated_cv_results[m]['min'] if m in repeated_cv_results else 0 for m in models_sorted],
    'Max': [repeated_cv_results[m]['max'] if m in repeated_cv_results else 0 for m in models_sorted],
    'Median': [repeated_cv_results[m]['median'] if m in repeated_cv_results else 0 for m in models_sorted],
    'CV%': [repeated_cv_results[m]['std']/repeated_cv_results[m]['mean']*100 if m in repeated_cv_results and repeated_cv_results[m]['mean'] > 0 else 0 for m in models_sorted]
})

print(summary_df.to_string(index=False))

# ----------------------------------------------------------------
# I. VISUALIZE STATISTICAL RESULTS
# ----------------------------------------------------------------
print("\n[I] Generating Statistical Visualization Plots...")

# Plot 1: CV Score Distributions (Box Plots)
fig, ax = plt.subplots(figsize=(14, 8), facecolor='white')

cv_scores_list = [repeated_cv_results[m]['scores'] for m in models_sorted if m in repeated_cv_results]
bp = ax.boxplot(cv_scores_list, labels=models_sorted, patch_artist=True,
                notch=True, showmeans=True,
                meanprops=dict(marker='D', markerfacecolor='red', markersize=8),
                medianprops=dict(color='black', linewidth=2))

for patch, color in zip(bp['boxes'], research_colors[:len(models_sorted)]):
    patch.set_facecolor(color)
    patch.set_alpha(0.6)

ax.set_ylabel('Cross-Validation Accuracy', fontsize=12, fontweight='bold')
ax.set_title('Cross-Validation Score Distributions (5x3 Repeated CV)', fontsize=14, fontweight='bold')
ax.grid(True, axis='y', alpha=0.3, linestyle='--')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('cv_distributions.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# Plot 2: Statistical Comparison Heatmap
if comparison_results:
    fig, ax = plt.subplots(figsize=(10, 6), facecolor='white')
    
    comparison_matrix = np.zeros((len(comparison_results), 3))
    comparison_labels = []
    
    for i, res in enumerate(comparison_results):
        comparison_matrix[i, 0] = res['mean_diff']
        comparison_matrix[i, 1] = -np.log10(res['t_pvalue']) if res['t_pvalue'] > 0 else 10
        comparison_matrix[i, 2] = res['cohens_d']
        comparison_labels.append(res['model'][:25])
    
    sns.heatmap(comparison_matrix, annot=True, fmt='.3f', 
                xticklabels=['Mean Diff', '-log10(p)', "Cohen's d"],
                yticklabels=comparison_labels, cmap='RdYlGn', center=0,
                cbar_kws={'label': 'Effect Magnitude'}, linewidths=0.5, ax=ax)
    ax.set_title(f'Statistical Comparison vs {baseline_name}', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('statistical_comparison.png', dpi=600, bbox_inches='tight', facecolor='white')
    plt.show()

# Plot 3: Bias-Variance Visualization
fig, ax = plt.subplots(figsize=(10, 8), facecolor='white')

for i, name in enumerate(models_sorted[:4]):
    if name in repeated_cv_results:
        train_score = repeated_cv_results[name].get('train_mean', 0)
        test_score = repeated_cv_results[name]['mean']
        variance = repeated_cv_results[name]['std']
        
        ax.scatter(1 - test_score, variance, s=300, alpha=0.7,
                  color=research_colors[i % len(research_colors)],
                  edgecolor='black', linewidth=2, label=name)
        
        ax.annotate(name, (1 - test_score, variance), 
                   xytext=(10, 10), textcoords='offset points',
                   fontsize=9, fontweight='bold',
                   bbox=dict(boxstyle='round,pad=0.5', facecolor='white', 
                            edgecolor='gray', alpha=0.8))

ax.set_xlabel('Bias (1 - Test Accuracy)', fontsize=12, fontweight='bold')
ax.set_ylabel('Variance (CV Std)', fontsize=12, fontweight='bold')
ax.set_title('Bias-Variance Tradeoff', fontsize=14, fontweight='bold')
ax.legend(loc='best', fontsize=9)
ax.grid(True, alpha=0.3, linestyle='--')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('bias_variance_tradeoff.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

print("\n‚úì Statistical analysis complete!")
print("  Additional plots saved:")
print("    - cv_distributions.png")
print("    - statistical_comparison.png")
print("    - bias_variance_tradeoff.png")

# ----------------------------------------------------------------
# J. PRECISION-RECALL CURVES (MULTI-CLASS)
# ----------------------------------------------------------------
print("\n[J] Generating Precision-Recall Curves...")

fig, ax = plt.subplots(figsize=(14, 10), facecolor='white')

for idx, (name, color) in enumerate(zip(models_sorted[:4], research_colors)):
    if name in predictions:
        y_pred, y_proba = predictions[name]
        
        # Binarize for multi-class PR curves
        y_test_bin = label_binarize(y_test, classes=np.unique(y))
        n_classes = y_test_bin.shape[1]
        
        # Compute PR curve for each class and average
        precision_dict = {}
        recall_dict = {}
        avg_precision = {}
        
        for i in range(n_classes):
            precision_dict[i], recall_dict[i], _ = precision_recall_curve(
                y_test_bin[:, i], y_proba[:, i]
            )
            avg_precision[i] = average_precision_score(y_test_bin[:, i], y_proba[:, i])
        
        # Compute micro-average PR curve
        precision_micro, recall_micro, _ = precision_recall_curve(
            y_test_bin.ravel(), y_proba.ravel()
        )
        avg_precision_micro = average_precision_score(y_test_bin, y_proba, average='micro')
        
        # Plot micro-average PR curve
        ax.plot(recall_micro, precision_micro, color=color, lw=3, alpha=0.8,
                label=f'{name} (AP={avg_precision_micro:.3f})')

ax.set_xlabel('Recall', fontsize=14, fontweight='bold')
ax.set_ylabel('Precision', fontsize=14, fontweight='bold')
ax.set_title('Precision-Recall Curves (Micro-Average)', fontsize=16, fontweight='bold')
ax.legend(loc='lower left', fontsize=11, framealpha=0.95)
ax.grid(True, alpha=0.3, linestyle='--')
ax.set_xlim([0.0, 1.0])
ax.set_ylim([0.0, 1.05])
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

# Add iso-F1 curves
f_scores = np.linspace(0.2, 0.9, num=8)
for f_score in f_scores:
    x = np.linspace(0.01, 1)
    y = f_score * x / (2 * x - f_score)
    ax.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2, linestyle=':', lw=1)
    ax.annotate(f'F1={f_score:.1f}', xy=(0.9, y[45] + 0.02), fontsize=8, color='gray')

plt.tight_layout()
plt.savefig('precision_recall_curves.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# ----------------------------------------------------------------
# K. CUMULATIVE GAIN AND LIFT CHARTS
# ----------------------------------------------------------------
print("\n[K] Generating Cumulative Gain and Lift Charts...")

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8), facecolor='white')

for idx, (name, color) in enumerate(zip(models_sorted[:4], research_colors)):
    if name in predictions:
        y_pred, y_proba = predictions[name]
        
        # Use max probability as "score" for ranking
        y_scores = np.max(y_proba, axis=1)
        
        # Sort by predicted probability (descending)
        sorted_indices = np.argsort(y_scores)[::-1]
        y_test_sorted = y_test.iloc[sorted_indices].values
        y_pred_sorted = y_pred[sorted_indices]
        
        # Calculate cumulative gains
        total_positives = len(y_test)
        cumulative_correct = np.cumsum(y_test_sorted == y_pred_sorted)
        cumulative_samples = np.arange(1, len(y_test_sorted) + 1)
        
        # Cumulative Gain Curve
        gain_x = cumulative_samples / len(y_test_sorted) * 100
        gain_y = cumulative_correct / total_positives * 100
        
        ax1.plot(gain_x, gain_y, color=color, lw=3, alpha=0.8, label=name)
        
        # Lift Curve
        random_lift = cumulative_samples / len(y_test_sorted) * 100
        lift_values = gain_y / (random_lift + 1e-10)
        
        ax2.plot(gain_x, lift_values, color=color, lw=3, alpha=0.8, label=name)

# Cumulative Gain Chart styling
ax1.plot([0, 100], [0, 100], 'k--', lw=2, label='Random Model', alpha=0.5)
ax1.set_xlabel('% of Samples', fontsize=12, fontweight='bold')
ax1.set_ylabel('% of Correct Predictions', fontsize=12, fontweight='bold')
ax1.set_title('Cumulative Gain Chart', fontsize=14, fontweight='bold')
ax1.legend(loc='lower right', fontsize=10)
ax1.grid(True, alpha=0.3, linestyle='--')
ax1.set_xlim([0, 100])
ax1.set_ylim([0, 100])
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)

# Lift Chart styling
ax2.axhline(y=1, color='k', linestyle='--', lw=2, label='Random Model', alpha=0.5)
ax2.set_xlabel('% of Samples', fontsize=12, fontweight='bold')
ax2.set_ylabel('Lift', fontsize=12, fontweight='bold')
ax2.set_title('Lift Chart', fontsize=14, fontweight='bold')
ax2.legend(loc='upper right', fontsize=10)
ax2.grid(True, alpha=0.3, linestyle='--')
ax2.set_xlim([0, 100])
ax2.set_ylim([0, max(3, ax2.get_ylim()[1])])
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig('cumulative_gain_lift.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# ----------------------------------------------------------------
# L. LEARNING CURVES (BIAS-VARIANCE ANALYSIS)
# ----------------------------------------------------------------
print("\n[L] Generating Learning Curves for Bias-Variance Analysis...")

from sklearn.model_selection import learning_curve

fig, axes = plt.subplots(2, 2, figsize=(16, 14), facecolor='white')
axes = axes.ravel()

for idx, (name, color) in enumerate(zip(models_sorted[:4], research_colors)):
    if idx >= 4:
        break
    
    ax = axes[idx]
    model = models[name]
    
    print(f"  Computing learning curve for {name}...")
    
    try:
        if name == 'Temporal Random Forest' and season_data is not None:
            # Custom learning curve for temporal models
            train_sizes_abs = [50, 100, 200, 400, 600, 800, len(X_train)]
            train_scores_mean = []
            test_scores_mean = []
            train_scores_std = []
            test_scores_std = []
            
            for train_size in train_sizes_abs:
                if train_size > len(X_train):
                    continue
                    
                fold_train_scores = []
                fold_test_scores = []
                
                # Simple 3-fold validation
                for fold in range(3):
                    indices = np.random.choice(len(X_train), train_size, replace=False)
                    X_fold = X_train[indices]
                    y_fold = y_train.iloc[indices]
                    temporal_fold = season_data_train[indices]
                    
                    model_lc = clone(model)
                    model_lc.fit(X_fold, y_fold, temporal_data=temporal_fold)
                    
                    train_pred = model_lc.predict(X_fold, temporal_data=temporal_fold)
                    test_pred = model_lc.predict(X_test, temporal_data=season_data_test)
                    
                    fold_train_scores.append(accuracy_score(y_fold, train_pred))
                    fold_test_scores.append(accuracy_score(y_test, test_pred))
                
                train_scores_mean.append(np.mean(fold_train_scores))
                train_scores_std.append(np.std(fold_train_scores))
                test_scores_mean.append(np.mean(fold_test_scores))
                test_scores_std.append(np.std(fold_test_scores))
            
            train_sizes_abs = train_sizes_abs[:len(train_scores_mean)]
            train_sizes = np.array(train_sizes_abs)
            train_scores_mean = np.array(train_scores_mean)
            train_scores_std = np.array(train_scores_std)
            test_scores_mean = np.array(test_scores_mean)
            test_scores_std = np.array(test_scores_std)
            
        else:
            train_sizes, train_scores, test_scores = learning_curve(
                model, X_scaled, y, cv=3, n_jobs=-1,
                train_sizes=np.linspace(0.1, 1.0, 10),
                scoring='accuracy', shuffle=True, random_state=SEED
            )
            
            train_scores_mean = np.mean(train_scores, axis=1)
            train_scores_std = np.std(train_scores, axis=1)
            test_scores_mean = np.mean(test_scores, axis=1)
            test_scores_std = np.std(test_scores, axis=1)
        
        # Plot learning curves
        ax.plot(train_sizes, train_scores_mean, 'o-', color=color, 
                lw=3, alpha=0.8, label='Training score')
        ax.fill_between(train_sizes, 
                        train_scores_mean - train_scores_std,
                        train_scores_mean + train_scores_std, 
                        alpha=0.2, color=color)
        
        ax.plot(train_sizes, test_scores_mean, 's--', color=color, 
                lw=3, alpha=0.6, label='CV score')
        ax.fill_between(train_sizes, 
                        test_scores_mean - test_scores_std,
                        test_scores_mean + test_scores_std, 
                        alpha=0.1, color=color)
        
        # Calculate and display bias-variance indicators
        final_gap = train_scores_mean[-1] - test_scores_mean[-1]
        final_variance = test_scores_std[-1]
        
        ax.set_title(f'{name}\n(Gap={final_gap:.3f}, Var={final_variance:.3f})', 
                    fontsize=12, fontweight='bold')
        ax.set_xlabel('Training Set Size', fontsize=11, fontweight='bold')
        ax.set_ylabel('Accuracy Score', fontsize=11, fontweight='bold')
        ax.legend(loc='lower right', fontsize=9)
        ax.grid(True, alpha=0.3, linestyle='--')
        ax.set_ylim([0.85, 1.0])
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        
        # Add bias-variance annotation
        bias_level = "High Bias" if test_scores_mean[-1] < 0.95 else "Low Bias"
        variance_level = "High Variance" if final_gap > 0.03 else "Low Variance"
        ax.text(0.05, 0.05, f'{bias_level}\n{variance_level}', 
               transform=ax.transAxes, fontsize=10, 
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
               verticalalignment='bottom')
        
    except Exception as e:
        print(f"    Error computing learning curve for {name}: {e}")
        ax.text(0.5, 0.5, f'Error: {str(e)[:50]}', 
               transform=ax.transAxes, ha='center', va='center')
        ax.set_title(name, fontsize=12, fontweight='bold')

plt.suptitle('Learning Curves - Bias-Variance Analysis', 
            fontsize=16, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig('learning_curves_bias_variance.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# ----------------------------------------------------------------
# M. ROC-PR COMBINED ANALYSIS
# ----------------------------------------------------------------
print("\n[M] Generating Combined ROC-PR Analysis...")

fig, axes = plt.subplots(2, 2, figsize=(16, 14), facecolor='white')

for idx, (name, color) in enumerate(zip(models_sorted[:4], research_colors)):
    if idx >= 4:
        break
    
    ax = axes[idx // 2, idx % 2]
    
    if name in predictions:
        y_pred, y_proba = predictions[name]
        
        # Binarize for multi-class
        y_test_bin = label_binarize(y_test, classes=np.unique(y))
        n_classes = y_test_bin.shape[1]
        
        # Compute micro-average ROC
        fpr_micro, tpr_micro, _ = roc_curve(y_test_bin.ravel(), y_proba.ravel())
        roc_auc_micro = auc(fpr_micro, tpr_micro)
        
        # Compute micro-average PR
        precision_micro, recall_micro, _ = precision_recall_curve(
            y_test_bin.ravel(), y_proba.ravel()
        )
        pr_auc_micro = auc(recall_micro, precision_micro)
        
        # Plot ROC on primary axis
        ax_roc = ax
        ax_roc.plot(fpr_micro, tpr_micro, color=color, lw=3, 
                   label=f'ROC (AUC={roc_auc_micro:.3f})', alpha=0.8)
        ax_roc.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.3)
        
        # Plot PR on secondary axis
        ax_pr = ax_roc.twinx()
        ax_pr.plot(recall_micro, precision_micro, color='darkred', lw=3, 
                  linestyle=':', label=f'PR (AUC={pr_auc_micro:.3f})', alpha=0.8)
        
        ax_roc.set_xlabel('False Positive Rate / Recall', fontsize=11, fontweight='bold')
        ax_roc.set_ylabel('True Positive Rate (ROC)', fontsize=11, fontweight='bold', color=color)
        ax_pr.set_ylabel('Precision (PR)', fontsize=11, fontweight='bold', color='darkred')
        ax_roc.set_title(f'{name}', fontsize=12, fontweight='bold')
        
        # Combine legends
        lines1, labels1 = ax_roc.get_legend_handles_labels()
        lines2, labels2 = ax_pr.get_legend_handles_labels()
        ax_roc.legend(lines1 + lines2, labels1 + labels2, loc='lower right', fontsize=9)
        
        ax_roc.grid(True, alpha=0.3, linestyle='--')
        ax_roc.set_xlim([0.0, 1.0])
        ax_roc.set_ylim([0.0, 1.05])
        ax_pr.set_ylim([0.0, 1.05])
        ax_roc.spines['top'].set_visible(False)

plt.suptitle('Combined ROC-PR Analysis (Micro-Average)', 
            fontsize=16, fontweight='bold', y=0.995)
plt.tight_layout()
plt.savefig('roc_pr_combined.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

print("\n‚úì Additional advanced plots generated successfully!")
print("  New plots saved:")
print("    - precision_recall_curves.png")
print("    - cumulative_gain_lift.png")
print("    - learning_curves_bias_variance.png")
print("    - roc_pr_combined.png")

# ============================================================
# 5. DETAILED ANALYSIS
# ============================================================

print("\n" + "="*80)
print("[5] DETAILED ANALYSIS")
print("="*80)

# Best model analysis
best_model_name = results_df.index[0]
print(f"\n[5.1] Best Model: {best_model_name}")
print(f"  Test Accuracy: {results_df.loc[best_model_name, 'Test Accuracy']:.4f}")
print(f"  CV Accuracy: {results_df.loc[best_model_name, 'CV Accuracy Mean']:.4f} "
      f"¬± {results_df.loc[best_model_name, 'CV Accuracy Std']:.4f}")
print(f"  F1-Score: {results_df.loc[best_model_name, 'Test F1']:.4f}")
print(f"  Training Time: {results_df.loc[best_model_name, 'Training Time']:.2f}s")

# Improvement over baseline
if 'Standard Random Forest' in results_df.index:
    baseline_acc = results_df.loc['Standard Random Forest', 'Test Accuracy']
    best_acc = results_df.loc[best_model_name, 'Test Accuracy']
    improvement = ((best_acc - baseline_acc) / baseline_acc) * 100
    
    print(f"\n[5.2] Improvement over Standard Random Forest:")
    print(f"  Baseline (Standard RF): {baseline_acc:.4f}")
    print(f"  Best Model ({best_model_name}): {best_acc:.4f}")
    print(f"  Improvement: {improvement:+.2f}%")

# Feature importance analysis
print("\n[5.3] Feature Importance Analysis:")
for model_name in models_sorted:
    if model_name in models and hasattr(models[model_name], 'feature_importances_'):
        importances = models[model_name].feature_importances_
        top_3_idx = np.argsort(importances)[-3:][::-1]
        
        # Get feature names if available
        if hasattr(X, 'columns'):
            feature_names = X.columns.tolist()
            top_features = [feature_names[i] for i in top_3_idx]
        else:
            top_features = top_3_idx.tolist()
        
        print(f"  {model_name}:")
        print(f"    Top 3 features: {top_features}")
        print(f"    Importance values: {importances[top_3_idx].round(4)}")

# Confusion matrix for best model
if best_model_name in predictions:
    print(f"\n[5.4] Confusion Matrix for {best_model_name}:")
    y_pred_best, _ = predictions[best_model_name]
    
    cm = confusion_matrix(y_test, y_pred_best)
    
    # Create custom colormap for research paper
    from matplotlib.colors import LinearSegmentedColormap
    colors_cm = ['#FFFFFF', '#E8F4F8', '#2E86AB']
    n_bins = 100
    cmap_custom = LinearSegmentedColormap.from_list('research', colors_cm, N=n_bins)
    
    plt.figure(figsize=(12, 10), facecolor='white')
    sns.set_style("whitegrid")
    ax = sns.heatmap(cm, annot=True, fmt='d', cmap=cmap_custom, 
                     cbar=True, linewidths=0.5, linecolor='gray',
                     annot_kws={'fontsize': 11, 'fontweight': 'bold'},
                     cbar_kws={'label': 'Count', 'shrink': 0.8})
    ax.set_facecolor('white')
    plt.title(f'Confusion Matrix - {best_model_name}', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('Predicted Label', fontsize=13, fontweight='bold')
    plt.ylabel('True Label', fontsize=13, fontweight='bold')
    ax.grid(False)
    plt.tight_layout()
    plt.savefig('confusion_matrix_best_model.png', dpi=600, bbox_inches='tight', facecolor='white')
    plt.show()
    
    # Classification report
    print("\n  Classification Report:")
    print(classification_report(y_test, y_pred_best))

# ============================================================
# 6. ENSEMBLE OF NOVEL MODELS
# ============================================================

print("\n" + "="*80)
print("[6] ENSEMBLE OF NOVEL MODELS")
print("="*80)

# Create an ensemble of the three novel models
print("\n[6.1] Creating Ensemble of Novel Models...")

# Get predictions from novel models
novel_model_names = ['Cascade Random Forest', 'Hierarchical Random Forest', 'Temporal Random Forest']
novel_predictions = []

for model_name in novel_model_names:
    if model_name in predictions:
        _, y_pred_proba = predictions[model_name]
        novel_predictions.append(y_pred_proba)

if len(novel_predictions) >= 2:
    # Average predictions
    ensemble_proba = np.mean(novel_predictions, axis=0)
    ensemble_pred = np.argmax(ensemble_proba, axis=1)
    
    # Calculate ensemble metrics
    ensemble_accuracy = accuracy_score(y_test, ensemble_pred)
    ensemble_f1 = f1_score(y_test, ensemble_pred, average='weighted', zero_division=0)
    
    print(f"  Ensemble of {len(novel_predictions)} novel models:")
    print(f"    Test Accuracy: {ensemble_accuracy:.4f}")
    print(f"    F1-Score: {ensemble_f1:.4f}")
    
    # Compare with individual models
    print(f"\n  Comparison with individual models:")
    for model_name in novel_model_names:
        if model_name in results_df.index:
            model_acc = results_df.loc[model_name, 'Test Accuracy']
            diff = ensemble_accuracy - model_acc
            print(f"    vs {model_name}: {ensemble_accuracy:.4f} vs {model_acc:.4f} "
                  f"(Œî = {diff:+.4f})")

# ============================================================
# 7. FINAL CONCLUSIONS
# ============================================================

print("\n" + "="*80)
print("[7] FINAL CONCLUSIONS")
print("="*80)

print(f"\nüìä SUMMARY OF NOVEL RANDOM FOREST VARIANTS:")
print(f"   Models tested: {len(models)}")
print(f"   Best performing model: {best_model_name}")
print(f"   Best test accuracy: {results_df['Test Accuracy'].max():.4f}")
print(f"   Most stable model (lowest CV std): "
      f"{results_df['CV Accuracy Std'].idxmin()} "
      f"({results_df['CV Accuracy Std'].min():.4f})")

print(f"\nüîç KEY INSIGHTS:")
print(f"   1. Cascade RF: Focuses on misclassified instances iteratively")
print(f"   2. Hierarchical RF: Creates specialized models for data clusters")
print(f"   3. Temporal RF: Incorporates seasonal/temporal patterns")
print(f"   4. Standard RF: Serves as baseline comparison")

print(f"\nüéØ RECOMMENDATIONS:")
print(f"   1. Use {best_model_name} for deployment")
print(f"   2. Consider ensemble of novel models for robustness")
print(f"   3. Monitor model performance on new data")
print(f"   4. Regularly retrain models with updated data")

print(f"\nüìà PERFORMANCE METRICS:")
for model_name in models_sorted:
    acc = results_df.loc[model_name, 'Test Accuracy']
    cv_mean = results_df.loc[model_name, 'CV Accuracy Mean']
    cv_std = results_df.loc[model_name, 'CV Accuracy Std']
    time_val = results_df.loc[model_name, 'Training Time']
    
    print(f"   {model_name:30s}: "
          f"Test={acc:.4f}, CV={cv_mean:.4f}¬±{cv_std:.4f}, Time={time_val:.1f}s")

print("\n" + "="*80)
print("EXPERIMENT COMPLETE - NOVEL RANDOM FOREST VARIANTS IMPLEMENTED")
print("="*80)


# ============================================================
# COMPREHENSIVE CROP RECOMMENDATION SYSTEM WITH NOVEL RANDOM FOREST VARIANTS
# ============================================================

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# Data Visualization
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import cm

# Machine Learning Models
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans
from sklearn.base import clone, BaseEstimator, ClassifierMixin
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, fbeta_score, confusion_matrix, classification_report,
                           roc_curve, auc, roc_auc_score, precision_recall_curve,
                           average_precision_score, matthews_corrcoef, cohen_kappa_score,
                           balanced_accuracy_score, jaccard_score, log_loss)
from sklearn.calibration import calibration_curve
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from sklearn.preprocessing import label_binarize
from itertools import cycle
from scipy import stats
from sklearn.model_selection import cross_validate, RepeatedStratifiedKFold, LeaveOneOut
from sklearn.utils import resample

# Additional utilities
import time
from tqdm import tqdm
import random
from collections import defaultdict

# Set random seeds for reproducibility
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# ============================================================
# 1. CUSTOM RANDOM FOREST VARIANTS
# ============================================================

print("="*80)
print("CUSTOM RANDOM FOREST VARIANTS - IMPLEMENTATION")
print("="*80)

# ------------------------------------------------------------
# 1.1 CASCADE RANDOM FOREST
# ------------------------------------------------------------
class CascadeRandomForest(ClassifierMixin, BaseEstimator):
    """
    Cascade Random Forest: Trees arranged in cascade where each layer 
    focuses on instances misclassified by previous layer
    """
    
    def __init__(self, n_layers=3, n_estimators_per_layer=50, 
                 max_depth=15, min_samples_split=5, random_state=42):
        self.n_layers = n_layers
        self.n_estimators_per_layer = n_estimators_per_layer
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.random_state = random_state
        self.layers = []
        self.feature_importances_ = None
        
    def fit(self, X, y):
        X, y = check_X_y(X, y)
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        
        # Initialize feature importances
        n_features = X.shape[1]
        self.feature_importances_ = np.zeros(n_features)
        
        # First layer: Train on all data
        print(f"\n[Layer 1] Training Random Forest on all data...")
        rf_layer1 = RandomForestClassifier(
            n_estimators=self.n_estimators_per_layer,
            max_depth=self.max_depth,
            min_samples_split=self.min_samples_split,
            random_state=self.random_state,
            n_jobs=-1
        )
        rf_layer1.fit(X, y)
        self.layers.append(rf_layer1)
        
        # Update feature importances
        self.feature_importances_ += rf_layer1.feature_importances_
        
        # Get predictions from first layer
        y_pred_layer1 = rf_layer1.predict(X)
        
        # Identify misclassified instances
        misclassified_mask = (y_pred_layer1 != y)
        
        if misclassified_mask.sum() == 0:
            print("‚úì All instances correctly classified in first layer!")
            return self
        
        X_misclassified = X[misclassified_mask]
        y_misclassified = y[misclassified_mask]
        
        print(f"  Misclassified instances: {len(X_misclassified)}/{len(X)} "
              f"({len(X_misclassified)/len(X)*100:.1f}%)")
        
        # Subsequent layers: Focus on misclassified instances
        for layer_idx in range(1, self.n_layers):
            if len(X_misclassified) < 10:  # Stop if too few instances
                print(f"\nStopping early: Only {len(X_misclassified)} misclassified instances remain")
                break
            
            print(f"\n[Layer {layer_idx+1}] Training Random Forest on misclassified instances...")
            print(f"  Training on {len(X_misclassified)} instances")
            
            # Train new RF on misclassified instances
            rf_layer = RandomForestClassifier(
                n_estimators=self.n_estimators_per_layer,
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                random_state=self.random_state + layer_idx,
                n_jobs=-1
            )
            rf_layer.fit(X_misclassified, y_misclassified)
            self.layers.append(rf_layer)
            
            # Update feature importances
            self.feature_importances_ += rf_layer.feature_importances_
            
            # Get predictions for current misclassified set
            y_pred_layer = rf_layer.predict(X_misclassified)
            
            # Update misclassified mask
            new_misclassified_mask = (y_pred_layer != y_misclassified)
            
            if new_misclassified_mask.sum() == 0:
                print(f"‚úì All remaining instances correctly classified!")
                break
            
            X_misclassified = X_misclassified[new_misclassified_mask]
            y_misclassified = y_misclassified[new_misclassified_mask]
            
            print(f"  Still misclassified: {len(X_misclassified)} instances")
        
        # Normalize feature importances
        self.feature_importances_ /= len(self.layers)
        
        return self
    
    def predict_proba(self, X):
        check_is_fitted(self)
        X = check_array(X)
        
        # Initialize probability matrix
        proba = np.zeros((X.shape[0], self.n_classes_))
        
        # Get predictions from all layers with adaptive exponential weights
        total_weight = sum([2.0 ** (len(self.layers) - i - 1) for i in range(len(self.layers))])
        for i, layer in enumerate(self.layers):
            # Exponential weighting: first layer gets most weight
            layer_weight = (2.0 ** (len(self.layers) - i - 1)) / total_weight
            layer_proba = layer.predict_proba(X)
            
            # Ensure consistent class ordering
            layer_proba_aligned = np.zeros_like(proba)
            for cls_idx, cls in enumerate(self.classes_):
                if cls in layer.classes_:
                    class_idx_in_layer = np.where(layer.classes_ == cls)[0][0]
                    layer_proba_aligned[:, cls_idx] = layer_proba[:, class_idx_in_layer]
            
            proba += layer_weight * layer_proba_aligned
        
        # Normalize probabilities
        proba_sum = proba.sum(axis=1, keepdims=True)
        proba_sum[proba_sum == 0] = 1  # Avoid division by zero
        proba = proba / proba_sum
        
        return proba
    
    def predict(self, X):
        proba = self.predict_proba(X)
        return self.classes_[np.argmax(proba, axis=1)]

# ------------------------------------------------------------
# 1.2 HIERARCHICAL RANDOM FOREST
# ------------------------------------------------------------
class HierarchicalRandomForest(ClassifierMixin, BaseEstimator):
    """
    Hierarchical Random Forest: First layer clusters data, 
    second layer builds specialized forests per cluster
    """
    
    def __init__(self, n_clusters=3, n_estimators_global=50, 
                 n_estimators_local=30, max_depth=12, random_state=42):
        self.n_clusters = n_clusters
        self.n_estimators_global = n_estimators_global
        self.n_estimators_local = n_estimators_local
        self.max_depth = max_depth
        self.random_state = random_state
        self.global_rf = None
        self.cluster_models = {}
        self.kmeans = None
        self.feature_importances_ = None
        
    def fit(self, X, y):
        X, y = check_X_y(X, y)
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        
        print(f"\n[Hierarchical RF] Training with {self.n_clusters} clusters...")
        
        # Step 1: Cluster the feature space
        print("  Step 1: Clustering feature space...")
        self.kmeans = KMeans(
            n_clusters=self.n_clusters,
            random_state=self.random_state,
            n_init=10
        )
        clusters = self.kmeans.fit_predict(X)
        
        print(f"  Cluster distribution: {np.bincount(clusters)}")
        
        # Step 2: Train global Random Forest on all data
        print("  Step 2: Training global Random Forest...")
        self.global_rf = RandomForestClassifier(
            n_estimators=self.n_estimators_global,
            max_depth=self.max_depth,
            random_state=self.random_state,
            n_jobs=-1
        )
        self.global_rf.fit(X, y)
        
        # Initialize feature importances from global model
        self.feature_importances_ = self.global_rf.feature_importances_.copy()
        
        # Step 3: Train specialized Random Forest for each cluster
        print("  Step 3: Training specialized forests per cluster...")
        for cluster_id in range(self.n_clusters):
            cluster_mask = (clusters == cluster_id)
            
            if cluster_mask.sum() < 10:  # Skip if too few samples
                print(f"    Cluster {cluster_id}: Skipped (only {cluster_mask.sum()} samples)")
                continue
            
            X_cluster = X[cluster_mask]
            y_cluster = y[cluster_mask]
            
            print(f"    Cluster {cluster_id}: Training on {len(X_cluster)} samples "
                  f"({len(np.unique(y_cluster))} classes)")
            
            # Train specialized RF for this cluster
            cluster_rf = RandomForestClassifier(
                n_estimators=self.n_estimators_local,
                max_depth=self.max_depth,
                random_state=self.random_state + cluster_id,
                n_jobs=-1
            )
            cluster_rf.fit(X_cluster, y_cluster)
            self.cluster_models[cluster_id] = cluster_rf
            
            # Aggregate feature importances
            self.feature_importances_ += cluster_rf.feature_importances_
        
        # Average feature importances
        self.feature_importances_ /= (1 + len(self.cluster_models))
        
        return self
    
    def predict_proba(self, X):
        check_is_fitted(self)
        X = check_array(X)
        
        # Get cluster assignments
        clusters = self.kmeans.predict(X)
        
        # Initialize probability matrix
        proba = np.zeros((X.shape[0], self.n_classes_))
        
        # Global predictions
        global_proba = self.global_rf.predict_proba(X)
        
        # Align global probabilities with class ordering
        global_proba_aligned = np.zeros_like(proba)
        for cls_idx, cls in enumerate(self.classes_):
            if cls in self.global_rf.classes_:
                class_idx_in_global = np.where(self.global_rf.classes_ == cls)[0][0]
                global_proba_aligned[:, cls_idx] = global_proba[:, class_idx_in_global]
        
        # Weight for global vs local predictions (optimized for better accuracy)
        global_weight = 0.25  # 25% weight to global predictions
        local_weight = 0.75   # 75% weight to local predictions
        
        proba = global_weight * global_proba_aligned
        
        # Add local predictions for each cluster
        for cluster_id, cluster_model in self.cluster_models.items():
            cluster_mask = (clusters == cluster_id)
            
            if cluster_mask.sum() == 0:
                continue
            
            X_cluster = X[cluster_mask]
            cluster_proba = cluster_model.predict_proba(X_cluster)
            
            # Align cluster probabilities with class ordering
            cluster_proba_aligned = np.zeros((len(X_cluster), self.n_classes_))
            for cls_idx, cls in enumerate(self.classes_):
                if cls in cluster_model.classes_:
                    class_idx_in_cluster = np.where(cluster_model.classes_ == cls)[0][0]
                    cluster_proba_aligned[:, cls_idx] = cluster_proba[:, class_idx_in_cluster]
            
            proba[cluster_mask] += local_weight * cluster_proba_aligned
        
        # Normalize probabilities
        proba_sum = proba.sum(axis=1, keepdims=True)
        proba_sum[proba_sum == 0] = 1
        proba = proba / proba_sum
        
        return proba
    
    def predict(self, X):
        proba = self.predict_proba(X)
        return self.classes_[np.argmax(proba, axis=1)]

# ------------------------------------------------------------
# 1.3 TEMPORAL RANDOM FOREST (Season-aware)
# ------------------------------------------------------------
class TemporalRandomForest(ClassifierMixin, BaseEstimator):
    """
    Temporal Random Forest: Incorporates temporal/seasonal patterns
    Creates specialized forests for different seasons/time periods
    """
    
    def __init__(self, temporal_feature='Season', n_estimators_base=50, 
                 n_estimators_temporal=30, max_depth=12, random_state=42):
        self.temporal_feature = temporal_feature
        self.n_estimators_base = n_estimators_base
        self.n_estimators_temporal = n_estimators_temporal
        self.max_depth = max_depth
        self.random_state = random_state
        self.base_rf = None
        self.temporal_models = {}
        self.temporal_categories_ = None
        self.feature_importances_ = None
        
    def fit(self, X, y, temporal_data=None):
        """
        temporal_data: Series or array containing temporal information
                      (e.g., season, month, time period)
        """
        X, y = check_X_y(X, y)
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        
        # Use provided temporal data or extract from X if column exists
        if temporal_data is None:
            # Check if temporal feature is in column names
            if hasattr(X, 'columns') and self.temporal_feature in X.columns:
                temporal_data = X[self.temporal_feature]
                X = X.drop(columns=[self.temporal_feature])
            else:
                # If no temporal data provided, fall back to base RF
                print("Warning: No temporal data provided. Using base Random Forest.")
                self.base_rf = RandomForestClassifier(
                    n_estimators=self.n_estimators_base,
                    max_depth=self.max_depth,
                    random_state=self.random_state,
                    n_jobs=-1
                )
                self.base_rf.fit(X, y)
                self.feature_importances_ = self.base_rf.feature_importances_
                return self
        
        # Convert temporal data to array if needed
        if hasattr(temporal_data, 'values'):
            temporal_data = temporal_data.values
        
        print(f"\n[Temporal RF] Training with temporal feature '{self.temporal_feature}'...")
        
        # Get unique temporal categories
        self.temporal_categories_ = np.unique(temporal_data)
        print(f"  Temporal categories: {self.temporal_categories_}")
        
        # Step 1: Train base Random Forest on all data
        print("  Step 1: Training base Random Forest...")
        self.base_rf = RandomForestClassifier(
            n_estimators=self.n_estimators_base,
            max_depth=self.max_depth,
            random_state=self.random_state,
            n_jobs=-1
        )
        self.base_rf.fit(X, y)
        
        # Initialize feature importances
        self.feature_importances_ = self.base_rf.feature_importances_.copy()
        
        # Step 2: Train specialized Random Forest for each temporal category
        print("  Step 2: Training specialized forests per temporal category...")
        for category in self.temporal_categories_:
            category_mask = (temporal_data == category)
            
            if category_mask.sum() < 10:  # Skip if too few samples
                print(f"    {self.temporal_feature} '{category}': "
                      f"Skipped (only {category_mask.sum()} samples)")
                continue
            
            X_category = X[category_mask]
            y_category = y[category_mask]
            
            print(f"    {self.temporal_feature} '{category}': "
                  f"Training on {len(X_category)} samples "
                  f"({len(np.unique(y_category))} classes)")
            
            # Train specialized RF for this temporal category
            category_rf = RandomForestClassifier(
                n_estimators=self.n_estimators_temporal,
                max_depth=self.max_depth,
                random_state=self.random_state + hash(category) % 1000,
                n_jobs=-1
            )
            category_rf.fit(X_category, y_category)
            self.temporal_models[category] = category_rf
            
            # Aggregate feature importances
            self.feature_importances_ += category_rf.feature_importances_
        
        # Average feature importances
        self.feature_importances_ /= (1 + len(self.temporal_models))
        
        return self
    
    def predict_proba(self, X, temporal_data=None):
        check_is_fitted(self)
        X = check_array(X)
        
        # If no temporal models trained (fallback case)
        if not self.temporal_models:
            return self.base_rf.predict_proba(X)
        
        # Get temporal data for predictions
        if temporal_data is None:
            if hasattr(X, 'columns') and self.temporal_feature in X.columns:
                temporal_data = X[self.temporal_feature].values
                X = X.drop(columns=[self.temporal_feature])
            else:
                # If no temporal data for prediction, use base model only
                return self.base_rf.predict_proba(X)
        
        # Initialize probability matrix
        proba = np.zeros((X.shape[0], self.n_classes_))
        
        # Base predictions
        base_proba = self.base_rf.predict_proba(X)
        
        # Align base probabilities with class ordering
        base_proba_aligned = np.zeros_like(proba)
        for cls_idx, cls in enumerate(self.classes_):
            if cls in self.base_rf.classes_:
                class_idx_in_base = np.where(self.base_rf.classes_ == cls)[0][0]
                base_proba_aligned[:, cls_idx] = base_proba[:, class_idx_in_base]
        
        # Weight for base vs temporal predictions (optimized for better accuracy)
        base_weight = 0.3  # 30% weight to base predictions
        temporal_weight = 0.7  # 70% weight to temporal predictions
        
        proba = base_weight * base_proba_aligned
        
        # Group by temporal categories
        unique_categories = np.unique(temporal_data)
        
        for category in unique_categories:
            category_mask = (temporal_data == category)
            
            if category_mask.sum() == 0 or category not in self.temporal_models:
                continue
            
            X_category = X[category_mask]
            category_model = self.temporal_models[category]
            
            if len(X_category) == 0:
                continue
            
            category_proba = category_model.predict_proba(X_category)
            
            # Align category probabilities with class ordering
            category_proba_aligned = np.zeros((len(X_category), self.n_classes_))
            for cls_idx, cls in enumerate(self.classes_):
                if cls in category_model.classes_:
                    class_idx_in_category = np.where(category_model.classes_ == cls)[0][0]
                    category_proba_aligned[:, cls_idx] = category_proba[:, class_idx_in_category]
            
            proba[category_mask] += temporal_weight * category_proba_aligned
        
        # Normalize probabilities
        proba_sum = proba.sum(axis=1, keepdims=True)
        proba_sum[proba_sum == 0] = 1
        proba = proba / proba_sum
        
        return proba
    
    def predict(self, X, temporal_data=None):
        proba = self.predict_proba(X, temporal_data)
        return self.classes_[np.argmax(proba, axis=1)]

# ============================================================
# 2. DATA LOADING AND PREPROCESSING
# ============================================================

print("\n" + "="*80)
print("[2] DATA LOADING AND PREPROCESSING")
print("="*80)

# Load dataset
try:
    data = pd.read_csv('/kaggle/input/crop-dataset/crop_dataset.csv')
    print(f"‚úì Dataset loaded successfully")
    print(f"  Shape: {data.shape[0]} rows √ó {data.shape[1]} columns")
    
    # Display basic info
    print("\nDataset info:")
    print(data.info())
    print(f"\nCrop distribution:")
    print(data['Crop Name'].value_counts())
    
except FileNotFoundError:
    # Create synthetic data for testing if file not found
    print("‚úó File not found. Creating synthetic data for testing...")
    n_samples = 1000
    data = pd.DataFrame({
        'Area': np.random.uniform(10000, 500000, n_samples),
        'AP Ratio': np.random.uniform(0.5, 1.5, n_samples),
        'District': np.random.choice(['District_A', 'District_B', 'District_C'], n_samples),
        'Season': np.random.choice(['Kharif', 'Rabi', 'Summer'], n_samples),
        'Avg Temp': np.random.uniform(20, 35, n_samples),
        'Avg Humidity': np.random.uniform(50, 90, n_samples),
        'Max Temp': np.random.uniform(30, 45, n_samples),
        'Min Temp': np.random.uniform(10, 25, n_samples),
        'Max Relative Humidity': np.random.uniform(60, 95, n_samples),
        'Min Relative Humidity': np.random.uniform(40, 80, n_samples),
        'Transplant': np.random.choice(['Jan', 'Feb', 'Mar', 'Apr'], n_samples),
        'Growth': np.random.choice(['Spring', 'Summer', 'Fall'], n_samples),
        'Harvest': np.random.choice(['Winter', 'Spring'], n_samples),
        'Crop Name': np.random.choice(['Rice', 'Wheat', 'Maize', 'Cotton', 'Sugarcane'], n_samples)
    })

# Data preprocessing
print("\n[2.1] Preprocessing data...")

# Clean data - replace Excel error strings with NaN
print("  Cleaning error values...")
error_values = ['#DIV/0!', '#N/A', '#VALUE!', '#REF!', '#NAME?', '#NUM!', '#NULL!']
for col in data.columns:
    if data[col].dtype == 'object':
        # Replace error strings with NaN
        data[col] = data[col].replace(error_values, np.nan)
        # Try to convert to numeric if possible
        try:
            data[col] = pd.to_numeric(data[col], errors='ignore')
        except:
            pass

# Drop rows with missing values
data_cleaned = data.dropna()
if len(data) != len(data_cleaned):
    print(f"  Removed {len(data) - len(data_cleaned)} rows with missing values/errors")
    data = data_cleaned

# Encode categorical variables
label_encoders = {}
categorical_cols = ['District', 'Season', 'Crop Name', 'Transplant', 'Growth', 'Harvest']

for col in categorical_cols:
    if col in data.columns:
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col])
        label_encoders[col] = le
        print(f"  Encoded '{col}' with {len(le.classes_)} classes")

# Feature engineering
print("\n[2.2] Feature engineering...")
if 'Max Temp' in data.columns and 'Min Temp' in data.columns:
    data['Temp_Range'] = data['Max Temp'] - data['Min Temp']
    print("  Added: Temp_Range")

if 'Max Relative Humidity' in data.columns and 'Min Relative Humidity' in data.columns:
    data['Humidity_Range'] = data['Max Relative Humidity'] - data['Min Relative Humidity']
    print("  Added: Humidity_Range")

if 'Avg Temp' in data.columns and 'Avg Humidity' in data.columns:
    data['Temp_Humidity_Index'] = data['Avg Temp'] * data['Avg Humidity'] / 100
    print("  Added: Temp_Humidity_Index")

# Prepare features and target
X = data.drop(['Crop Name'], axis=1)
y = data['Crop Name']

# Get class names for visualization
if 'Crop Name' in label_encoders:
    class_names = label_encoders['Crop Name'].classes_
else:
    class_names = [f'Class {i}' for i in range(len(np.unique(y)))]

# Store season data before splitting (for Temporal RF)
season_data = None
if 'Season' in data.columns:
    season_data = data['Season'].values.copy()

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data (including season data if available)
if season_data is not None:
    X_train, X_test, y_train, y_test, season_train, season_test = train_test_split(
        X_scaled, y, season_data, test_size=0.2, random_state=SEED, stratify=y
    )
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=SEED, stratify=y
    )
    season_train, season_test = None, None

print(f"\n[2.3] Data split:")
print(f"  Training set: {X_train.shape[0]} samples")
print(f"  Test set: {X_test.shape[0]} samples")
print(f"  Features: {X_train.shape[1]}")
print(f"  Classes: {len(np.unique(y))}")

# ============================================================
# 3. MODEL TRAINING AND EVALUATION
# ============================================================

print("\n" + "="*80)
print("[3] TRAINING AND EVALUATING MODELS")
print("="*80)

# Define models to compare
models = {
    'Standard Random Forest': RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        random_state=SEED,
        n_jobs=-1,
        verbose=0
    ),
    'Cascade Random Forest': CascadeRandomForest(
        n_layers=4,
        n_estimators_per_layer=80,
        max_depth=18,
        random_state=SEED
    ),
    'Hierarchical Random Forest': HierarchicalRandomForest(
        n_clusters=5,
        n_estimators_global=80,
        n_estimators_local=60,
        max_depth=18,
        random_state=SEED
    ),
    'Temporal Random Forest': TemporalRandomForest(
        temporal_feature='Season',
        n_estimators_base=80,
        n_estimators_temporal=70,
        max_depth=18,
        random_state=SEED
    )
}

# For Temporal RF, we need temporal data (already split above)
temporal_data_train = season_train
temporal_data_test = season_test

# Train and evaluate models
results = {}
predictions = {}
training_times = {}

print("\n[3.1] Training models...")
print("-" * 60)

for name, model in tqdm(models.items(), desc="Models"):
    print(f"\n{'='*50}")
    print(f"Training: {name}")
    print(f"{'='*50}")
    
    start_time = time.time()
    
    try:
        # Special handling for Temporal RF
        if name == 'Temporal Random Forest' and temporal_data_train is not None:
            model.fit(X_train, y_train, temporal_data=temporal_data_train)
        else:
            model.fit(X_train, y_train)
        
        train_time = time.time() - start_time
        training_times[name] = train_time
        
        # Make predictions
        if name == 'Temporal Random Forest' and temporal_data_test is not None:
            y_pred = model.predict(X_test, temporal_data=temporal_data_test)
            y_pred_proba = model.predict_proba(X_test, temporal_data=temporal_data_test)
        else:
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)
        
        predictions[name] = (y_pred, y_pred_proba)
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # Cross-validation
        print(f"  Performing cross-validation...")
        
        # Special CV for Temporal RF
        if name == 'Temporal Random Forest':
            # Custom CV that preserves temporal structure
            cv_scores = []
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)
            
            # Use season_data for CV if available
            if season_data is not None:
                for train_idx, val_idx in skf.split(X_scaled, y):
                    X_cv_train, X_cv_val = X_scaled[train_idx], X_scaled[val_idx]
                    y_cv_train, y_cv_val = y.iloc[train_idx], y.iloc[val_idx]
                    
                    # Get temporal data for CV fold
                    temporal_cv_train = season_data[train_idx]
                    temporal_cv_val = season_data[val_idx]
                    
                    # Train and evaluate
                    model_cv = clone(model)
                    model_cv.fit(X_cv_train, y_cv_train, temporal_data=temporal_cv_train)
                    y_cv_pred = model_cv.predict(X_cv_val, temporal_data=temporal_cv_val)
                    
                    cv_scores.append(accuracy_score(y_cv_val, y_cv_pred))
            else:
                # Fallback to standard CV if no season data
                cv_scores = cross_val_score(
                    model, X_scaled, y, cv=3, scoring='accuracy', n_jobs=-1
                )
            
            cv_accuracy_mean = np.mean(cv_scores)
            cv_accuracy_std = np.std(cv_scores)
            
        else:
            # Standard CV for other models
            cv_scores = cross_val_score(
                model, X_scaled, y, cv=3, scoring='accuracy', n_jobs=-1
            )
            cv_accuracy_mean = cv_scores.mean()
            cv_accuracy_std = cv_scores.std()
                
        # Store results
        results[name] = {
            'Test Accuracy': accuracy,
            'Test Precision': precision,
            'Test Recall': recall,
            'Test F1': f1,
            'CV Accuracy Mean': cv_accuracy_mean,
            'CV Accuracy Std': cv_accuracy_std,
            'Training Time': train_time
        }
        
        print(f"  ‚úì Training completed in {train_time:.2f}s")
        print(f"  Test Accuracy: {accuracy:.4f}")
        print(f"  CV Accuracy: {cv_accuracy_mean:.4f} ¬± {cv_accuracy_std:.4f}")
        
        # Print feature importance if available
        if hasattr(model, 'feature_importances_'):
            top_features = np.argsort(model.feature_importances_)[-5:][::-1]
            print(f"  Top 5 features by importance: {top_features}")
        
    except Exception as e:
        print(f"  ‚úó Error training {name}: {str(e)}")
        results[name] = {
            'Test Accuracy': 0,
            'Test Precision': 0,
            'Test Recall': 0,
            'Test F1': 0,
            'CV Accuracy Mean': 0,
            'CV Accuracy Std': 0,
            'Training Time': 0
        }

# ============================================================
# 4. RESULTS COMPARISON
# ============================================================

print("\n" + "="*80)
print("[4] RESULTS COMPARISON")
print("="*80)

# Create results DataFrame
results_df = pd.DataFrame(results).T
results_df = results_df.sort_values('Test Accuracy', ascending=False)

print("\n[4.1] Performance Comparison:")
print("-" * 80)
print(results_df.round(4).to_string())

# Visualize results with research-worthy styling
sns.set_style("whitegrid")
research_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E', '#BC4B51']
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.patch.set_facecolor('white')

# 1. Test Accuracy Comparison
ax1 = axes[0, 0]
models_sorted = results_df.index.tolist()
accuracies = results_df['Test Accuracy'].values
bars1 = ax1.barh(models_sorted, accuracies, color=research_colors[:len(models_sorted)], edgecolor='black', linewidth=1.2)
ax1.set_xlabel('Accuracy', fontsize=12)
ax1.set_title('Test Accuracy Comparison', fontsize=14)
ax1.axvline(x=accuracies.max(), color='#C73E1D', linestyle='--', alpha=0.7, linewidth=2)
ax1.grid(False)
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)

# 2. CV Accuracy with Error Bars
ax2 = axes[0, 1]
cv_means = results_df['CV Accuracy Mean'].values
cv_stds = results_df['CV Accuracy Std'].values
y_pos = np.arange(len(models_sorted))
bars2 = ax2.barh(y_pos, cv_means, xerr=cv_stds, color=research_colors[:len(models_sorted)], 
                 capsize=5, edgecolor='black', linewidth=1.2, ecolor='#333333', error_kw={'linewidth': 2})
ax2.set_yticks(y_pos)
ax2.set_yticklabels(models_sorted)
ax2.set_xlabel('Accuracy', fontsize=12)
ax2.set_title('Cross-Validation Accuracy (¬± std)', fontsize=14)
ax2.grid(False)
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)

# 3. Training Time Comparison
ax3 = axes[0, 2]
training_times_vals = results_df['Training Time'].values
bars3 = ax3.barh(models_sorted, training_times_vals, color=research_colors[:len(models_sorted)], 
                 edgecolor='black', linewidth=1.2)
ax3.set_xlabel('Time (seconds)', fontsize=12)
ax3.set_title('Training Time Comparison', fontsize=14)
ax3.grid(False)
ax3.spines['top'].set_visible(False)
ax3.spines['right'].set_visible(False)

# 4. F1-Score Comparison
ax4 = axes[1, 0]
f1_scores = results_df['Test F1'].values
bars4 = ax4.barh(models_sorted, f1_scores, color=research_colors[:len(models_sorted)], 
                 edgecolor='black', linewidth=1.2)
ax4.set_xlabel('F1-Score', fontsize=12)
ax4.set_title('Test F1-Score Comparison', fontsize=14)
ax4.axvline(x=f1_scores.max(), color='#C73E1D', linestyle='--', alpha=0.7, linewidth=2)
ax4.grid(False)
ax4.spines['top'].set_visible(False)
ax4.spines['right'].set_visible(False)

# 5. Precision-Recall Tradeoff
ax5 = axes[1, 1]
precision = results_df['Test Precision'].values
recall = results_df['Test Recall'].values
for i, model in enumerate(models_sorted):
    ax5.scatter(precision[i], recall[i], s=200, alpha=0.8, 
                color=research_colors[i % len(research_colors)], 
                edgecolor='black', linewidth=1.5, zorder=3)
    ax5.annotate(model, (precision[i], recall[i]), xytext=(8, 8), 
                 textcoords='offset points', fontsize=9,
                 bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='gray', alpha=0.8))
ax5.set_xlabel('Precision', fontsize=12)
ax5.set_ylabel('Recall', fontsize=12)
ax5.set_title('Precision-Recall Tradeoff', fontsize=14)
ax5.grid(False)
ax5.spines['top'].set_visible(False)
ax5.spines['right'].set_visible(False)

# 6. Accuracy vs Training Time
ax6 = axes[1, 2]
for i, model in enumerate(models_sorted):
    ax6.scatter(training_times_vals[i], accuracies[i], s=200, alpha=0.8,
                color=research_colors[i % len(research_colors)], 
                edgecolor='black', linewidth=1.5, zorder=3)
    ax6.annotate(model, (training_times_vals[i], accuracies[i]), 
                 xytext=(8, 8), textcoords='offset points', fontsize=9,
                 bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='gray', alpha=0.8))
ax6.set_xlabel('Training Time (seconds)', fontsize=12)
ax6.set_ylabel('Test Accuracy', fontsize=12)
ax6.set_title('Accuracy vs Training Time', fontsize=14)
ax6.grid(False)
ax6.spines['top'].set_visible(False)
ax6.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig('model_comparison.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# ============================================================
# 4.2 COMPREHENSIVE EVALUATION METRICS
# ============================================================

print("\n" + "="*80)
print("[4.2] COMPREHENSIVE EVALUATION METRICS - ALL CATEGORIES")
print("="*80)

# Helper function to calculate comprehensive metrics
def calculate_comprehensive_metrics(y_true, y_pred, y_proba):
    """Calculate all classification metrics from the comprehensive list"""
    
    # Binarize for multiclass metrics
    y_true_bin = label_binarize(y_true, classes=np.unique(y_true))
    n_classes = len(np.unique(y_true))
    
    # Get confusion matrix elements
    cm = confusion_matrix(y_true, y_pred)
    
    # Calculate per-class metrics for averaging
    per_class_metrics = {}
    for i in range(n_classes):
        y_true_binary = (y_true == i).astype(int)
        y_pred_binary = (y_pred == i).astype(int)
        
        TP = np.sum((y_true_binary == 1) & (y_pred_binary == 1))
        TN = np.sum((y_true_binary == 0) & (y_pred_binary == 0))
        FP = np.sum((y_true_binary == 0) & (y_pred_binary == 1))
        FN = np.sum((y_true_binary == 1) & (y_pred_binary == 0))
        
        per_class_metrics[i] = {
            'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN
        }
    
    # Aggregate TP, TN, FP, FN for overall metrics
    total_TP = sum([per_class_metrics[i]['TP'] for i in range(n_classes)])
    total_TN = sum([per_class_metrics[i]['TN'] for i in range(n_classes)])
    total_FP = sum([per_class_metrics[i]['FP'] for i in range(n_classes)])
    total_FN = sum([per_class_metrics[i]['FN'] for i in range(n_classes)])
    
    metrics = {}
    
    # ===== 1. CONFUSION MATRIX-BASED METRICS =====
    metrics['Accuracy'] = accuracy_score(y_true, y_pred)
    metrics['Error Rate'] = 1 - metrics['Accuracy']
    metrics['Precision (Macro)'] = precision_score(y_true, y_pred, average='macro', zero_division=0)
    metrics['Recall (Macro)'] = recall_score(y_true, y_pred, average='macro', zero_division=0)
    metrics['Specificity (Macro)'] = np.mean([per_class_metrics[i]['TN'] / (per_class_metrics[i]['TN'] + per_class_metrics[i]['FP'] + 1e-10) for i in range(n_classes)])
    metrics['FPR (Macro)'] = 1 - metrics['Specificity (Macro)']
    metrics['FNR (Macro)'] = 1 - metrics['Recall (Macro)']
    metrics['NPV (Macro)'] = np.mean([per_class_metrics[i]['TN'] / (per_class_metrics[i]['TN'] + per_class_metrics[i]['FN'] + 1e-10) for i in range(n_classes)])
    metrics['FDR (Macro)'] = 1 - metrics['Precision (Macro)']
    metrics['FOR (Macro)'] = 1 - metrics['NPV (Macro)']
    metrics['Prevalence'] = np.sum([per_class_metrics[i]['TP'] + per_class_metrics[i]['FN'] for i in range(n_classes)]) / len(y_true)
    metrics['Detection Rate'] = total_TP / len(y_true)
    metrics['Detection Prevalence'] = (total_TP + total_FP) / len(y_true)
    metrics['Balanced Accuracy'] = balanced_accuracy_score(y_true, y_pred)
    
    # ===== 2. COMPOSITE / HARMONIC METRICS =====
    metrics['F1-Score (Macro)'] = f1_score(y_true, y_pred, average='macro', zero_division=0)
    metrics['F1-Score (Micro)'] = f1_score(y_true, y_pred, average='micro', zero_division=0)
    metrics['F1-Score (Weighted)'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    
    # F-beta scores
    metrics['F0.5-Score (Macro)'] = fbeta_score(y_true, y_pred, beta=0.5, average='macro', zero_division=0)
    metrics['F2-Score (Macro)'] = fbeta_score(y_true, y_pred, beta=2.0, average='macro', zero_division=0)
    
    # G-Mean (geometric mean of sensitivity and specificity)
    sensitivity = metrics['Recall (Macro)']
    specificity = metrics['Specificity (Macro)']
    metrics['G-Mean'] = np.sqrt(sensitivity * specificity)
    
    metrics['Matthews Corr Coef (MCC)'] = matthews_corrcoef(y_true, y_pred)
    
    # Youden's J Statistic
    metrics["Youden's J Statistic"] = sensitivity + specificity - 1
    
    metrics['Jaccard Index (Macro)'] = jaccard_score(y_true, y_pred, average='macro', zero_division=0)
    
    # ===== 3. PROBABILITY & RANKING-BASED METRICS =====
    try:
        # Ensure y_proba has correct shape
        if y_proba.shape[1] == n_classes:
            metrics['ROC-AUC (Macro)'] = roc_auc_score(y_true_bin, y_proba, average='macro', multi_class='ovr')
            metrics['ROC-AUC (Weighted)'] = roc_auc_score(y_true_bin, y_proba, average='weighted', multi_class='ovr')
        else:
            metrics['ROC-AUC (Macro)'] = np.nan
            metrics['ROC-AUC (Weighted)'] = np.nan
    except:
        metrics['ROC-AUC (Macro)'] = np.nan
        metrics['ROC-AUC (Weighted)'] = np.nan
    
    metrics['Log Loss'] = log_loss(y_true, y_proba)
    
    # Brier Score (multiclass)
    try:
        y_proba_clipped = np.clip(y_proba, 1e-10, 1 - 1e-10)
        brier_scores = []
        for i in range(n_classes):
            y_true_binary = (y_true == i).astype(int)
            brier = np.mean((y_proba_clipped[:, i] - y_true_binary) ** 2)
            brier_scores.append(brier)
        metrics['Brier Score (Mean)'] = np.mean(brier_scores)
    except:
        metrics['Brier Score (Mean)'] = np.nan
    
    # Average Precision
    try:
        if y_proba.shape[1] == n_classes:
            metrics['Average Precision (Macro)'] = average_precision_score(y_true_bin, y_proba, average='macro')
        else:
            metrics['Average Precision (Macro)'] = np.nan
    except:
        metrics['Average Precision (Macro)'] = np.nan
    
    # ===== 4. MULTICLASS CLASSIFICATION METRICS =====
    metrics['Precision (Micro)'] = precision_score(y_true, y_pred, average='micro', zero_division=0)
    metrics['Precision (Weighted)'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)
    metrics['Recall (Micro)'] = recall_score(y_true, y_pred, average='micro', zero_division=0)
    metrics['Recall (Weighted)'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)
    metrics["Cohen's Kappa"] = cohen_kappa_score(y_true, y_pred)
    
    # Top-k Accuracy (top-3)
    try:
        top_3_pred = np.argsort(y_proba, axis=1)[:, -3:]
        top_3_accuracy = np.mean([y_true.iloc[i] if hasattr(y_true, 'iloc') else y_true[i] in top_3_pred[i] for i in range(len(y_true))])
        metrics['Top-3 Accuracy'] = top_3_accuracy
    except:
        metrics['Top-3 Accuracy'] = np.nan
    
    # Hamming Loss (multiclass)
    metrics['Hamming Loss'] = 1 - metrics['Accuracy']
    
    return metrics

# Calculate comprehensive metrics for all models
print("\nCalculating comprehensive metrics for all models...")
comprehensive_metrics = {}
for name in models_sorted:
    if name in predictions:
        print(f"  Processing {name}...")
        y_pred, y_proba = predictions[name]
        comprehensive_metrics[name] = calculate_comprehensive_metrics(y_test, y_pred, y_proba)

# Create comprehensive metrics DataFrame
comprehensive_df = pd.DataFrame(comprehensive_metrics).T

# Organize metrics by category
print("\n" + "="*80)
print("CATEGORY 1: CONFUSION MATRIX-BASED METRICS")
print("="*80)
confusion_metrics = [
    'Accuracy', 'Error Rate', 'Precision (Macro)', 'Recall (Macro)', 
    'Specificity (Macro)', 'FPR (Macro)', 'FNR (Macro)', 'NPV (Macro)',
    'FDR (Macro)', 'FOR (Macro)', 'Prevalence', 'Detection Rate', 
    'Detection Prevalence', 'Balanced Accuracy'
]
print(comprehensive_df[confusion_metrics].round(4).to_string())

print("\n" + "="*80)
print("CATEGORY 2: COMPOSITE / HARMONIC METRICS")
print("="*80)
composite_metrics = [
    'F1-Score (Macro)', 'F1-Score (Micro)', 'F1-Score (Weighted)',
    'F0.5-Score (Macro)', 'F2-Score (Macro)', 'G-Mean',
    'Matthews Corr Coef (MCC)', "Youden's J Statistic", 'Jaccard Index (Macro)'
]
print(comprehensive_df[composite_metrics].round(4).to_string())

print("\n" + "="*80)
print("CATEGORY 3: PROBABILITY & RANKING-BASED METRICS")
print("="*80)
probability_metrics = [
    'ROC-AUC (Macro)', 'ROC-AUC (Weighted)', 'Log Loss', 
    'Brier Score (Mean)', 'Average Precision (Macro)'
]
print(comprehensive_df[probability_metrics].round(4).to_string())

print("\n" + "="*80)
print("CATEGORY 4: MULTICLASS CLASSIFICATION METRICS")
print("="*80)
multiclass_metrics = [
    'Precision (Micro)', 'Precision (Weighted)', 'Recall (Micro)', 
    'Recall (Weighted)', "Cohen's Kappa", 'Top-3 Accuracy', 'Hamming Loss'
]
print(comprehensive_df[multiclass_metrics].round(4).to_string())

# Save comprehensive metrics to CSV
comprehensive_df.to_csv('comprehensive_metrics_all_categories.csv')
print("\n‚úì Comprehensive metrics saved to: comprehensive_metrics_all_categories.csv")

# ===== VISUALIZATION OF COMPREHENSIVE METRICS =====
print("\n[4.2.1] Generating Comprehensive Metrics Visualizations...")

# Create heatmap of key metrics
fig, axes = plt.subplots(2, 2, figsize=(18, 14), facecolor='white')

# Subplot A: Confusion Matrix-Based Metrics
ax = axes[0, 0]
cm_metrics = ['Accuracy', 'Balanced Accuracy', 'Precision (Macro)', 'Recall (Macro)', 
              'Specificity (Macro)', 'NPV (Macro)']
cm_data = comprehensive_df[cm_metrics].T
sns.heatmap(cm_data, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax, 
            cbar_kws={'label': 'Score'}, vmin=0.8, vmax=1.0, linewidths=0.5)
ax.set_title('A) Confusion Matrix-Based Metrics', fontsize=12, pad=10)
ax.set_xlabel('Models', fontsize=10)
ax.set_ylabel('Metrics', fontsize=10)
plt.setp(ax.get_xticklabels(), rotation=45, ha='right')

# Subplot B: Composite Metrics
ax = axes[0, 1]
comp_metrics = ['F1-Score (Macro)', 'F1-Score (Weighted)', 'G-Mean', 
                'Matthews Corr Coef (MCC)', 'Jaccard Index (Macro)', "Youden's J Statistic"]
comp_data = comprehensive_df[comp_metrics].T
sns.heatmap(comp_data, annot=True, fmt='.3f', cmap='viridis', ax=ax, 
            cbar_kws={'label': 'Score'}, vmin=0.7, vmax=1.0, linewidths=0.5)
ax.set_title('B) Composite & Harmonic Metrics', fontsize=12, pad=10)
ax.set_xlabel('Models', fontsize=10)
ax.set_ylabel('Metrics', fontsize=10)
plt.setp(ax.get_xticklabels(), rotation=45, ha='right')

# Subplot C: Probability-Based Metrics
ax = axes[1, 0]
prob_metrics = ['ROC-AUC (Macro)', 'Average Precision (Macro)', 'Log Loss', 'Brier Score (Mean)']
prob_data = comprehensive_df[prob_metrics].T
# Normalize log loss and brier score (inverse since lower is better)
prob_data_viz = prob_data.copy()
if 'Log Loss' in prob_data_viz.index:
    max_ll = prob_data_viz.loc['Log Loss'].max()
    prob_data_viz.loc['Log Loss'] = 1 - (prob_data_viz.loc['Log Loss'] / (max_ll + 0.1))
if 'Brier Score (Mean)' in prob_data_viz.index:
    max_bs = prob_data_viz.loc['Brier Score (Mean)'].max()
    prob_data_viz.loc['Brier Score (Mean)'] = 1 - (prob_data_viz.loc['Brier Score (Mean)'] / (max_bs + 0.1))
sns.heatmap(prob_data_viz, annot=True, fmt='.3f', cmap='plasma', ax=ax, 
            cbar_kws={'label': 'Score (normalized)'}, vmin=0.8, vmax=1.0, linewidths=0.5)
ax.set_title('C) Probability & Ranking-Based Metrics', fontsize=12, pad=10)
ax.set_xlabel('Models', fontsize=10)
ax.set_ylabel('Metrics', fontsize=10)
plt.setp(ax.get_xticklabels(), rotation=45, ha='right')

# Subplot D: Multiclass Metrics Comparison
ax = axes[1, 1]
multi_metrics = ['Precision (Micro)', 'Recall (Micro)', "Cohen's Kappa", 
                 'Top-3 Accuracy', 'Hamming Loss']
multi_data = comprehensive_df[multi_metrics].T
# Invert Hamming Loss
multi_data_viz = multi_data.copy()
if 'Hamming Loss' in multi_data_viz.index:
    multi_data_viz.loc['Hamming Loss'] = 1 - multi_data_viz.loc['Hamming Loss']
sns.heatmap(multi_data_viz, annot=True, fmt='.3f', cmap='coolwarm', ax=ax, 
            cbar_kws={'label': 'Score'}, vmin=0.8, vmax=1.0, linewidths=0.5)
ax.set_title('D) Multiclass Classification Metrics', fontsize=12, pad=10)
ax.set_xlabel('Models', fontsize=10)
ax.set_ylabel('Metrics', fontsize=10)
plt.setp(ax.get_xticklabels(), rotation=45, ha='right')

plt.suptitle('Comprehensive Classification Metrics Across All Categories', fontsize=14, y=0.995)
plt.tight_layout()
plt.savefig('comprehensive_metrics_heatmap.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# Create radar chart for top metrics comparison
print("  Creating radar chart for key metrics...")
fig = plt.figure(figsize=(16, 12), facecolor='white')
subplot_labels_radar = ['A)', 'B)', 'C)', 'D)']

for idx, model_name in enumerate(models_sorted[:4]):
    ax = fig.add_subplot(2, 2, idx+1, projection='polar')
    
    # Select key metrics for radar chart
    radar_metrics = ['Accuracy', 'Balanced Accuracy', 'F1-Score (Macro)', 
                     'Matthews Corr Coef (MCC)', 'G-Mean', "Cohen's Kappa"]
    values = comprehensive_df.loc[model_name, radar_metrics].values
    
    # Number of variables
    num_vars = len(radar_metrics)
    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
    values = values.tolist()
    
    # Close the plot
    values += values[:1]
    angles += angles[:1]
    
    # Plot
    ax.plot(angles, values, 'o-', linewidth=2, label=model_name, 
            color=research_colors[idx], alpha=0.8)
    ax.fill(angles, values, alpha=0.25, color=research_colors[idx])
    
    # Fix axis to go in the right order
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)
    
    # Set labels
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(radar_metrics, fontsize=9)
    ax.set_ylim(0.7, 1.0)
    ax.set_title(f'{subplot_labels_radar[idx]} {model_name}', fontsize=11, pad=20)
    ax.grid(True, alpha=0.3)

plt.suptitle('Key Performance Metrics - Radar Chart Comparison', fontsize=14, y=0.995)
plt.tight_layout()
plt.savefig('comprehensive_metrics_radar.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

print("\n‚úì Comprehensive metrics visualizations saved:")
print("    - comprehensive_metrics_heatmap.png")
print("    - comprehensive_metrics_radar.png")

# ============================================================
# 4.3 RESEARCH-GRADE EVALUATION PLOTS
# ============================================================

print("\n" + "="*80)
print("[4.3] RESEARCH-GRADE EVALUATION PLOTS")
print("="*80)

# Plotting Configuration for Research Paper Quality
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 12
plt.rcParams['figure.dpi'] = 600

print("\n[4.3] Generating Research-Grade Evaluation Plots...")

# PLOT 1: Performance Comparison with Values Inside Bars
print("  Plot 1: Performance Comparison with Values Inside Bars...")
fig, ax = plt.subplots(figsize=(14, 9), facecolor='white')

models_plot = models_sorted
metrics_to_plot = ['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1']
y_pos = np.arange(len(models_plot))
bar_height = 0.2

for i, metric in enumerate(metrics_to_plot):
    values = [results_df.loc[model, metric] for model in models_plot]
    bars = ax.barh([p + i * bar_height for p in y_pos], values, bar_height,
                   label=metric, color=research_colors[i], edgecolor='black', linewidth=1.5, alpha=0.85)
    
    # Add values inside bars
    for j, (bar, val) in enumerate(zip(bars, values)):
        width = bar.get_width()
        ax.text(width - 0.01, bar.get_y() + bar.get_height()/2, 
               f'{val:.4f}', ha='right', va='center', 
               fontsize=9, color='white')

ax.set_yticks([p + 1.5 * bar_height for p in y_pos])
ax.set_yticklabels(models_plot, fontsize=11)
ax.set_xlabel('Score', fontsize=13)
ax.set_title('Comprehensive Performance Comparison', fontsize=15, pad=15)
ax.legend(loc='lower right', fontsize=10, framealpha=0.95, edgecolor='black')
ax.set_xlim([0.90, 1.0])
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.grid(axis='x', alpha=0.3, linestyle='--', linewidth=0.8)
plt.tight_layout()
plt.savefig('performance_comparison_detailed.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# Plot 2: ROC Curves for Multi-Class (One-vs-Rest)
print("  Generating ROC curves...")
fig, axes = plt.subplots(2, 2, figsize=(16, 12), facecolor='white')
axes = axes.ravel()
subplot_labels_roc = ['A)', 'B)', 'C)', 'D)']

# Binarize the labels
y_test_bin = label_binarize(y_test, classes=np.unique(y))
n_classes = y_test_bin.shape[1]

colors = cycle(['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E', '#BC4B51'])

for idx, model_name in enumerate(models_sorted[:4]):  # Plot top 4 models
    if model_name in predictions:
        ax = axes[idx]
        _, y_pred_proba = predictions[model_name]
        
        try:
            # Ensure proba has correct shape
            if y_pred_proba.shape[1] != n_classes:
                # Align classes
                y_proba_aligned = np.zeros((len(y_test), n_classes))
                model = models[model_name]
                for i, cls in enumerate(np.unique(y)):
                    if cls in model.classes_:
                        cls_idx = np.where(model.classes_ == cls)[0][0]
                        y_proba_aligned[:, i] = y_pred_proba[:, cls_idx]
                y_pred_proba = y_proba_aligned
            
            # Compute ROC curve and ROC area for each class
            fpr = dict()
            tpr = dict()
            roc_auc = dict()
            
            for i in range(n_classes):
                fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])
                roc_auc[i] = auc(fpr[i], tpr[i])
            
            # Compute micro-average ROC curve
            y_test_flat = y_test_bin.ravel()
            y_proba_flat = y_pred_proba.ravel()
            min_len = min(len(y_test_flat), len(y_proba_flat))
            fpr["micro"], tpr["micro"], _ = roc_curve(y_test_flat[:min_len], y_proba_flat[:min_len])
            roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
            
            # Plot micro-average ROC curve
            ax.plot(fpr["micro"], tpr["micro"],
                   label=f'Micro-average (AUC = {roc_auc["micro"]:.3f})',
                   color='black', linestyle='--', linewidth=2.5, alpha=0.9)
            
            # Plot ROC curve for each class (only show first 5 to avoid clutter)
            for i, color in zip(range(min(5, n_classes)), colors):
                ax.plot(fpr[i], tpr[i], color=color, lw=2,
                       label=f'{class_names[i][:15]} (AUC = {roc_auc[i]:.2f})', alpha=0.7)
            
            ax.plot([0, 1], [0, 1], 'k:', lw=1.5, alpha=0.4, label='Random')
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate', fontsize=10)
            ax.set_ylabel('True Positive Rate', fontsize=10)
            ax.set_title(f'{subplot_labels_roc[idx]} {model_name}', fontsize=11, pad=10)
            ax.legend(loc="lower right", fontsize=8, framealpha=0.9)
            ax.grid(True, alpha=0.25, linestyle='--')
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            
        except Exception as e:
            print(f"    Error plotting ROC for {model_name}: {e}")
            ax.text(0.5, 0.5, f'Error: {str(e)[:80]}', 
                   transform=ax.transAxes, ha='center', va='center', fontsize=9,
                   bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))
            ax.set_title(f'{subplot_labels_roc[idx]} {model_name}', fontsize=11)

plt.suptitle('Multi-Class ROC Curves (One-vs-Rest)', fontsize=14, y=0.995)
plt.tight_layout()
plt.savefig('roc_curves.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# Plot 3: Model Complexity vs Performance
print("  Generating complexity vs performance plot...")
fig, ax = plt.subplots(figsize=(12, 8), facecolor='white')

# Estimate model complexity (for custom models, use proxy metrics)
complexity_metrics = {
    'Standard Random Forest': 100 * 15,  # n_estimators * max_depth
    'Cascade Random Forest': 80 * 18 * 4,  # estimators * depth * layers
    'Hierarchical Random Forest': (80 * 18) + (60 * 18 * 5),  # global + local * clusters
    'Temporal Random Forest': (80 * 18) + (70 * 18 * 3)  # base + temporal * categories (approx)
}

complexity_vals = [complexity_metrics.get(m, 0) for m in models_sorted]
accuracy_vals = results_df['Test Accuracy'].values

for i, model in enumerate(models_sorted):
    ax.scatter(complexity_vals[i], accuracy_vals[i], s=300, alpha=0.7,
              color=research_colors[i % len(research_colors)],
              edgecolor='black', linewidth=2, zorder=3)
    ax.annotate(model, (complexity_vals[i], accuracy_vals[i]), 
               xytext=(10, 10), textcoords='offset points', fontsize=10,
               bbox=dict(boxstyle='round,pad=0.5', facecolor='white', edgecolor='gray', alpha=0.9))

ax.set_xlabel('Model Complexity (Estimators √ó Depth √ó Components)', fontsize=12)
ax.set_ylabel('Test Accuracy', fontsize=12)
ax.set_title('Model Complexity vs Performance Tradeoff', fontsize=14)
ax.grid(True, alpha=0.2, linestyle='--')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('complexity_vs_performance.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# Plot 4: Calibration Curves
print("  Generating calibration curves...")
fig, axes = plt.subplots(2, 2, figsize=(14, 12), facecolor='white')
axes = axes.ravel()
subplot_labels_cal = ['A)', 'B)', 'C)', 'D)']

for idx, model_name in enumerate(models_sorted[:4]):
    if model_name in predictions:
        ax = axes[idx]
        _, y_pred_proba = predictions[model_name]
        
        try:
            # Ensure proba has correct shape
            if y_pred_proba.shape[1] != n_classes:
                y_proba_aligned = np.zeros((len(y_test), n_classes))
                model = models[model_name]
                for i, cls in enumerate(np.unique(y)):
                    if cls in model.classes_:
                        cls_idx = np.where(model.classes_ == cls)[0][0]
                        y_proba_aligned[:, i] = y_pred_proba[:, cls_idx]
                y_pred_proba = y_proba_aligned
            
            # Calculate calibration for each class and average
            mean_predicted_values = []
            fraction_of_positives_list = []
            
            for class_idx in range(min(3, n_classes)):  # Plot first 3 classes
                # Binary problem: class vs rest
                y_binary = (y_test == class_idx).astype(int)
                prob_pos = y_pred_proba[:, class_idx]
                
                fraction_of_positives, mean_predicted_value = calibration_curve(
                    y_binary, prob_pos, n_bins=10, strategy='uniform'
                )
                
                ax.plot(mean_predicted_value, fraction_of_positives, 
                       marker='o', linewidth=2, markersize=7, 
                       label=f'{class_names[class_idx][:15]}', alpha=0.8)
            
            # Plot perfectly calibrated line
            ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Calibration', alpha=0.5)
            
            ax.set_xlabel('Mean Predicted Probability', fontsize=10)
            ax.set_ylabel('Fraction of Positives', fontsize=10)
            ax.set_title(f'{subplot_labels_cal[idx]} {model_name}', fontsize=11, pad=10)
            ax.legend(loc='lower right', fontsize=9, framealpha=0.9)
            ax.grid(True, alpha=0.25, linestyle='--')
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            ax.set_xlim([0, 1])
            ax.set_ylim([0, 1])
            
        except Exception as e:
            print(f"    Error plotting calibration for {model_name}: {e}")
            ax.text(0.5, 0.5, f'Error: {str(e)[:80]}', 
                   transform=ax.transAxes, ha='center', va='center', fontsize=9,
                   bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))
            ax.set_title(f'{subplot_labels_cal[idx]} {model_name}', fontsize=11)

plt.suptitle('Model Calibration Curves', fontsize=14, y=0.995)
plt.tight_layout()
plt.savefig('calibration_curves.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

print("\\n‚úì Research-grade evaluation plots generated successfully!")
print("  Saved plots:")
print("    - performance_comparison_detailed.png")
print("    - roc_curves.png")
print("    - complexity_vs_performance.png")
print("    - calibration_curves.png")

# ============================================================
# NOTE: Following plots removed (too cluttered with many classes):
# - Confusion matrices (unreadable with 19 classes)
# - Per-class F1 heatmaps (too many classes)
# - Error analysis heatmaps (misclassification patterns unclear)
# - Radar charts (redundant with bar charts)
# ============================================================

# ============================================================
# 4.3 COMPREHENSIVE STATISTICAL ANALYSIS & VALIDATION
# ============================================================

print("\n" + "="*80)
print("[4.3] STATISTICAL TESTS AND ADVANCED CROSS-VALIDATION")
print("="*80)

# ----------------------------------------------------------------
# A. REPEATED STRATIFIED K-FOLD CROSS-VALIDATION
# ----------------------------------------------------------------
print("\n[A] Repeated Stratified K-Fold Cross-Validation (5x3)...")

repeated_cv_results = {}
rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=SEED)

for name, model in models.items():
    print(f"  Processing {name}...")
    try:
        if name == 'Temporal Random Forest' and season_data is not None:
            # Custom handling for temporal models
            scores = []
            for train_idx, val_idx in rskf.split(X_scaled, y):
                X_cv_train, X_cv_val = X_scaled[train_idx], X_scaled[val_idx]
                y_cv_train, y_cv_val = y.iloc[train_idx], y.iloc[val_idx]
                temporal_cv_train = season_data[train_idx]
                temporal_cv_val = season_data[val_idx]
                
                model_cv = clone(model)
                model_cv.fit(X_cv_train, y_cv_train, temporal_data=temporal_cv_train)
                y_cv_pred = model_cv.predict(X_cv_val, temporal_data=temporal_cv_val)
                scores.append(accuracy_score(y_cv_val, y_cv_pred))
            
            repeated_cv_results[name] = {
                'scores': np.array(scores),
                'mean': np.mean(scores),
                'std': np.std(scores),
                'min': np.min(scores),
                'max': np.max(scores),
                'median': np.median(scores)
            }
        else:
            cv_results = cross_validate(
                model, X_scaled, y, cv=rskf, 
                scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],
                return_train_score=True, n_jobs=-1
            )
            repeated_cv_results[name] = {
                'scores': cv_results['test_accuracy'],
                'mean': cv_results['test_accuracy'].mean(),
                'std': cv_results['test_accuracy'].std(),
                'min': cv_results['test_accuracy'].min(),
                'max': cv_results['test_accuracy'].max(),
                'median': np.median(cv_results['test_accuracy']),
                'train_mean': cv_results['train_accuracy'].mean(),
                'precision': cv_results['test_precision_weighted'].mean(),
                'recall': cv_results['test_recall_weighted'].mean(),
                'f1': cv_results['test_f1_weighted'].mean()
            }
    except Exception as e:
        print(f"    Error with {name}: {e}")
        repeated_cv_results[name] = {
            'scores': np.array([0]),
            'mean': 0, 'std': 0, 'min': 0, 'max': 0, 'median': 0
        }

print("\n  Repeated CV Results:")
print("  " + "-"*70)
for name in models_sorted:
    if name in repeated_cv_results:
        res = repeated_cv_results[name]
        print(f"  {name:30s}: {res['mean']:.4f} ¬± {res['std']:.4f} "
              f"[{res['min']:.4f}, {res['max']:.4f}]")

# ----------------------------------------------------------------
# B. BOOTSTRAP VALIDATION
# ----------------------------------------------------------------
print("\n[B] Bootstrap Validation (100 iterations)...")

bootstrap_results = {}
n_bootstrap = 100

for name in models_sorted[:3]:  # Top 3 models only (computational cost)
    if name in predictions:
        print(f"  Bootstrapping {name}...")
        y_pred, _ = predictions[name]
        bootstrap_scores = []
        
        for i in range(n_bootstrap):
            # Resample with replacement
            indices = resample(range(len(y_test)), n_samples=len(y_test), random_state=SEED+i)
            y_test_boot = y_test.iloc[indices]
            y_pred_boot = y_pred[indices]
            bootstrap_scores.append(accuracy_score(y_test_boot, y_pred_boot))
        
        bootstrap_results[name] = {
            'mean': np.mean(bootstrap_scores),
            'std': np.std(bootstrap_scores),
            'ci_lower': np.percentile(bootstrap_scores, 2.5),
            'ci_upper': np.percentile(bootstrap_scores, 97.5)
        }

print("\n  Bootstrap 95% Confidence Intervals:")
print("  " + "-"*70)
for name, res in bootstrap_results.items():
    print(f"  {name:30s}: {res['mean']:.4f} ¬± {res['std']:.4f} "
          f"CI: [{res['ci_lower']:.4f}, {res['ci_upper']:.4f}]")

# ----------------------------------------------------------------
# C. PAIRED STATISTICAL TESTS
# ----------------------------------------------------------------
print("\n[C] Paired Statistical Significance Tests...")

# Compare each custom model with Standard RF
baseline_name = 'Standard Random Forest'
if baseline_name in repeated_cv_results:
    baseline_scores = repeated_cv_results[baseline_name]['scores']
    
    print(f"\n  Comparing models against {baseline_name}:")
    print("  " + "-"*70)
    
    comparison_results = []
    
    for name in models_sorted:
        if name != baseline_name and name in repeated_cv_results:
            model_scores = repeated_cv_results[name]['scores']
            
            # Paired t-test
            t_stat, t_pvalue = stats.ttest_rel(model_scores, baseline_scores)
            
            # Wilcoxon signed-rank test (non-parametric alternative)
            w_stat, w_pvalue = stats.wilcoxon(model_scores, baseline_scores, 
                                              alternative='two-sided', zero_method='wilcox')
            
            # Effect size (Cohen's d)
            pooled_std = np.sqrt((np.std(model_scores)**2 + np.std(baseline_scores)**2) / 2)
            cohens_d = (np.mean(model_scores) - np.mean(baseline_scores)) / pooled_std if pooled_std > 0 else 0
            
            comparison_results.append({
                'model': name,
                'mean_diff': np.mean(model_scores) - np.mean(baseline_scores),
                't_stat': t_stat,
                't_pvalue': t_pvalue,
                'w_stat': w_stat,
                'w_pvalue': w_pvalue,
                'cohens_d': cohens_d
            })
            
            significance = "***" if t_pvalue < 0.001 else "**" if t_pvalue < 0.01 else "*" if t_pvalue < 0.05 else "ns"
            
            print(f"  {name:30s}")
            print(f"    Mean Difference: {np.mean(model_scores) - np.mean(baseline_scores):+.4f}")
            print(f"    Paired t-test: t={t_stat:.3f}, p={t_pvalue:.4f} {significance}")
            print(f"    Wilcoxon test: W={w_stat:.1f}, p={w_pvalue:.4f}")
            print(f"    Cohen's d: {cohens_d:.3f} ({'small' if abs(cohens_d) < 0.5 else 'medium' if abs(cohens_d) < 0.8 else 'large'} effect)")
            print()

# ----------------------------------------------------------------
# D. MCNEMAR'S TEST FOR PAIRWISE COMPARISON
# ----------------------------------------------------------------
print("\n[D] McNemar's Test (Pairwise Model Comparison)...")

# Create contingency tables and perform McNemar's test
mcnemar_results = []

for i, name1 in enumerate(models_sorted):
    for name2 in models_sorted[i+1:]:
        if name1 in predictions and name2 in predictions:
            y_pred1, _ = predictions[name1]
            y_pred2, _ = predictions[name2]
            
            # Create contingency table
            # n00: both wrong, n01: model1 wrong & model2 correct
            # n10: model1 correct & model2 wrong, n11: both correct
            n01 = np.sum((y_pred1 != y_test) & (y_pred2 == y_test))
            n10 = np.sum((y_pred1 == y_test) & (y_pred2 != y_test))
            
            # McNemar's test statistic with continuity correction
            if n01 + n10 > 0:
                mcnemar_stat = ((abs(n10 - n01) - 1) ** 2) / (n10 + n01)
                mcnemar_pvalue = 1 - stats.chi2.cdf(mcnemar_stat, 1)
                
                mcnemar_results.append({
                    'model1': name1,
                    'model2': name2,
                    'n01': n01,
                    'n10': n10,
                    'statistic': mcnemar_stat,
                    'pvalue': mcnemar_pvalue
                })

print("\n  Pairwise McNemar's Test Results:")
print("  " + "-"*70)
for res in mcnemar_results[:6]:  # Show top 6 comparisons
    sig = "***" if res['pvalue'] < 0.001 else "**" if res['pvalue'] < 0.01 else "*" if res['pvalue'] < 0.05 else "ns"
    print(f"  {res['model1'][:25]} vs {res['model2'][:25]}")
    print(f"    œá¬≤={res['statistic']:.3f}, p={res['pvalue']:.4f} {sig} "
          f"(disagreements: {res['n01']}‚Üî{res['n10']})")

# ----------------------------------------------------------------
# E. FRIEDMAN TEST (MULTIPLE MODEL COMPARISON)
# ----------------------------------------------------------------
print("\n[E] Friedman Test (Non-parametric ANOVA for Multiple Models)...")

# Organize scores for Friedman test
friedman_data = []
model_names_friedman = []

for name in models_sorted:
    if name in repeated_cv_results and len(repeated_cv_results[name]['scores']) > 0:
        friedman_data.append(repeated_cv_results[name]['scores'])
        model_names_friedman.append(name)

if len(friedman_data) >= 3:
    friedman_stat, friedman_pvalue = stats.friedmanchisquare(*friedman_data)
    
    print(f"\n  Friedman Test Results:")
    print(f"    Statistic: œá¬≤({len(friedman_data)-1}) = {friedman_stat:.3f}")
    print(f"    p-value: {friedman_pvalue:.4f}")
    
    if friedman_pvalue < 0.05:
        print(f"    ‚úì Significant differences detected between models (p < 0.05)")
        
        # Post-hoc: Nemenyi test (rank-based pairwise comparison)
        print("\n  Post-hoc Analysis (Mean Ranks):")
        ranks = []
        for scores in friedman_data:
            ranks.append(stats.rankdata(scores).mean())
        
        for name, rank in zip(model_names_friedman, ranks):
            print(f"    {name:30s}: Rank = {rank:.2f}")
    else:
        print(f"    No significant differences detected between models (p ‚â• 0.05)")

# ----------------------------------------------------------------
# F. VARIANCE ANALYSIS
# ----------------------------------------------------------------
print("\n[F] Variance and Stability Analysis...")

print("\n  Model Stability (CV Coefficient of Variation):")
print("  " + "-"*70)

stability_results = []
for name in models_sorted:
    if name in repeated_cv_results:
        mean_acc = repeated_cv_results[name]['mean']
        std_acc = repeated_cv_results[name]['std']
        cv_coef = (std_acc / mean_acc * 100) if mean_acc > 0 else 0  # Coefficient of variation
        
        stability_results.append({
            'model': name,
            'cv_coefficient': cv_coef,
            'std': std_acc,
            'stability_score': 1 / (1 + cv_coef)  # Higher is more stable
        })
        
        print(f"  {name:30s}: CV={cv_coef:.2f}%, Std={std_acc:.4f}, "
              f"Stability={1/(1+cv_coef):.4f}")

# ----------------------------------------------------------------
# G. BIAS-VARIANCE TRADEOFF ESTIMATION
# ----------------------------------------------------------------
print("\n[G] Bias-Variance Tradeoff Analysis...")

for name in models_sorted[:3]:  # Top 3 models
    if name in repeated_cv_results:
        train_score = repeated_cv_results[name].get('train_mean', 0)
        test_score = repeated_cv_results[name]['mean']
        
        bias_estimate = 1 - test_score  # Approximation
        variance_estimate = repeated_cv_results[name]['std'] ** 2
        overfitting_gap = train_score - test_score if train_score > 0 else 0
        
        print(f"\n  {name}:")
        print(f"    Train Score: {train_score:.4f}")
        print(f"    Test Score:  {test_score:.4f}")
        print(f"    Overfitting Gap: {overfitting_gap:.4f}")
        print(f"    Bias (approx): {bias_estimate:.4f}")
        print(f"    Variance: {variance_estimate:.6f}")
        print(f"    Assessment: {'Potential Overfitting' if overfitting_gap > 0.02 else 'Well Generalized'}")

# ----------------------------------------------------------------
# H. STATISTICAL SUMMARY TABLE
# ----------------------------------------------------------------
print("\n[H] Comprehensive Statistical Summary Table:")
print("="*80)

summary_df = pd.DataFrame({
    'Model': models_sorted,
    'Mean Acc': [repeated_cv_results[m]['mean'] if m in repeated_cv_results else 0 for m in models_sorted],
    'Std': [repeated_cv_results[m]['std'] if m in repeated_cv_results else 0 for m in models_sorted],
    'Min': [repeated_cv_results[m]['min'] if m in repeated_cv_results else 0 for m in models_sorted],
    'Max': [repeated_cv_results[m]['max'] if m in repeated_cv_results else 0 for m in models_sorted],
    'Median': [repeated_cv_results[m]['median'] if m in repeated_cv_results else 0 for m in models_sorted],
    'CV%': [repeated_cv_results[m]['std']/repeated_cv_results[m]['mean']*100 if m in repeated_cv_results and repeated_cv_results[m]['mean'] > 0 else 0 for m in models_sorted]
})

print(summary_df.to_string(index=False))

# ----------------------------------------------------------------
# I. VISUALIZE STATISTICAL RESULTS
# ----------------------------------------------------------------
print("\n[I] Generating Statistical Visualization Plots...")

# Plot 1: CV Score Distributions (Box Plots)
fig, ax = plt.subplots(figsize=(14, 8), facecolor='white')

cv_scores_list = [repeated_cv_results[m]['scores'] for m in models_sorted if m in repeated_cv_results]
bp = ax.boxplot(cv_scores_list, labels=models_sorted, patch_artist=True,
                notch=True, showmeans=True,
                meanprops=dict(marker='D', markerfacecolor='red', markersize=8),
                medianprops=dict(color='black', linewidth=2))

for patch, color in zip(bp['boxes'], research_colors[:len(models_sorted)]):
    patch.set_facecolor(color)
    patch.set_alpha(0.6)

ax.set_ylabel('Cross-Validation Accuracy', fontsize=12)
ax.set_title('Cross-Validation Score Distributions (5x3 Repeated CV)', fontsize=14)
ax.grid(True, axis='y', alpha=0.3, linestyle='--')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('cv_distributions.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# Plot 2: Statistical Comparison Heatmap
if comparison_results:
    fig, ax = plt.subplots(figsize=(10, 6), facecolor='white')
    
    comparison_matrix = np.zeros((len(comparison_results), 3))
    comparison_labels = []
    
    for i, res in enumerate(comparison_results):
        comparison_matrix[i, 0] = res['mean_diff']
        comparison_matrix[i, 1] = -np.log10(res['t_pvalue']) if res['t_pvalue'] > 0 else 10
        comparison_matrix[i, 2] = res['cohens_d']
        comparison_labels.append(res['model'][:25])
    
    sns.heatmap(comparison_matrix, annot=True, fmt='.3f', 
                xticklabels=['Mean Diff', '-log10(p)', "Cohen's d"],
                yticklabels=comparison_labels, cmap='RdYlGn', center=0,
                cbar_kws={'label': 'Effect Magnitude'}, linewidths=0.5, ax=ax)
    ax.set_title(f'Statistical Comparison vs {baseline_name}', fontsize=14)
    plt.tight_layout()
    plt.savefig('statistical_comparison.png', dpi=600, bbox_inches='tight', facecolor='white')
    plt.show()

# Plot 3: Bias-Variance Visualization
fig, ax = plt.subplots(figsize=(10, 8), facecolor='white')

for i, name in enumerate(models_sorted[:4]):
    if name in repeated_cv_results:
        train_score = repeated_cv_results[name].get('train_mean', 0)
        test_score = repeated_cv_results[name]['mean']
        variance = repeated_cv_results[name]['std']
        
        ax.scatter(1 - test_score, variance, s=300, alpha=0.7,
                  color=research_colors[i % len(research_colors)],
                  edgecolor='black', linewidth=2, label=name)
        
        ax.annotate(name, (1 - test_score, variance), 
                   xytext=(10, 10), textcoords='offset points',
                   fontsize=9,
                   bbox=dict(boxstyle='round,pad=0.5', facecolor='white', 
                            edgecolor='gray', alpha=0.8))

ax.set_xlabel('Bias (1 - Test Accuracy)', fontsize=12)
ax.set_ylabel('Variance (CV Std)', fontsize=12)
ax.set_title('Bias-Variance Tradeoff', fontsize=14)
ax.legend(loc='best', fontsize=9)
ax.grid(True, alpha=0.3, linestyle='--')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('bias_variance_tradeoff.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

print("\n‚úì Statistical analysis complete!")
print("  Additional plots saved:")
print("    - cv_distributions.png")
print("    - statistical_comparison.png")
print("    - bias_variance_tradeoff.png")

# ----------------------------------------------------------------
# J. PRECISION-RECALL CURVES (MULTI-CLASS)
# ----------------------------------------------------------------
print("\n[J] Generating Precision-Recall Curves...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12), facecolor='white')
axes = axes.ravel()
subplot_labels_pr = ['A)', 'B)', 'C)', 'D)']

for idx, (name, color) in enumerate(zip(models_sorted[:4], research_colors)):
    if idx >= 4:
        break
    
    ax = axes[idx]
    
    if name in predictions:
        y_pred, y_proba = predictions[name]
        
        try:
            # Binarize for multi-class PR curves
            y_test_bin = label_binarize(y_test, classes=np.unique(y))
            n_classes = y_test_bin.shape[1]
            
            # Ensure y_proba has correct shape
            y_proba_use = y_proba.copy()
            if y_proba_use.shape[1] != n_classes:
                y_proba_aligned = np.zeros((len(y_test), n_classes))
                model = models[name]
                for i, cls in enumerate(np.unique(y)):
                    if cls in model.classes_:
                        cls_idx = np.where(model.classes_ == cls)[0][0]
                        y_proba_aligned[:, i] = y_proba[:, cls_idx]
                y_proba_use = y_proba_aligned
            
            # Compute PR curve for each class and average
            precision_dict = {}
            recall_dict = {}
            avg_precision = {}
            
            for i in range(n_classes):
                precision_dict[i], recall_dict[i], _ = precision_recall_curve(
                    y_test_bin[:, i], y_proba_use[:, i]
                )
                avg_precision[i] = average_precision_score(y_test_bin[:, i], y_proba_use[:, i])
            
            # Compute micro-average PR curve
            y_test_flat = y_test_bin.ravel()
            y_proba_flat = y_proba_use.ravel()
            min_len = min(len(y_test_flat), len(y_proba_flat))
            precision_micro, recall_micro, _ = precision_recall_curve(
                y_test_flat[:min_len], y_proba_flat[:min_len]
            )
            avg_precision_micro = average_precision_score(y_test_bin, y_proba_use, average='micro')
            
            # Plot micro-average PR curve
            ax.plot(recall_micro, precision_micro, color=color, lw=2.5, alpha=0.85,
                    label=f'Micro-average (AP={avg_precision_micro:.3f})')
            
            # Plot PR curves for first 3 classes
            colors_class = cycle(['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E'])
            for i, c in zip(range(min(3, n_classes)), colors_class):
                ax.plot(recall_dict[i], precision_dict[i], color=c, lw=2, alpha=0.7,
                       label=f'{class_names[i][:15]} (AP={avg_precision[i]:.2f})')
            
            ax.set_xlabel('Recall', fontsize=10)
            ax.set_ylabel('Precision', fontsize=10)
            ax.set_title(f'{subplot_labels_pr[idx]} {name}', fontsize=11, pad=10)
            ax.legend(loc='lower left', fontsize=8, framealpha=0.9)
            ax.grid(True, alpha=0.25, linestyle='--')
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            
            # Add iso-F1 curves
            f_scores = np.linspace(0.2, 0.9, num=5)
            for f_score in f_scores:
                x = np.linspace(0.01, 1)
                y = f_score * x / (2 * x - f_score)
                valid_idx = y >= 0
                ax.plot(x[valid_idx], y[valid_idx], color='gray', alpha=0.15, linestyle=':', lw=1)
                if len(x[valid_idx]) > 40:
                    ax.annotate(f'F1={f_score:.1f}', xy=(0.85, y[40] + 0.02), 
                               fontsize=7, color='gray', alpha=0.6)
            
        except Exception as e:
            print(f"    Error plotting PR curve for {name}: {e}")
            ax.text(0.5, 0.5, f'Error: {str(e)[:80]}', 
                   transform=ax.transAxes, ha='center', va='center', fontsize=9,
                   bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))
            ax.set_title(f'{subplot_labels_pr[idx]} {name}', fontsize=11)

plt.suptitle('Precision-Recall Curves (Multi-Class)', fontsize=14, y=0.995)
plt.tight_layout()
plt.savefig('precision_recall_curves.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# ----------------------------------------------------------------
# K. CUMULATIVE GAIN AND LIFT CHARTS
# ----------------------------------------------------------------
print("\n[K] Generating Cumulative Gain and Lift Charts...")

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8), facecolor='white')

for idx, (name, color) in enumerate(zip(models_sorted[:4], research_colors)):
    if name in predictions:
        y_pred, y_proba = predictions[name]
        
        # Use max probability as "score" for ranking
        y_scores = np.max(y_proba, axis=1)
        
        # Sort by predicted probability (descending)
        sorted_indices = np.argsort(y_scores)[::-1]
        y_test_sorted = y_test.iloc[sorted_indices].values
        y_pred_sorted = y_pred[sorted_indices]
        
        # Calculate cumulative gains
        total_positives = len(y_test)
        cumulative_correct = np.cumsum(y_test_sorted == y_pred_sorted)
        cumulative_samples = np.arange(1, len(y_test_sorted) + 1)
        
        # Cumulative Gain Curve
        gain_x = cumulative_samples / len(y_test_sorted) * 100
        gain_y = cumulative_correct / total_positives * 100
        
        ax1.plot(gain_x, gain_y, color=color, lw=3, alpha=0.8, label=name)
        
        # Lift Curve
        random_lift = cumulative_samples / len(y_test_sorted) * 100
        lift_values = gain_y / (random_lift + 1e-10)
        
        ax2.plot(gain_x, lift_values, color=color, lw=3, alpha=0.8, label=name)

# Cumulative Gain Chart styling
ax1.plot([0, 100], [0, 100], 'k--', lw=2, label='Random Model', alpha=0.5)
ax1.set_xlabel('% of Samples', fontsize=12)
ax1.set_ylabel('% of Correct Predictions', fontsize=12)
ax1.set_title('Cumulative Gain Chart', fontsize=14)
ax1.legend(loc='lower right', fontsize=10)
ax1.grid(True, alpha=0.3, linestyle='--')
ax1.set_xlim([0, 100])
ax1.set_ylim([0, 100])
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)

# Lift Chart styling
ax2.axhline(y=1, color='k', linestyle='--', lw=2, label='Random Model', alpha=0.5)
ax2.set_xlabel('% of Samples', fontsize=12)
ax2.set_ylabel('Lift', fontsize=12)
ax2.set_title('Lift Chart', fontsize=14)
ax2.legend(loc='upper right', fontsize=10)
ax2.grid(True, alpha=0.3, linestyle='--')
ax2.set_xlim([0, 100])
ax2.set_ylim([0, max(3, ax2.get_ylim()[1])])
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig('cumulative_gain_lift.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# ----------------------------------------------------------------
# L. LEARNING CURVES (BIAS-VARIANCE ANALYSIS)
# ----------------------------------------------------------------
print("\n[L] Generating Learning Curves for Bias-Variance Analysis...")

from sklearn.model_selection import learning_curve

fig, axes = plt.subplots(2, 2, figsize=(16, 12), facecolor='white')
axes = axes.ravel()
subplot_labels = ['A)', 'B)', 'C)', 'D)']

for idx, (name, color) in enumerate(zip(models_sorted[:4], research_colors)):
    if idx >= 4:
        break
    
    ax = axes[idx]
    model = models[name]
    
    print(f"  Computing learning curve for {name}...")
    
    try:
        if name == 'Temporal Random Forest' and temporal_data_train is not None:
            # Custom learning curve for temporal models - OPTIMIZED with fewer points
            train_sizes_abs = [100, 400, 800, len(X_train)]
            train_scores_mean = []
            test_scores_mean = []
            train_scores_std = []
            test_scores_std = []
            
            for train_size in train_sizes_abs:
                if train_size > len(X_train):
                    continue
                    
                fold_train_scores = []
                fold_test_scores = []
                
                # Reduced to 2-fold validation for speed
                for fold in range(2):
                    indices = np.random.choice(len(X_train), train_size, replace=False)
                    X_fold = X_train[indices]
                    y_fold = y_train.iloc[indices] if hasattr(y_train, 'iloc') else y_train[indices]
                    temporal_fold = temporal_data_train[indices]
                    
                    model_lc = clone(model)
                    model_lc.fit(X_fold, y_fold, temporal_data=temporal_fold)
                    
                    train_pred = model_lc.predict(X_fold, temporal_data=temporal_fold)
                    test_pred = model_lc.predict(X_test, temporal_data=temporal_data_test)
                    
                    fold_train_scores.append(accuracy_score(y_fold, train_pred))
                    fold_test_scores.append(accuracy_score(y_test, test_pred))
                
                train_scores_mean.append(np.mean(fold_train_scores))
                train_scores_std.append(np.std(fold_train_scores))
                test_scores_mean.append(np.mean(fold_test_scores))
                test_scores_std.append(np.std(fold_test_scores))
            
            train_sizes_abs = train_sizes_abs[:len(train_scores_mean)]
            train_sizes = np.array(train_sizes_abs)
            train_scores_mean = np.array(train_scores_mean)
            train_scores_std = np.array(train_scores_std)
            test_scores_mean = np.array(test_scores_mean)
            test_scores_std = np.array(test_scores_std)
            
        else:
            train_sizes, train_scores, test_scores = learning_curve(
                model, X_train, y_train, cv=2, n_jobs=-1,
                train_sizes=np.linspace(0.2, 1.0, 5),
                scoring='accuracy', shuffle=True, random_state=SEED
            )
            
            train_scores_mean = np.mean(train_scores, axis=1)
            train_scores_std = np.std(train_scores, axis=1)
            test_scores_mean = np.mean(test_scores, axis=1)
            test_scores_std = np.std(test_scores, axis=1)
        
        # Plot learning curves
        ax.plot(train_sizes, train_scores_mean, 'o-', color=color, 
                lw=2.5, alpha=0.85, label='Training score', markersize=6)
        ax.fill_between(train_sizes, 
                        train_scores_mean - train_scores_std,
                        train_scores_mean + train_scores_std, 
                        alpha=0.15, color=color)
        
        ax.plot(train_sizes, test_scores_mean, 's--', color=color, 
                lw=2.5, alpha=0.65, label='Validation score', markersize=6)
        ax.fill_between(train_sizes, 
                        test_scores_mean - test_scores_std,
                        test_scores_mean + test_scores_std, 
                        alpha=0.1, color=color)
        
        # Calculate and display bias-variance indicators
        final_gap = train_scores_mean[-1] - test_scores_mean[-1]
        final_variance = test_scores_std[-1]
        
        ax.set_title(f'{subplot_labels[idx]} {name} (Gap={final_gap:.3f}, Variance={final_variance:.3f})', 
                    fontsize=11, pad=10)
        ax.set_xlabel('Training Set Size', fontsize=10)
        ax.set_ylabel('Accuracy Score', fontsize=10)
        ax.legend(loc='lower right', fontsize=9, framealpha=0.9)
        ax.grid(True, alpha=0.3, linestyle='--')
        ax.set_ylim([0.85, 1.02])
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        
        # Add bias-variance annotation
        bias_level = "High Bias" if test_scores_mean[-1] < 0.95 else "Low Bias"
        variance_level = "High Variance" if final_gap > 0.03 else "Low Variance"
        ax.text(0.05, 0.95, f'{bias_level}\n{variance_level}', 
               transform=ax.transAxes, fontsize=9, 
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.6, edgecolor='gray', linewidth=1),
               verticalalignment='top')
        
    except Exception as e:
        print(f"    Error computing learning curve for {name}: {e}")
        ax.text(0.5, 0.5, f'Error: {str(e)[:80]}', 
               transform=ax.transAxes, ha='center', va='center', fontsize=9,
               bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))
        ax.set_title(f'{subplot_labels[idx]} {name}', fontsize=11)

plt.suptitle('Learning Curves and Bias-Variance Trade-off Analysis', 
            fontsize=14, y=0.995)
plt.tight_layout()
plt.savefig('learning_curves_bias_variance.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# ----------------------------------------------------------------
# M. ROC-PR COMBINED ANALYSIS
# ----------------------------------------------------------------
print("\n[M] Generating Combined ROC-PR Analysis...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12), facecolor='white')
subplot_labels = ['A)', 'B)', 'C)', 'D)']

for idx, (name, color) in enumerate(zip(models_sorted[:4], research_colors)):
    if idx >= 4:
        break
    
    ax = axes[idx // 2, idx % 2]
    
    if name in predictions:
        y_pred, y_proba = predictions[name]
        
        try:
            # Binarize for multi-class
            y_test_bin = label_binarize(y_test, classes=np.unique(y))
            n_classes = y_test_bin.shape[1]
            
            # Ensure y_proba has correct shape
            if y_proba.shape[1] != n_classes:
                print(f"    Warning: Adjusting probability shape for {name}")
                # Align classes
                y_proba_aligned = np.zeros((len(y_test), n_classes))
                model = models[name]
                for i, cls in enumerate(np.unique(y)):
                    if cls in model.classes_:
                        cls_idx = np.where(model.classes_ == cls)[0][0]
                        y_proba_aligned[:, i] = y_proba[:, cls_idx]
                y_proba = y_proba_aligned
            
            # Flatten properly for micro-average
            y_test_flat = y_test_bin.ravel()
            y_proba_flat = y_proba.ravel()
            
            # Ensure same length
            min_len = min(len(y_test_flat), len(y_proba_flat))
            y_test_flat = y_test_flat[:min_len]
            y_proba_flat = y_proba_flat[:min_len]
            
            # Compute micro-average ROC
            fpr_micro, tpr_micro, _ = roc_curve(y_test_flat, y_proba_flat)
            roc_auc_micro = auc(fpr_micro, tpr_micro)
        
            # Compute micro-average PR
            precision_micro, recall_micro, _ = precision_recall_curve(
                y_test_flat, y_proba_flat
            )
            pr_auc_micro = auc(recall_micro, precision_micro)
            
            # Plot ROC on primary axis
            ax_roc = ax
            ax_roc.plot(fpr_micro, tpr_micro, color=color, lw=2.5, 
                       label=f'ROC (AUC={roc_auc_micro:.3f})', alpha=0.85)
            ax_roc.plot([0, 1], [0, 1], 'k--', lw=1.5, alpha=0.3, label='Random')
            
            # Plot PR on secondary axis
            ax_pr = ax_roc.twinx()
            ax_pr.plot(recall_micro, precision_micro, color='darkred', lw=2.5, 
                      linestyle='--', label=f'PR (AUC={pr_auc_micro:.3f})', alpha=0.75)
            
            ax_roc.set_xlabel('False Positive Rate / Recall', fontsize=10)
            ax_roc.set_ylabel('True Positive Rate (ROC)', fontsize=10, color=color)
            ax_pr.set_ylabel('Precision (PR)', fontsize=10, color='darkred')
            ax_roc.set_title(f'{subplot_labels[idx]} {name}', fontsize=11, pad=10)
            
            # Combine legends
            lines1, labels1 = ax_roc.get_legend_handles_labels()
            lines2, labels2 = ax_pr.get_legend_handles_labels()
            ax_roc.legend(lines1 + lines2, labels1 + labels2, loc='center right', fontsize=9, framealpha=0.9)
            
            ax_roc.grid(True, alpha=0.3, linestyle='--')
            ax_roc.set_xlim([0.0, 1.0])
            ax_roc.set_ylim([0.0, 1.05])
            ax_pr.set_ylim([0.0, 1.05])
            ax_roc.spines['top'].set_visible(False)
            ax_roc.tick_params(axis='y', labelcolor=color)
            ax_pr.tick_params(axis='y', labelcolor='darkred')
            
        except Exception as e:
            print(f"    Error plotting ROC-PR for {name}: {e}")
            ax.text(0.5, 0.5, f'Error: {str(e)[:80]}', 
                   transform=ax.transAxes, ha='center', va='center', fontsize=9,
                   bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))
            ax.set_title(f'{subplot_labels[idx]} {name}', fontsize=11)

plt.suptitle('ROC and Precision-Recall Curve Analysis (Micro-Average)', 
            fontsize=14, y=0.995)
plt.tight_layout()
plt.savefig('roc_pr_combined.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

print("\n‚úì Additional advanced plots generated successfully!")
print("  New plots saved:")
print("    - precision_recall_curves.png")
print("    - cumulative_gain_lift.png")
print("    - learning_curves_bias_variance.png")
print("    - roc_pr_combined.png")

# ============================================================
# 5. DETAILED ANALYSIS
# ============================================================

print("\n" + "="*80)
print("[5] DETAILED ANALYSIS")
print("="*80)

# Best model analysis
best_model_name = results_df.index[0]
print(f"\n[5.1] Best Model: {best_model_name}")
print(f"  Test Accuracy: {results_df.loc[best_model_name, 'Test Accuracy']:.4f}")
print(f"  CV Accuracy: {results_df.loc[best_model_name, 'CV Accuracy Mean']:.4f} "
      f"¬± {results_df.loc[best_model_name, 'CV Accuracy Std']:.4f}")
print(f"  F1-Score: {results_df.loc[best_model_name, 'Test F1']:.4f}")
print(f"  Training Time: {results_df.loc[best_model_name, 'Training Time']:.2f}s")

# Get predictions for best model
if best_model_name in predictions:
    y_pred_best, y_proba_best = predictions[best_model_name]
else:
    y_pred_best = None
    y_proba_best = None

# Improvement over baseline
if 'Standard Random Forest' in results_df.index:
    baseline_acc = results_df.loc['Standard Random Forest', 'Test Accuracy']
    best_acc = results_df.loc[best_model_name, 'Test Accuracy']
    improvement = ((best_acc - baseline_acc) / baseline_acc) * 100
    
    print(f"\n[5.2] Improvement over Standard Random Forest:")
    print(f"  Baseline (Standard RF): {baseline_acc:.4f}")
    print(f"  Best Model ({best_model_name}): {best_acc:.4f}")
    print(f"  Improvement: {improvement:+.2f}%")

# Feature importance analysis
print("\n[5.3] Feature Importance Analysis:")
for model_name in models_sorted:
    if model_name in models and hasattr(models[model_name], 'feature_importances_'):
        importances = models[model_name].feature_importances_
        top_3_idx = np.argsort(importances)[-3:][::-1]
        
        # Get feature names if available
        if hasattr(X, 'columns'):
            feature_names = X.columns.tolist()
            top_features = [feature_names[i] for i in top_3_idx]
        else:
            top_features = top_3_idx.tolist()
        
        print(f"  {model_name}:")
        print(f"    Top 3 features: {top_features}")
        print(f"    Importance values: {importances[top_3_idx].round(4)}")

# Note: Confusion matrix plot removed - too cluttered with 19 classes
# Refer to classification report above for per-class performance details
    
# Classification report for best model
if y_pred_best is not None:
    print("\n  Classification Report (Best Model):")
    print(classification_report(y_test, y_pred_best))

# ============================================================
# 5.5. ADVANCED PLOTS - FEATURE IMPORTANCE & MODEL DIAGNOSTICS
# ============================================================

print("\n" + "="*80)
print("[5.5] ADVANCED VISUALIZATION - FEATURE IMPORTANCE & DIAGNOSTICS")
print("="*80)

# ----------------------------------------------------------------
# N. FEATURE IMPORTANCE COMPARISON PLOTS
# ----------------------------------------------------------------
print("\n[N] Generating Feature Importance Comparison Plots...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12), facecolor='white')
axes = axes.ravel()
subplot_labels_fi = ['A)', 'B)', 'C)', 'D)']

# Get feature names
if hasattr(X, 'columns'):
    feature_names = X.columns.tolist()
else:
    feature_names = [f'Feature {i}' for i in range(X_train.shape[1])]

for idx, model_name in enumerate(models_sorted[:4]):
    if idx >= 4:
        break
    
    ax = axes[idx]
    model = models[model_name]
    
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        indices = np.argsort(importances)[-10:]  # Top 10 features
        
        # Plot horizontal bar chart
        ax.barh(range(10), importances[indices], color=research_colors[idx], 
               alpha=0.8, edgecolor='black', linewidth=1)
        ax.set_yticks(range(10))
        ax.set_yticklabels([feature_names[i] for i in indices], fontsize=9)
        ax.set_xlabel('Importance Score', fontsize=10)
        ax.set_title(f'{subplot_labels_fi[idx]} {model_name} - Top 10 Features', 
                    fontsize=11, pad=10)
        ax.grid(True, alpha=0.3, linestyle='--', axis='x')
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        
        # Add values on bars
        for i, (bar, val) in enumerate(zip(ax.patches, importances[indices])):
            ax.text(val, bar.get_y() + bar.get_height()/2, 
                   f'{val:.3f}', ha='left', va='center', fontsize=8, 
                   color='black', fontweight='normal')

plt.suptitle('Feature Importance Analysis - Top 10 Features per Model', fontsize=14, y=0.995)
plt.tight_layout()
plt.savefig('feature_importance_comparison.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# ----------------------------------------------------------------
# O. MODEL DIAGNOSTICS - TREE STATISTICS & OOB ANALYSIS
# ----------------------------------------------------------------
print("\n[O] Generating Model Diagnostics - Tree Statistics...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12), facecolor='white')
subplot_labels_diag = ['A)', 'B)', 'C)', 'D)']

# Subplot A: Number of Trees Performance (for Standard RF)
ax = axes[0, 0]
if 'Standard Random Forest' in models:
    std_rf = models['Standard Random Forest']
    if hasattr(std_rf, 'estimators_'):
        n_trees_range = range(10, len(std_rf.estimators_) + 1, 10)
        train_scores_trees = []
        test_scores_trees = []
        
        for n in n_trees_range:
            # Use subset of trees
            temp_preds_train = np.array([tree.predict(X_train) for tree in std_rf.estimators_[:n]], dtype=int)
            temp_pred_train = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=temp_preds_train)
            train_scores_trees.append(accuracy_score(y_train, temp_pred_train))
            
            temp_preds_test = np.array([tree.predict(X_test) for tree in std_rf.estimators_[:n]], dtype=int)
            temp_pred_test = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=temp_preds_test)
            test_scores_trees.append(accuracy_score(y_test, temp_pred_test))
        
        ax.plot(n_trees_range, train_scores_trees, 'o-', label='Training', 
               color=research_colors[0], lw=2, alpha=0.8)
        ax.plot(n_trees_range, test_scores_trees, 's-', label='Test', 
               color=research_colors[1], lw=2, alpha=0.8)
        ax.set_xlabel('Number of Trees', fontsize=10)
        ax.set_ylabel('Accuracy', fontsize=10)
        ax.set_title(f'{subplot_labels_diag[0]} Accuracy vs Number of Trees', fontsize=11, pad=10)
        ax.legend(fontsize=9, framealpha=0.9)
        ax.grid(True, alpha=0.3, linestyle='--')

# Subplot B: Tree Depth Distribution
ax = axes[0, 1]
if 'Standard Random Forest' in models:
    std_rf = models['Standard Random Forest']
    if hasattr(std_rf, 'estimators_'):
        tree_depths = [tree.tree_.max_depth for tree in std_rf.estimators_]
        ax.hist(tree_depths, bins=20, color=research_colors[1], alpha=0.7, 
               edgecolor='black', linewidth=1)
        ax.axvline(np.mean(tree_depths), color='red', linestyle='--', 
                  linewidth=2, label=f'Mean: {np.mean(tree_depths):.1f}')
        ax.set_xlabel('Tree Depth', fontsize=10)
        ax.set_ylabel('Frequency', fontsize=10)
        ax.set_title(f'{subplot_labels_diag[1]} Tree Depth Distribution', fontsize=11, pad=10)
        ax.legend(fontsize=9, framealpha=0.9)
        ax.grid(True, alpha=0.3, linestyle='--', axis='y')

# Subplot C: Number of Leaves per Tree
ax = axes[1, 0]
if 'Standard Random Forest' in models:
    std_rf = models['Standard Random Forest']
    if hasattr(std_rf, 'estimators_'):
        n_leaves = [tree.tree_.n_leaves for tree in std_rf.estimators_]
        ax.hist(n_leaves, bins=20, color=research_colors[2], alpha=0.7, 
               edgecolor='black', linewidth=1)
        ax.axvline(np.mean(n_leaves), color='red', linestyle='--', 
                  linewidth=2, label=f'Mean: {np.mean(n_leaves):.1f}')
        ax.set_xlabel('Number of Leaves', fontsize=10)
        ax.set_ylabel('Frequency', fontsize=10)
        ax.set_title(f'{subplot_labels_diag[2]} Leaf Node Distribution', fontsize=11, pad=10)
        ax.legend(fontsize=9, framealpha=0.9)
        ax.grid(True, alpha=0.3, linestyle='--', axis='y')

# Subplot D: Prediction Confidence Distribution
ax = axes[1, 1]
for idx, (model_name, color) in enumerate(zip(models_sorted[:4], research_colors)):
    if model_name in predictions:
        _, y_proba = predictions[model_name]
        max_probs = np.max(y_proba, axis=1)
        ax.hist(max_probs, bins=30, alpha=0.5, label=model_name, 
               color=color, edgecolor='black', linewidth=0.5)

ax.set_xlabel('Prediction Confidence', fontsize=10)
ax.set_ylabel('Frequency', fontsize=10)
ax.set_title(f'{subplot_labels_diag[3]} Prediction Confidence Distribution', fontsize=11, pad=10)
ax.legend(fontsize=8, framealpha=0.9)
ax.grid(True, alpha=0.3, linestyle='--', axis='y')

for ax in axes.ravel():
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

plt.suptitle('Model Diagnostics - Tree Statistics & Confidence Analysis', fontsize=14, y=0.995)
plt.tight_layout()
plt.savefig('model_diagnostics_trees.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# ----------------------------------------------------------------
# P. ERROR ANALYSIS & MISCLASSIFICATION PATTERNS
# ----------------------------------------------------------------
print("\n[P] Generating Error Analysis & Misclassification Patterns...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12), facecolor='white')
subplot_labels_err = ['A)', 'B)', 'C)', 'D)']

# Subplot A: Confidence vs Correctness
ax = axes[0, 0]
for idx, (model_name, color) in enumerate(zip(models_sorted[:4], research_colors)):
    if model_name in predictions:
        y_pred, y_proba = predictions[model_name]
        max_probs = np.max(y_proba, axis=1)
        correct = (y_pred == y_test).astype(int)
        
        # Bin by confidence
        bins = np.linspace(0, 1, 11)
        bin_centers = (bins[:-1] + bins[1:]) / 2
        error_rates = []
        for i in range(len(bins)-1):
            mask = (max_probs >= bins[i]) & (max_probs < bins[i+1])
            if mask.sum() > 0:
                error_rates.append(1 - correct[mask].mean())
            else:
                error_rates.append(0)
        
        ax.plot(bin_centers, error_rates, 'o-', label=model_name, 
               color=color, lw=2, alpha=0.8, markersize=6)

ax.set_xlabel('Prediction Confidence', fontsize=10)
ax.set_ylabel('Error Rate', fontsize=10)
ax.set_title(f'{subplot_labels_err[0]} Error Rate by Confidence Level', fontsize=11, pad=10)
ax.legend(fontsize=8, framealpha=0.9, loc='upper right')
ax.grid(True, alpha=0.3, linestyle='--')
ax.set_ylim([0, max(0.5, ax.get_ylim()[1])])

# Subplot B: Class-wise Error Rate
ax = axes[0, 1]
if 'Standard Random Forest' in predictions:
    y_pred, _ = predictions['Standard Random Forest']
    per_class_errors = []
    for class_idx in range(len(np.unique(y_test))):
        mask = y_test == class_idx
        if mask.sum() > 0:
            error_rate = 1 - accuracy_score(y_test[mask], y_pred[mask])
            per_class_errors.append(error_rate)
        else:
            per_class_errors.append(0)
    
    # Show top 10 classes with highest error
    top_error_indices = np.argsort(per_class_errors)[-10:]
    ax.barh(range(10), [per_class_errors[i] for i in top_error_indices],
           color=research_colors[1], alpha=0.7, edgecolor='black', linewidth=1)
    ax.set_yticks(range(10))
    ax.set_yticklabels([class_names[i][:20] for i in top_error_indices], fontsize=9)
    ax.set_xlabel('Error Rate', fontsize=10)
    ax.set_title(f'{subplot_labels_err[1]} Top 10 Classes with Highest Error', fontsize=11, pad=10)
    ax.grid(True, alpha=0.3, linestyle='--', axis='x')

# Subplot C: Margin Distribution (Classification Confidence)
ax = axes[1, 0]
for idx, (model_name, color) in enumerate(zip(models_sorted[:4], research_colors)):
    if model_name in predictions:
        _, y_proba = predictions[model_name]
        # Margin = difference between top 2 probabilities
        sorted_probs = np.sort(y_proba, axis=1)
        margins = sorted_probs[:, -1] - sorted_probs[:, -2]
        
        ax.hist(margins, bins=30, alpha=0.5, label=model_name, 
               color=color, edgecolor='black', linewidth=0.5, density=True)

ax.set_xlabel('Classification Margin', fontsize=10)
ax.set_ylabel('Density', fontsize=10)
ax.set_title(f'{subplot_labels_err[2]} Decision Margin Distribution', fontsize=11, pad=10)
ax.legend(fontsize=8, framealpha=0.9)
ax.grid(True, alpha=0.3, linestyle='--', axis='y')

# Subplot D: Correct vs Incorrect Predictions Confidence
ax = axes[1, 1]
if 'Standard Random Forest' in predictions:
    y_pred, y_proba = predictions['Standard Random Forest']
    max_probs = np.max(y_proba, axis=1)
    correct_mask = (y_pred == y_test)
    
    ax.hist(max_probs[correct_mask], bins=30, alpha=0.6, label='Correct', 
           color='green', edgecolor='black', linewidth=0.5, density=True)
    ax.hist(max_probs[~correct_mask], bins=30, alpha=0.6, label='Incorrect', 
           color='red', edgecolor='black', linewidth=0.5, density=True)
    
    ax.axvline(np.mean(max_probs[correct_mask]), color='darkgreen', 
              linestyle='--', linewidth=2, label=f'Mean Correct: {np.mean(max_probs[correct_mask]):.3f}')
    ax.axvline(np.mean(max_probs[~correct_mask]), color='darkred', 
              linestyle='--', linewidth=2, label=f'Mean Incorrect: {np.mean(max_probs[~correct_mask]):.3f}')

ax.set_xlabel('Prediction Confidence', fontsize=10)
ax.set_ylabel('Density', fontsize=10)
ax.set_title(f'{subplot_labels_err[3]} Confidence: Correct vs Incorrect', fontsize=11, pad=10)
ax.legend(fontsize=8, framealpha=0.9)
ax.grid(True, alpha=0.3, linestyle='--', axis='y')

for ax in axes.ravel():
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

plt.suptitle('Error Analysis and Misclassification Patterns', fontsize=14, y=0.995)
plt.tight_layout()
plt.savefig('error_analysis_patterns.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# ----------------------------------------------------------------
# Q. CLASS PROBABILITY DISTRIBUTION & DATA INSIGHTS
# ----------------------------------------------------------------
print("\n[Q] Generating Class Distribution & Data Insights...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12), facecolor='white')
subplot_labels_cls = ['A)', 'B)', 'C)', 'D)']

# Subplot A: Class Distribution
ax = axes[0, 0]
class_counts = pd.Series(y).value_counts().sort_index()
ax.bar(range(len(class_counts)), class_counts.values, color=research_colors[0], 
      alpha=0.7, edgecolor='black', linewidth=1)
ax.set_xlabel('Class Index', fontsize=10)
ax.set_ylabel('Number of Samples', fontsize=10)
ax.set_title(f'{subplot_labels_cls[0]} Training Data Class Distribution', fontsize=11, pad=10)
ax.grid(True, alpha=0.3, linestyle='--', axis='y')

# Subplot B: Test Set Class Distribution
ax = axes[0, 1]
test_class_counts = pd.Series(y_test).value_counts().sort_index()
ax.bar(range(len(test_class_counts)), test_class_counts.values, color=research_colors[1], 
      alpha=0.7, edgecolor='black', linewidth=1)
ax.set_xlabel('Class Index', fontsize=10)
ax.set_ylabel('Number of Samples', fontsize=10)
ax.set_title(f'{subplot_labels_cls[1]} Test Data Class Distribution', fontsize=11, pad=10)
ax.grid(True, alpha=0.3, linestyle='--', axis='y')

# Subplot C: Model Agreement Matrix
ax = axes[1, 0]
if len(models_sorted) >= 2:
    agreement_matrix = np.zeros((len(models_sorted[:4]), len(models_sorted[:4])))
    for i, model1 in enumerate(models_sorted[:4]):
        for j, model2 in enumerate(models_sorted[:4]):
            if model1 in predictions and model2 in predictions:
                pred1, _ = predictions[model1]
                pred2, _ = predictions[model2]
                agreement = np.mean(pred1 == pred2)
                agreement_matrix[i, j] = agreement
    
    im = ax.imshow(agreement_matrix, cmap='RdYlGn', vmin=0.8, vmax=1.0, aspect='auto')
    ax.set_xticks(range(len(models_sorted[:4])))
    ax.set_yticks(range(len(models_sorted[:4])))
    ax.set_xticklabels([m[:15] for m in models_sorted[:4]], rotation=45, ha='right', fontsize=9)
    ax.set_yticklabels([m[:15] for m in models_sorted[:4]], fontsize=9)
    
    # Add text annotations
    for i in range(len(models_sorted[:4])):
        for j in range(len(models_sorted[:4])):
            text = ax.text(j, i, f'{agreement_matrix[i, j]:.3f}',
                          ha="center", va="center", color="black", fontsize=9)
    
    ax.set_title(f'{subplot_labels_cls[2]} Model Prediction Agreement', fontsize=11, pad=10)
    plt.colorbar(im, ax=ax, label='Agreement Rate')

# Subplot D: Per-Class Accuracy Comparison
ax = axes[1, 1]
if 'Standard Random Forest' in predictions:
    y_pred_best, _ = predictions[best_model_name]
    per_class_acc = []
    class_indices = []
    
    for class_idx in range(len(np.unique(y_test))):
        mask = y_test == class_idx
        if mask.sum() > 0:
            acc = accuracy_score(y_test[mask], y_pred_best[mask])
            per_class_acc.append(acc)
            class_indices.append(class_idx)
    
    # Show bottom 10 classes
    if len(per_class_acc) > 10:
        bottom_indices = np.argsort(per_class_acc)[:10]
        ax.barh(range(10), [per_class_acc[i] for i in bottom_indices],
               color=research_colors[3], alpha=0.7, edgecolor='black', linewidth=1)
        ax.set_yticks(range(10))
        ax.set_yticklabels([class_names[class_indices[i]][:20] for i in bottom_indices], fontsize=9)
    else:
        ax.barh(range(len(per_class_acc)), per_class_acc,
               color=research_colors[3], alpha=0.7, edgecolor='black', linewidth=1)
        ax.set_yticks(range(len(per_class_acc)))
        ax.set_yticklabels([class_names[i][:20] for i in class_indices], fontsize=9)
    
    ax.set_xlabel('Accuracy', fontsize=10)
    ax.set_title(f'{subplot_labels_cls[3]} Worst Performing Classes ({best_model_name[:15]})', 
                fontsize=11, pad=10)
    ax.grid(True, alpha=0.3, linestyle='--', axis='x')
    ax.set_xlim([0, 1])

for ax in axes.ravel():
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

plt.suptitle('Class Distribution and Model Agreement Analysis', fontsize=14, y=0.995)
plt.tight_layout()
plt.savefig('class_distribution_insights.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

print("\n‚úì Advanced visualization plots generated successfully!")
print("  New plots saved:")
print("    - feature_importance_comparison.png")
print("    - model_diagnostics_trees.png")
print("    - error_analysis_patterns.png")
print("    - class_distribution_insights.png")

# ============================================================
# 6. ENSEMBLE OF NOVEL MODELS
# ============================================================

print("\n" + "="*80)
print("[6] ENSEMBLE OF NOVEL MODELS")
print("="*80)

# Create an ensemble of the three novel models
print("\n[6.1] Creating Ensemble of Novel Models...")

# Get predictions from novel models
novel_model_names = ['Cascade Random Forest', 'Hierarchical Random Forest', 'Temporal Random Forest']
novel_predictions = []

for model_name in novel_model_names:
    if model_name in predictions:
        _, y_pred_proba = predictions[model_name]
        novel_predictions.append(y_pred_proba)

if len(novel_predictions) >= 2:
    # Average predictions
    ensemble_proba = np.mean(novel_predictions, axis=0)
    ensemble_pred = np.argmax(ensemble_proba, axis=1)
    
    # Calculate ensemble metrics
    ensemble_accuracy = accuracy_score(y_test, ensemble_pred)
    ensemble_f1 = f1_score(y_test, ensemble_pred, average='weighted', zero_division=0)
    
    print(f"  Ensemble of {len(novel_predictions)} novel models:")
    print(f"    Test Accuracy: {ensemble_accuracy:.4f}")
    print(f"    F1-Score: {ensemble_f1:.4f}")
    
    # Compare with individual models
    print(f"\n  Comparison with individual models:")
    for model_name in novel_model_names:
        if model_name in results_df.index:
            model_acc = results_df.loc[model_name, 'Test Accuracy']
            diff = ensemble_accuracy - model_acc
            print(f"    vs {model_name}: {ensemble_accuracy:.4f} vs {model_acc:.4f} "
                  f"(Œî = {diff:+.4f})")

# ============================================================
# 7. FINAL CONCLUSIONS
# ============================================================

print("\n" + "="*80)
print("[7] FINAL CONCLUSIONS")
print("="*80)

print(f"\nüìä SUMMARY OF NOVEL RANDOM FOREST VARIANTS:")
print(f"   Models tested: {len(models)}")
print(f"   Best performing model: {best_model_name}")
print(f"   Best test accuracy: {results_df['Test Accuracy'].max():.4f}")
print(f"   Most stable model (lowest CV std): "
      f"{results_df['CV Accuracy Std'].idxmin()} "
      f"({results_df['CV Accuracy Std'].min():.4f})")

print(f"\nüîç KEY INSIGHTS:")
print(f"   1. Cascade RF: Focuses on misclassified instances iteratively")
print(f"   2. Hierarchical RF: Creates specialized models for data clusters")
print(f"   3. Temporal RF: Incorporates seasonal/temporal patterns")
print(f"   4. Standard RF: Serves as baseline comparison")

print(f"\nüéØ RECOMMENDATIONS:")
print(f"   1. Use {best_model_name} for deployment")
print(f"   2. Consider ensemble of novel models for robustness")
print(f"   3. Monitor model performance on new data")
print(f"   4. Regularly retrain models with updated data")

print(f"\nüìà PERFORMANCE METRICS:")
for model_name in models_sorted:
    acc = results_df.loc[model_name, 'Test Accuracy']
    cv_mean = results_df.loc[model_name, 'CV Accuracy Mean']
    cv_std = results_df.loc[model_name, 'CV Accuracy Std']
    time_val = results_df.loc[model_name, 'Training Time']
    
    print(f"   {model_name:30s}: "
          f"Test={acc:.4f}, CV={cv_mean:.4f}¬±{cv_std:.4f}, Time={time_val:.1f}s")

print("\n" + "="*80)
print("EXPERIMENT COMPLETE - NOVEL RANDOM FOREST VARIANTS IMPLEMENTED")
print("="*80)


get_ipython().getoutput("zip -r icsasd.zip /kaggle/working")


print("\n[N] Generating Feature Importance Comparison Plots...")

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.cm import inferno

fig, axes = plt.subplots(2, 2, figsize=(16, 12), facecolor='white')
axes = axes.ravel()
subplot_labels_fi = ['A)', 'B)', 'C)', 'D)']

# Inferno-based colors (one per subplot)
cmap = inferno
subplot_colors = cmap(np.linspace(0.25, 0.85, 4))

# Get feature names
if hasattr(X, 'columns'):
    feature_names = X.columns.tolist()
else:
    feature_names = [f'Feature {i}' for i in range(X_train.shape[1])]

for idx, model_name in enumerate(models_sorted[:4]):
    ax = axes[idx]
    model = models[model_name]

    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        indices = np.argsort(importances)[-10:]

        values = importances[indices]
        y_pos = np.arange(len(indices))

        ax.barh(
            y_pos,
            values,
            color=subplot_colors[idx],
            alpha=0.9
        )

        ax.set_yticks(y_pos)
        ax.set_yticklabels(
            [feature_names[i] for i in indices],
            fontsize=9
        )

        ax.set_xlabel('Importance Score', fontsize=10)
        ax.set_title(
            f'{subplot_labels_fi[idx]} {model_name} - Top 10 Features',
            fontsize=11,
            pad=8
        )

        # Subtle grid (x-axis only)
        ax.grid(
            axis='x',
            linestyle=':',
            linewidth=0.8,
            alpha=0.1
        )

        # Clean spines
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['left'].set_alpha(0.4)
        ax.spines['bottom'].set_alpha(0.4)

        # Value annotations
        for y, val in zip(y_pos, values):
            ax.text(
                val + values.max() * 0.01,
                y,
                f'{val:.3f}',
                va='center',
                fontsize=8,
                color='#2f2f2f'
            )

plt.suptitle(
    'Feature Importance Analysis - Top 10 Features per Model',
    fontsize=14,
    y=0.98
)

plt.tight_layout()
plt.savefig(
    'feature_importance_comparison.png',
    dpi=600,
    bbox_inches='tight',
    facecolor='white'
)
plt.show()



print("\n[N] Top-10 Feature Importances per Model\n")

# Get feature names safely
if hasattr(X, 'columns'):
    feature_names = X.columns.tolist()
else:
    feature_names = [f'Feature {i}' for i in range(X_train.shape[1])]

for model_name in models_sorted[:4]:
    model = models[model_name]

    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        indices = np.argsort(importances)[-10:][::-1]  # highest ‚Üí lowest

        print(f"{model_name}")
        print("-" * len(model_name))

        for rank, idx in enumerate(indices, start=1):
            print(
                f"{rank:>2}. {feature_names[idx]:<25} "
                f"Importance = {importances[idx]:.6f}"
            )

        print()



# Visualize results with research-worthy styling
sns.set_style("whitegrid")
research_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E', '#BC4B51']
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.patch.set_facecolor('white')

# 1. Test Accuracy Comparison
ax1 = axes[0, 0]
models_sorted = results_df.index.tolist()
accuracies = results_df['Test Accuracy'].values
bars1 = ax1.barh(models_sorted, accuracies, color=research_colors[:len(models_sorted)], edgecolor='black', linewidth=1.2)
# Add value labels inside bars
for bar, val in zip(bars1, accuracies):
    width = bar.get_width()
    ax1.text(width - 0.005, bar.get_y() + bar.get_height()/2, 
            f'{val:.4f}', ha='right', va='center', 
            fontsize=9, color='white', fontweight='bold')
ax1.set_xlabel('Accuracy', fontsize=12)
ax1.set_title('Test Accuracy Comparison', fontsize=14)
ax1.axvline(x=accuracies.max(), color='#C73E1D', linestyle='--', alpha=0.7, linewidth=2)
ax1.grid(False)
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)

# 2. CV Accuracy with Error Bars
ax2 = axes[0, 1]
cv_means = results_df['CV Accuracy Mean'].values
cv_stds = results_df['CV Accuracy Std'].values
y_pos = np.arange(len(models_sorted))
bars2 = ax2.barh(y_pos, cv_means, xerr=cv_stds, color=research_colors[:len(models_sorted)], 
                 capsize=5, edgecolor='black', linewidth=1.2, ecolor='#333333', error_kw={'linewidth': 2})
# Add value labels inside bars
for bar, val in zip(bars2, cv_means):
    width = bar.get_width()
    ax2.text(width - 0.005, bar.get_y() + bar.get_height()/2, 
            f'{val:.4f}', ha='right', va='center', 
            fontsize=9, color='white', fontweight='bold')
ax2.set_yticks(y_pos)
ax2.set_yticklabels(models_sorted)
ax2.set_xlabel('Accuracy', fontsize=12)
ax2.set_title('Cross-Validation Accuracy (¬± std)', fontsize=14)
ax2.grid(False)
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)

# 3. Training Time Comparison
ax3 = axes[0, 2]
training_times_vals = results_df['Training Time'].values
bars3 = ax3.barh(models_sorted, training_times_vals, color=research_colors[:len(models_sorted)], 
                 edgecolor='black', linewidth=1.2)
# Add value labels inside bars
for bar, val in zip(bars3, training_times_vals):
    width = bar.get_width()
    # Use percentage-based offset to keep text inside bar
    offset = width * 0.08  # 8% from the right edge
    ax3.text(width - offset, bar.get_y() + bar.get_height()/2, 
            f'{val:.2f}s', ha='right', va='center', 
            fontsize=9, color='white', fontweight='bold')
ax3.set_xlabel('Time (seconds)', fontsize=12)
ax3.set_title('Training Time Comparison', fontsize=14)
ax3.grid(False)
ax3.spines['top'].set_visible(False)
ax3.spines['right'].set_visible(False)

# 4. F1-Score Comparison
ax4 = axes[1, 0]
f1_scores = results_df['Test F1'].values
bars4 = ax4.barh(models_sorted, f1_scores, color=research_colors[:len(models_sorted)], 
                 edgecolor='black', linewidth=1.2)
# Add value labels inside bars
for bar, val in zip(bars4, f1_scores):
    width = bar.get_width()
    ax4.text(width - 0.005, bar.get_y() + bar.get_height()/2, 
            f'{val:.4f}', ha='right', va='center', 
            fontsize=9, color='white', fontweight='bold')
ax4.set_xlabel('F1-Score', fontsize=12)
ax4.set_title('Test F1-Score Comparison', fontsize=14)
ax4.axvline(x=f1_scores.max(), color='#C73E1D', linestyle='--', alpha=0.7, linewidth=2)
ax4.grid(False)
ax4.spines['top'].set_visible(False)
ax4.spines['right'].set_visible(False)

# 5. Precision-Recall Tradeoff
ax5 = axes[1, 1]
precision = results_df['Test Precision'].values
recall = results_df['Test Recall'].values
for i, model in enumerate(models_sorted):
    ax5.scatter(precision[i], recall[i], s=200, alpha=0.8, 
                color=research_colors[i % len(research_colors)], 
                edgecolor='black', linewidth=1.5, zorder=3)
    ax5.annotate(model, (precision[i], recall[i]), xytext=(8, 8), 
                 textcoords='offset points', fontsize=9,
                 bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='gray', alpha=0.8))
ax5.set_xlabel('Precision', fontsize=12)
ax5.set_ylabel('Recall', fontsize=12)
ax5.set_title('Precision-Recall Tradeoff', fontsize=14)
ax5.grid(False)
ax5.spines['top'].set_visible(False)
ax5.spines['right'].set_visible(False)

# 6. Accuracy vs Training Time
ax6 = axes[1, 2]
for i, model in enumerate(models_sorted):
    ax6.scatter(training_times_vals[i], accuracies[i], s=200, alpha=0.8,
                color=research_colors[i % len(research_colors)], 
                edgecolor='black', linewidth=1.5, zorder=3)
    ax6.annotate(model, (training_times_vals[i], accuracies[i]), 
                 xytext=(8, 8), textcoords='offset points', fontsize=9,
                 bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='gray', alpha=0.8))
ax6.set_xlabel('Training Time (seconds)', fontsize=12)
ax6.set_ylabel('Test Accuracy', fontsize=12)
ax6.set_title('Accuracy vs Training Time', fontsize=14)
ax6.grid(False)
ax6.spines['top'].set_visible(False)
ax6.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig('model_comparison.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()


# Plot 2: ROC Curves for Multi-Class (One-vs-Rest)
print("  Generating ROC curves...")
fig, axes = plt.subplots(2, 2, figsize=(16, 12), facecolor='white')
axes = axes.ravel()
subplot_labels_roc = ['A)', 'B)', 'C)', 'D)']

# Binarize the labels
y_test_bin = label_binarize(y_test, classes=np.unique(y))
n_classes = y_test_bin.shape[1]

colors = cycle(['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E', '#BC4B51'])

for idx, model_name in enumerate(models_sorted[:4]):  # Plot top 4 models
    if model_name in predictions:
        ax = axes[idx]
        _, y_pred_proba = predictions[model_name]
        
        try:
            # Ensure proba has correct shape
            if y_pred_proba.shape[1] != n_classes:
                # Align classes
                y_proba_aligned = np.zeros((len(y_test), n_classes))
                model = models[model_name]
                for i, cls in enumerate(np.unique(y)):
                    if cls in model.classes_:
                        cls_idx = np.where(model.classes_ == cls)[0][0]
                        y_proba_aligned[:, i] = y_pred_proba[:, cls_idx]
                y_pred_proba = y_proba_aligned
            
            # Compute ROC curve and ROC area for each class
            fpr = dict()
            tpr = dict()
            roc_auc = dict()
            
            for i in range(n_classes):
                fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])
                roc_auc[i] = auc(fpr[i], tpr[i])
            
            # Compute micro-average ROC curve
            y_test_flat = y_test_bin.ravel()
            y_proba_flat = y_pred_proba.ravel()
            min_len = min(len(y_test_flat), len(y_proba_flat))
            fpr["micro"], tpr["micro"], _ = roc_curve(y_test_flat[:min_len], y_proba_flat[:min_len])
            roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
            
            # Plot micro-average ROC curve
            ax.plot(fpr["micro"], tpr["micro"],
                   label=f'Micro-average (AUC = {roc_auc["micro"]:.3f})',
                   color='black', linestyle='--', linewidth=2.5, alpha=0.9)
            
            # Plot ROC curve for each class (show first 10 classes)
            for i, color in zip(range(min(10, n_classes)), colors):
                ax.plot(fpr[i], tpr[i], color=color, lw=2,
                       label=f'{class_names[i][:15]} (AUC = {roc_auc[i]:.2f})', alpha=0.7)
            
            ax.plot([0, 1], [0, 1], 'k:', lw=1.5, alpha=0.4, label='Random')
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate', fontsize=10)
            ax.set_ylabel('True Positive Rate', fontsize=10)
            ax.set_title(f'{subplot_labels_roc[idx]} {model_name}', fontsize=11, pad=10)
            ax.legend(loc="lower right", fontsize=10, framealpha=0.9)
            ax.grid(True, alpha=0.25, linestyle='--')
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            
        except Exception as e:
            print(f"    Error plotting ROC for {model_name}: {e}")
            ax.text(0.5, 0.5, f'Error: {str(e)[:80]}', 
                   transform=ax.transAxes, ha='center', va='center', fontsize=9,
                   bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))
            ax.set_title(f'{subplot_labels_roc[idx]} {model_name}', fontsize=11)

plt.suptitle('Multi-Class ROC Curves (One-vs-Rest)', fontsize=14, y=0.995)
plt.tight_layout()
plt.savefig('roc_curves.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()


import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, confusion_matrix, classification_report)
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted

import joblib
import time
from collections import defaultdict

SEED = 42
np.random.seed(SEED)


print("="*80)
print("CUSTOM RANDOM FOREST VARIANTS")
print("="*80)

class CascadeRandomForest(ClassifierMixin, BaseEstimator):
    
    def __init__(self, n_layers=3, n_estimators_per_layer=50, max_depth=15, min_samples_split=5, random_state=42):
        self.n_layers = n_layers
        self.n_estimators_per_layer = n_estimators_per_layer
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.random_state = random_state
        self.layers = []
        self.feature_importances_ = None
        
    def fit(self, X, y):
        X, y = check_X_y(X, y)
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        
        n_features = X.shape[1]
        self.feature_importances_ = np.zeros(n_features)
        
        print(f"\n[Layer 1] Training Random Forest on all data...")
        rf_layer1 = RandomForestClassifier(
            n_estimators=self.n_estimators_per_layer,
            max_depth=self.max_depth,
            min_samples_split=self.min_samples_split,
            random_state=self.random_state,
            n_jobs=-1
        )
        rf_layer1.fit(X, y)
        self.layers.append(rf_layer1)
        self.feature_importances_ += rf_layer1.feature_importances_
        
        y_pred_layer1 = rf_layer1.predict(X)
        misclassified_mask = (y_pred_layer1 != y)
        
        if misclassified_mask.sum() == 0:
            print("All instances correctly classified in first layer!")
            return self
        
        X_misclassified = X[misclassified_mask]
        y_misclassified = y[misclassified_mask]
        
        print(f"  Misclassified instances: {len(X_misclassified)}/{len(X)} ({len(X_misclassified)/len(X)*100:.1f}%)")
        
        for layer_idx in range(1, self.n_layers):
            if len(X_misclassified) < 10:
                print(f"\nStopping early: Only {len(X_misclassified)} misclassified instances remain")
                break
            
            print(f"\n[Layer {layer_idx+1}] Training Random Forest on misclassified instances...")
            print(f"  Training on {len(X_misclassified)} instances")
            
            rf_layer = RandomForestClassifier(
                n_estimators=self.n_estimators_per_layer,
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                random_state=self.random_state + layer_idx,
                n_jobs=-1
            )
            rf_layer.fit(X_misclassified, y_misclassified)
            self.layers.append(rf_layer)
            self.feature_importances_ += rf_layer.feature_importances_
            
            y_pred_layer = rf_layer.predict(X_misclassified)
            new_misclassified_mask = (y_pred_layer != y_misclassified)
            
            if new_misclassified_mask.sum() == 0:
                print(f"‚úì All remaining instances correctly classified!")
                break
            
            X_misclassified = X_misclassified[new_misclassified_mask]
            y_misclassified = y_misclassified[new_misclassified_mask]
            
            print(f"  Still misclassified: {len(X_misclassified)} instances")
        
        self.feature_importances_ /= len(self.layers)
        return self
    
    def predict_proba(self, X):
        check_is_fitted(self)
        X = check_array(X)
        
        proba = np.zeros((X.shape[0], self.n_classes_))
        total_weight = sum([2.0 ** (len(self.layers) - i - 1) for i in range(len(self.layers))])
        
        for i, layer in enumerate(self.layers):
            layer_weight = (2.0 ** (len(self.layers) - i - 1)) / total_weight
            layer_proba = layer.predict_proba(X)
            
            layer_proba_aligned = np.zeros_like(proba)
            for cls_idx, cls in enumerate(self.classes_):
                if cls in layer.classes_:
                    class_idx_in_layer = np.where(layer.classes_ == cls)[0][0]
                    layer_proba_aligned[:, cls_idx] = layer_proba[:, class_idx_in_layer]
            
            proba += layer_weight * layer_proba_aligned
        
        proba_sum = proba.sum(axis=1, keepdims=True)
        proba_sum[proba_sum == 0] = 1
        return proba / proba_sum
    
    def predict(self, X):
        proba = self.predict_proba(X)
        return self.classes_[np.argmax(proba, axis=1)]

class HierarchicalRandomForest(ClassifierMixin, BaseEstimator):
    
    def __init__(self, n_clusters=3, n_estimators_global=50, n_estimators_local=30, max_depth=12, random_state=42):
        self.n_clusters = n_clusters
        self.n_estimators_global = n_estimators_global
        self.n_estimators_local = n_estimators_local
        self.max_depth = max_depth
        self.random_state = random_state
        self.global_rf = None
        self.cluster_models = {}
        self.kmeans = None
        self.feature_importances_ = None
        
    def fit(self, X, y):
        X, y = check_X_y(X, y)
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        
        print(f"\n[Hierarchical RF] Training with {self.n_clusters} clusters...")
        
        print("  Clustering feature space")
        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=self.random_state, n_init=10)
        clusters = self.kmeans.fit_predict(X)
        print(f"  Cluster distribution: {np.bincount(clusters)}")
        
        print("  Training global Random Forest")
        self.global_rf = RandomForestClassifier(
            n_estimators=self.n_estimators_global,
            max_depth=self.max_depth,
            random_state=self.random_state,
            n_jobs=-1
        )
        self.global_rf.fit(X, y)
        self.feature_importances_ = self.global_rf.feature_importances_.copy()
        
        print("  Training specialized forests per cluster")
        for cluster_id in range(self.n_clusters):
            cluster_mask = (clusters == cluster_id)
            
            if cluster_mask.sum() < 10:
                print(f"    Cluster {cluster_id}: Skipped (only {cluster_mask.sum()} samples)")
                continue
            
            X_cluster = X[cluster_mask]
            y_cluster = y[cluster_mask]
            
            print(f"    Cluster {cluster_id}: Training on {len(X_cluster)} samples ({len(np.unique(y_cluster))} classes)")
            
            cluster_rf = RandomForestClassifier(
                n_estimators=self.n_estimators_local,
                max_depth=self.max_depth,
                random_state=self.random_state + cluster_id,
                n_jobs=-1
            )
            cluster_rf.fit(X_cluster, y_cluster)
            self.cluster_models[cluster_id] = cluster_rf
            self.feature_importances_ += cluster_rf.feature_importances_
        
        self.feature_importances_ /= (1 + len(self.cluster_models))
        return self
    
    def predict_proba(self, X):
        check_is_fitted(self)
        X = check_array(X)
        
        clusters = self.kmeans.predict(X)
        proba = np.zeros((X.shape[0], self.n_classes_))
        
        global_proba = self.global_rf.predict_proba(X)
        global_proba_aligned = np.zeros_like(proba)
        
        for cls_idx, cls in enumerate(self.classes_):
            if cls in self.global_rf.classes_:
                class_idx_in_global = np.where(self.global_rf.classes_ == cls)[0][0]
                global_proba_aligned[:, cls_idx] = global_proba[:, class_idx_in_global]
        
        proba = 0.25 * global_proba_aligned
        
        for cluster_id, cluster_model in self.cluster_models.items():
            cluster_mask = (clusters == cluster_id)
            
            if cluster_mask.sum() == 0:
                continue
            
            X_cluster = X[cluster_mask]
            cluster_proba = cluster_model.predict_proba(X_cluster)
            cluster_proba_aligned = np.zeros((len(X_cluster), self.n_classes_))
            
            for cls_idx, cls in enumerate(self.classes_):
                if cls in cluster_model.classes_:
                    class_idx_in_cluster = np.where(cluster_model.classes_ == cls)[0][0]
                    cluster_proba_aligned[:, cls_idx] = cluster_proba[:, class_idx_in_cluster]
            
            proba[cluster_mask] += 0.75 * cluster_proba_aligned
        
        proba_sum = proba.sum(axis=1, keepdims=True)
        proba_sum[proba_sum == 0] = 1
        return proba / proba_sum
    
    def predict(self, X):
        proba = self.predict_proba(X)
        return self.classes_[np.argmax(proba, axis=1)]

# Load dataset
data = pd.read_csv('/kaggle/input/crop-dataset/crop_dataset.csv')
print(f"Dataset loaded successfully")
print(f"  Shape: {data.shape[0]} rows √ó {data.shape[1]} columns")

# Data preprocessing
print("\nPreprocessing data...")

error_values = ['#DIV/0!', '#N/A', '#VALUE!', '#REF!', '#NAME?', '#NUM!', '#NULL!']
for col in data.columns:
    if data[col].dtype == 'object':
        data[col] = data[col].replace(error_values, np.nan)
        try:
            data[col] = pd.to_numeric(data[col], errors='ignore')
        except:
            pass

data_cleaned = data.dropna()
if len(data) != len(data_cleaned):
    print(f"  Removed {len(data) - len(data_cleaned)} rows with missing values/errors")
    data = data_cleaned

label_encoders = {}
categorical_cols = ['District', 'Season', 'Crop Name', 'Transplant', 'Growth', 'Harvest']

for col in categorical_cols:
    if col in data.columns:
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col])
        label_encoders[col] = le
        print(f"  Encoded '{col}' with {len(le.classes_)} classes")

print("\nFeature engineering...")
if 'Max Temp' in data.columns and 'Min Temp' in data.columns:
    data['Temp_Range'] = data['Max Temp'] - data['Min Temp']
    print("  Added: Temp_Range")

if 'Max Relative Humidity' in data.columns and 'Min Relative Humidity' in data.columns:
    data['Humidity_Range'] = data['Max Relative Humidity'] - data['Min Relative Humidity']
    print("  Added: Humidity_Range")

if 'Avg Temp' in data.columns and 'Avg Humidity' in data.columns:
    data['Temp_Humidity_Index'] = data['Avg Temp'] * data['Avg Humidity'] / 100
    print("  Added: Temp_Humidity_Index")

X = data.drop(['Crop Name'], axis=1)
y = data['Crop Name']

season_data = None
if 'Season' in data.columns:
    season_data = data['Season'].values.copy()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

if season_data is not None:
    X_train, X_test, y_train, y_test, season_train, season_test = train_test_split(
        X_scaled, y, season_data, test_size=0.2, random_state=SEED, stratify=y
    )
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=SEED, stratify=y
    )
    season_train, season_test = None, None

print(f"\nData split:")
print(f"  Training set: {X_train.shape[0]} samples")
print(f"  Test set: {X_test.shape[0]} samples")
print(f"  Features: {X_train.shape[1]}")
print(f"  Classes: {len(np.unique(y))}")

print("\n" + "="*80)
print("TRAINING AND EVALUATING MODELS")
print("="*80)

models = {
    'Standard Random Forest': RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        random_state=SEED,
        n_jobs=-1,
        verbose=0
    ),
    'Cascade Random Forest': CascadeRandomForest(
        n_layers=4,
        n_estimators_per_layer=80,
        max_depth=18,
        random_state=SEED
    ),
    'Hierarchical Random Forest': HierarchicalRandomForest(
        n_clusters=5,
        n_estimators_global=80,
        n_estimators_local=60,
        max_depth=18,
        random_state=SEED
    )
}

temporal_data_train = season_train
temporal_data_test = season_test

results = {}
predictions = {}
training_times = {}

print("\nTraining models...")
print("-" * 60)

for name, model in models.items():
    print(f"\n{'='*50}")
    print(f"Training: {name}")
    print(f"{'='*50}")
    
    start_time = time.time()
    
    try:
        if name == 'Temporal Random Forest' and temporal_data_train is not None:
            model.fit(X_train, y_train, temporal_data=temporal_data_train)
        else:
            model.fit(X_train, y_train)
        
        train_time = time.time() - start_time
        training_times[name] = train_time
        
        if name == 'Temporal Random Forest' and temporal_data_test is not None:
            y_pred = model.predict(X_test, temporal_data=temporal_data_test)
            y_pred_proba = model.predict_proba(X_test, temporal_data=temporal_data_test)
        else:
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)
        
        predictions[name] = (y_pred, y_pred_proba)
        
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        cv_scores = cross_val_score(model, X_scaled, y, cv=3, scoring='accuracy', n_jobs=-1)
        cv_accuracy_mean = cv_scores.mean()
        cv_accuracy_std = cv_scores.std()
        
        results[name] = {
            'Test Accuracy': accuracy,
            'Test Precision': precision,
            'Test Recall': recall,
            'Test F1': f1,
            'CV Accuracy Mean': cv_accuracy_mean,
            'CV Accuracy Std': cv_accuracy_std,
            'Training Time': train_time
        }
        
        print(f"  ‚úì Training completed in {train_time:.2f}s")
        print(f"  Test Accuracy: {accuracy:.4f}")
        print(f"  CV Accuracy: {cv_accuracy_mean:.4f} ¬± {cv_accuracy_std:.4f}")
        
        if hasattr(model, 'feature_importances_'):
            top_features = np.argsort(model.feature_importances_)[-5:][::-1]
            print(f"  Top 5 features by importance: {top_features}")
        
    except Exception as e:
        print(f"  ‚úó Error training {name}: {str(e)}")
        results[name] = {
            'Test Accuracy': 0,
            'Test Precision': 0,
            'Test Recall': 0,
            'Test F1': 0,
            'CV Accuracy Mean': 0,
            'CV Accuracy Std': 0,
            'Training Time': 0
        }

results_df = pd.DataFrame(results).T
results_df = results_df.sort_values('Test Accuracy', ascending=False)

print("\nPERFORMANCE COMPARISON:")
print("-" * 80)
print(results_df.round(4).to_string())

print("\nSUMMARY:")
print("-" * 80)
best_model_name = results_df.index[0]
best_accuracy = results_df.loc[best_model_name, 'Test Accuracy']
print(f"Best Model: {best_model_name}")
print(f"Best Test Accuracy: {best_accuracy:.4f}")

print("\nRANKING BY METRIC:")
print("-" * 80)

for metric in ['Test Accuracy', 'Test F1', 'Training Time']:
    if metric == 'Training Time':
        sorted_df = results_df.sort_values(metric)
        best_val = sorted_df.iloc[0][metric]
        print(f"{metric}:")
        for idx, (model, row) in enumerate(sorted_df.iterrows()):
            print(f"  {idx+1}. {model}: {row[metric]:.4f}")
    else:
        sorted_df = results_df.sort_values(metric, ascending=False)
        best_val = sorted_df.iloc[0][metric]
        print(f"{metric}:")
        for idx, (model, row) in enumerate(sorted_df.iterrows()):
            print(f"  {idx+1}. {model}: {row[metric]:.4f}")
    print()

print("\nExporting models and preprocessing artifacts...")

# Create a dictionary to store all artifacts
model_artifacts = {
    'scaler': scaler,
    'label_encoders': label_encoders,
    'best_model_name': best_model_name,
    'models': {}
}

for name, model in models.items():
    try:
        filename = f"{name.replace(' ', '_').lower()}_model.joblib"
        joblib.dump(model, filename)
        model_artifacts['models'][name] = filename
        print(f"‚úì Exported {name} to {filename}")
    except Exception as e:
        print(f"‚úó Error exporting {name}: {str(e)}")

joblib.dump(scaler, 'scaler.joblib')
joblib.dump(label_encoders, 'label_encoders.joblib')

config = {
    'feature_columns': list(X.columns) if hasattr(X, 'columns') else list(range(X.shape[1])),
    'target_column': 'Crop Name',
    'categorical_columns': categorical_cols,
    'model_files': model_artifacts['models']
}

joblib.dump(config, 'model_config.joblib')

print("\n" + "="*80)
print("RECOMMENDATIONS")

print(f"Primary Model for Deployment: {best_model_name}")
print(f"   ‚Ä¢ Accuracy: {results_df.loc[best_model_name, 'Test Accuracy']:.4f}")
print(f"   ‚Ä¢ F1-Score: {results_df.loc[best_model_name, 'Test F1']:.4f}")

print(f"\nAlternative Models (if primary fails):")
for model_name in results_df.index:
    if model_name != best_model_name:
        acc = results_df.loc[model_name, 'Test Accuracy']
        f1 = results_df.loc[model_name, 'Test F1']
        print(f"   ‚Ä¢ {model_name}: Accuracy={acc:.4f}, F1={f1:.4f}")

print(f"\nFiles available for application use:")
for name, filename in model_artifacts['models'].items():
    print(f"   ‚Ä¢ {filename} - {name}")

print("\n" + "="*80)
print("MODELS TRAINED AND EXPORTED SUCCESSFULLY")


get_ipython().getoutput("zip -r exported_files.zip /kaggle/working")
