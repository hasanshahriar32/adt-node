{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T21:40:33.072842Z",
     "iopub.status.busy": "2025-12-16T21:40:33.072415Z",
     "iopub.status.idle": "2025-12-16T21:40:41.950180Z",
     "shell.execute_reply": "2025-12-16T21:40:41.949218Z",
     "shell.execute_reply.started": "2025-12-16T21:40:33.072812Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CUSTOM RANDOM FOREST VARIANTS\n",
      "================================================================================\n",
      "Dataset loaded successfully\n",
      "  Shape: 4608 rows × 15 columns\n",
      "\n",
      "Preprocessing data...\n",
      "  Removed 418 rows with missing values/errors\n",
      "  Encoded 'District' with 64 classes\n",
      "  Encoded 'Season' with 3 classes\n",
      "  Encoded 'Crop Name' with 72 classes\n",
      "  Encoded 'Transplant' with 19 classes\n",
      "  Encoded 'Growth' with 32 classes\n",
      "  Encoded 'Harvest' with 34 classes\n",
      "\n",
      "Feature engineering...\n",
      "  Added: Temp_Range\n",
      "  Added: Humidity_Range\n",
      "  Added: Temp_Humidity_Index\n",
      "\n",
      "Data split:\n",
      "  Training set: 3352 samples\n",
      "  Test set: 838 samples\n",
      "  Features: 17\n",
      "  Classes: 72\n",
      "\n",
      "================================================================================\n",
      "TRAINING AND EVALUATING MODELS\n",
      "================================================================================\n",
      "\n",
      "Training models...\n",
      "------------------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Training: Standard Random Forest\n",
      "==================================================\n",
      "  ✓ Training completed in 0.49s\n",
      "  Test Accuracy: 0.9642\n",
      "  CV Accuracy: 0.9654 ± 0.0035\n",
      "  Top 5 features by importance: [ 8  7 16  4  6]\n",
      "\n",
      "==================================================\n",
      "Training: Cascade Random Forest\n",
      "==================================================\n",
      "\n",
      "[Layer 1] Training Random Forest on all data...\n",
      "  Misclassified instances: 30/3352 (0.9%)\n",
      "\n",
      "[Layer 2] Training Random Forest on misclassified instances...\n",
      "  Training on 30 instances\n",
      "  Still misclassified: 4 instances\n",
      "\n",
      "Stopping early: Only 4 misclassified instances remain\n",
      "  ✓ Training completed in 0.84s\n",
      "  Test Accuracy: 0.9666\n",
      "  CV Accuracy: 0.9613 ± 0.0058\n",
      "  Top 5 features by importance: [ 8  6 11 10  9]\n",
      "\n",
      "==================================================\n",
      "Training: Hierarchical Random Forest\n",
      "==================================================\n",
      "\n",
      "[Hierarchical RF] Training with 5 clusters...\n",
      "  Clustering feature space\n",
      "  Cluster distribution: [1651  590  859   89  163]\n",
      "  Training global Random Forest\n",
      "  Training specialized forests per cluster\n",
      "    Cluster 0: Training on 1651 samples (36 classes)\n",
      "    Cluster 1: Training on 590 samples (14 classes)\n",
      "    Cluster 2: Training on 859 samples (18 classes)\n",
      "    Cluster 3: Training on 89 samples (6 classes)\n",
      "    Cluster 4: Training on 163 samples (4 classes)\n",
      "  ✓ Training completed in 1.57s\n",
      "  Test Accuracy: 0.9594\n",
      "  CV Accuracy: 0.9640 ± 0.0051\n",
      "  Top 5 features by importance: [ 8  7  6 16 12]\n",
      "\n",
      "PERFORMANCE COMPARISON:\n",
      "--------------------------------------------------------------------------------\n",
      "                            Test Accuracy  Test Precision  Test Recall  Test F1  CV Accuracy Mean  CV Accuracy Std  Training Time\n",
      "Cascade Random Forest              0.9666          0.9676       0.9666   0.9665            0.9613           0.0058         0.8390\n",
      "Standard Random Forest             0.9642          0.9649       0.9642   0.9641            0.9654           0.0035         0.4909\n",
      "Hierarchical Random Forest         0.9594          0.9610       0.9594   0.9591            0.9640           0.0051         1.5660\n",
      "\n",
      "SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "Best Model: Cascade Random Forest\n",
      "Best Test Accuracy: 0.9666\n",
      "\n",
      "RANKING BY METRIC:\n",
      "--------------------------------------------------------------------------------\n",
      "Test Accuracy:\n",
      "  1. Cascade Random Forest: 0.9666\n",
      "  2. Standard Random Forest: 0.9642\n",
      "  3. Hierarchical Random Forest: 0.9594\n",
      "\n",
      "Test F1:\n",
      "  1. Cascade Random Forest: 0.9665\n",
      "  2. Standard Random Forest: 0.9641\n",
      "  3. Hierarchical Random Forest: 0.9591\n",
      "\n",
      "Training Time:\n",
      "  1. Standard Random Forest: 0.4909\n",
      "  2. Cascade Random Forest: 0.8390\n",
      "  3. Hierarchical Random Forest: 1.5660\n",
      "\n",
      "\n",
      "Exporting models and preprocessing artifacts...\n",
      "✓ Exported Standard Random Forest to standard_random_forest_model.joblib\n",
      "✓ Exported Cascade Random Forest to cascade_random_forest_model.joblib\n",
      "✓ Exported Hierarchical Random Forest to hierarchical_random_forest_model.joblib\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATIONS\n",
      "Primary Model for Deployment: Cascade Random Forest\n",
      "   • Accuracy: 0.9666\n",
      "   • F1-Score: 0.9665\n",
      "\n",
      "Alternative Models (if primary fails):\n",
      "   • Standard Random Forest: Accuracy=0.9642, F1=0.9641\n",
      "   • Hierarchical Random Forest: Accuracy=0.9594, F1=0.9591\n",
      "\n",
      "Files available for application use:\n",
      "   • standard_random_forest_model.joblib - Standard Random Forest\n",
      "   • cascade_random_forest_model.joblib - Cascade Random Forest\n",
      "   • hierarchical_random_forest_model.joblib - Hierarchical Random Forest\n",
      "\n",
      "================================================================================\n",
      "MODELS TRAINED AND EXPORTED SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "import joblib\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CUSTOM RANDOM FOREST VARIANTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class CascadeRandomForest(ClassifierMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_layers=3, n_estimators_per_layer=50, max_depth=15, min_samples_split=5, random_state=42):\n",
    "        self.n_layers = n_layers\n",
    "        self.n_estimators_per_layer = n_estimators_per_layer\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.random_state = random_state\n",
    "        self.layers = []\n",
    "        self.feature_importances_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        \n",
    "        n_features = X.shape[1]\n",
    "        self.feature_importances_ = np.zeros(n_features)\n",
    "        \n",
    "        print(f\"\\n[Layer 1] Training Random Forest on all data...\")\n",
    "        rf_layer1 = RandomForestClassifier(\n",
    "            n_estimators=self.n_estimators_per_layer,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf_layer1.fit(X, y)\n",
    "        self.layers.append(rf_layer1)\n",
    "        self.feature_importances_ += rf_layer1.feature_importances_\n",
    "        \n",
    "        y_pred_layer1 = rf_layer1.predict(X)\n",
    "        misclassified_mask = (y_pred_layer1 != y)\n",
    "        \n",
    "        if misclassified_mask.sum() == 0:\n",
    "            print(\"All instances correctly classified in first layer!\")\n",
    "            return self\n",
    "        \n",
    "        X_misclassified = X[misclassified_mask]\n",
    "        y_misclassified = y[misclassified_mask]\n",
    "        \n",
    "        print(f\"  Misclassified instances: {len(X_misclassified)}/{len(X)} ({len(X_misclassified)/len(X)*100:.1f}%)\")\n",
    "        \n",
    "        for layer_idx in range(1, self.n_layers):\n",
    "            if len(X_misclassified) < 10:\n",
    "                print(f\"\\nStopping early: Only {len(X_misclassified)} misclassified instances remain\")\n",
    "                break\n",
    "            \n",
    "            print(f\"\\n[Layer {layer_idx+1}] Training Random Forest on misclassified instances...\")\n",
    "            print(f\"  Training on {len(X_misclassified)} instances\")\n",
    "            \n",
    "            rf_layer = RandomForestClassifier(\n",
    "                n_estimators=self.n_estimators_per_layer,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                random_state=self.random_state + layer_idx,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            rf_layer.fit(X_misclassified, y_misclassified)\n",
    "            self.layers.append(rf_layer)\n",
    "            self.feature_importances_ += rf_layer.feature_importances_\n",
    "            \n",
    "            y_pred_layer = rf_layer.predict(X_misclassified)\n",
    "            new_misclassified_mask = (y_pred_layer != y_misclassified)\n",
    "            \n",
    "            if new_misclassified_mask.sum() == 0:\n",
    "                print(f\"✓ All remaining instances correctly classified!\")\n",
    "                break\n",
    "            \n",
    "            X_misclassified = X_misclassified[new_misclassified_mask]\n",
    "            y_misclassified = y_misclassified[new_misclassified_mask]\n",
    "            \n",
    "            print(f\"  Still misclassified: {len(X_misclassified)} instances\")\n",
    "        \n",
    "        self.feature_importances_ /= len(self.layers)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        \n",
    "        proba = np.zeros((X.shape[0], self.n_classes_))\n",
    "        total_weight = sum([2.0 ** (len(self.layers) - i - 1) for i in range(len(self.layers))])\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer_weight = (2.0 ** (len(self.layers) - i - 1)) / total_weight\n",
    "            layer_proba = layer.predict_proba(X)\n",
    "            \n",
    "            layer_proba_aligned = np.zeros_like(proba)\n",
    "            for cls_idx, cls in enumerate(self.classes_):\n",
    "                if cls in layer.classes_:\n",
    "                    class_idx_in_layer = np.where(layer.classes_ == cls)[0][0]\n",
    "                    layer_proba_aligned[:, cls_idx] = layer_proba[:, class_idx_in_layer]\n",
    "            \n",
    "            proba += layer_weight * layer_proba_aligned\n",
    "        \n",
    "        proba_sum = proba.sum(axis=1, keepdims=True)\n",
    "        proba_sum[proba_sum == 0] = 1\n",
    "        return proba / proba_sum\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(proba, axis=1)]\n",
    "\n",
    "class HierarchicalRandomForest(ClassifierMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_clusters=3, n_estimators_global=50, n_estimators_local=30, max_depth=12, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_estimators_global = n_estimators_global\n",
    "        self.n_estimators_local = n_estimators_local\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.global_rf = None\n",
    "        self.cluster_models = {}\n",
    "        self.kmeans = None\n",
    "        self.feature_importances_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        \n",
    "        print(f\"\\n[Hierarchical RF] Training with {self.n_clusters} clusters...\")\n",
    "        \n",
    "        print(\"  Clustering feature space\")\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=self.random_state, n_init=10)\n",
    "        clusters = self.kmeans.fit_predict(X)\n",
    "        print(f\"  Cluster distribution: {np.bincount(clusters)}\")\n",
    "        \n",
    "        print(\"  Training global Random Forest\")\n",
    "        self.global_rf = RandomForestClassifier(\n",
    "            n_estimators=self.n_estimators_global,\n",
    "            max_depth=self.max_depth,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.global_rf.fit(X, y)\n",
    "        self.feature_importances_ = self.global_rf.feature_importances_.copy()\n",
    "        \n",
    "        print(\"  Training specialized forests per cluster\")\n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            cluster_mask = (clusters == cluster_id)\n",
    "            \n",
    "            if cluster_mask.sum() < 10:\n",
    "                print(f\"    Cluster {cluster_id}: Skipped (only {cluster_mask.sum()} samples)\")\n",
    "                continue\n",
    "            \n",
    "            X_cluster = X[cluster_mask]\n",
    "            y_cluster = y[cluster_mask]\n",
    "            \n",
    "            print(f\"    Cluster {cluster_id}: Training on {len(X_cluster)} samples ({len(np.unique(y_cluster))} classes)\")\n",
    "            \n",
    "            cluster_rf = RandomForestClassifier(\n",
    "                n_estimators=self.n_estimators_local,\n",
    "                max_depth=self.max_depth,\n",
    "                random_state=self.random_state + cluster_id,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            cluster_rf.fit(X_cluster, y_cluster)\n",
    "            self.cluster_models[cluster_id] = cluster_rf\n",
    "            self.feature_importances_ += cluster_rf.feature_importances_\n",
    "        \n",
    "        self.feature_importances_ /= (1 + len(self.cluster_models))\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        \n",
    "        clusters = self.kmeans.predict(X)\n",
    "        proba = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        global_proba = self.global_rf.predict_proba(X)\n",
    "        global_proba_aligned = np.zeros_like(proba)\n",
    "        \n",
    "        for cls_idx, cls in enumerate(self.classes_):\n",
    "            if cls in self.global_rf.classes_:\n",
    "                class_idx_in_global = np.where(self.global_rf.classes_ == cls)[0][0]\n",
    "                global_proba_aligned[:, cls_idx] = global_proba[:, class_idx_in_global]\n",
    "        \n",
    "        proba = 0.25 * global_proba_aligned\n",
    "        \n",
    "        for cluster_id, cluster_model in self.cluster_models.items():\n",
    "            cluster_mask = (clusters == cluster_id)\n",
    "            \n",
    "            if cluster_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            X_cluster = X[cluster_mask]\n",
    "            cluster_proba = cluster_model.predict_proba(X_cluster)\n",
    "            cluster_proba_aligned = np.zeros((len(X_cluster), self.n_classes_))\n",
    "            \n",
    "            for cls_idx, cls in enumerate(self.classes_):\n",
    "                if cls in cluster_model.classes_:\n",
    "                    class_idx_in_cluster = np.where(cluster_model.classes_ == cls)[0][0]\n",
    "                    cluster_proba_aligned[:, cls_idx] = cluster_proba[:, class_idx_in_cluster]\n",
    "            \n",
    "            proba[cluster_mask] += 0.75 * cluster_proba_aligned\n",
    "        \n",
    "        proba_sum = proba.sum(axis=1, keepdims=True)\n",
    "        proba_sum[proba_sum == 0] = 1\n",
    "        return proba / proba_sum\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(proba, axis=1)]\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('/kaggle/input/crop-dataset/crop_dataset.csv')\n",
    "print(f\"Dataset loaded successfully\")\n",
    "print(f\"  Shape: {data.shape[0]} rows × {data.shape[1]} columns\")\n",
    "\n",
    "# Data preprocessing\n",
    "print(\"\\nPreprocessing data...\")\n",
    "\n",
    "error_values = ['#DIV/0!', '#N/A', '#VALUE!', '#REF!', '#NAME?', '#NUM!', '#NULL!']\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object':\n",
    "        data[col] = data[col].replace(error_values, np.nan)\n",
    "        try:\n",
    "            data[col] = pd.to_numeric(data[col], errors='ignore')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "data_cleaned = data.dropna()\n",
    "if len(data) != len(data_cleaned):\n",
    "    print(f\"  Removed {len(data) - len(data_cleaned)} rows with missing values/errors\")\n",
    "    data = data_cleaned\n",
    "\n",
    "label_encoders = {}\n",
    "categorical_cols = ['District', 'Season', 'Crop Name', 'Transplant', 'Growth', 'Harvest']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in data.columns:\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "        label_encoders[col] = le\n",
    "        print(f\"  Encoded '{col}' with {len(le.classes_)} classes\")\n",
    "\n",
    "print(\"\\nFeature engineering...\")\n",
    "if 'Max Temp' in data.columns and 'Min Temp' in data.columns:\n",
    "    data['Temp_Range'] = data['Max Temp'] - data['Min Temp']\n",
    "    print(\"  Added: Temp_Range\")\n",
    "\n",
    "if 'Max Relative Humidity' in data.columns and 'Min Relative Humidity' in data.columns:\n",
    "    data['Humidity_Range'] = data['Max Relative Humidity'] - data['Min Relative Humidity']\n",
    "    print(\"  Added: Humidity_Range\")\n",
    "\n",
    "if 'Avg Temp' in data.columns and 'Avg Humidity' in data.columns:\n",
    "    data['Temp_Humidity_Index'] = data['Avg Temp'] * data['Avg Humidity'] / 100\n",
    "    print(\"  Added: Temp_Humidity_Index\")\n",
    "\n",
    "X = data.drop(['Crop Name'], axis=1)\n",
    "y = data['Crop Name']\n",
    "\n",
    "season_data = None\n",
    "if 'Season' in data.columns:\n",
    "    season_data = data['Season'].values.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "if season_data is not None:\n",
    "    X_train, X_test, y_train, y_test, season_train, season_test = train_test_split(\n",
    "        X_scaled, y, season_data, test_size=0.2, random_state=SEED, stratify=y\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=SEED, stratify=y\n",
    "    )\n",
    "    season_train, season_test = None, None\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Classes: {len(np.unique(y))}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING AND EVALUATING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = {\n",
    "    'Standard Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    ),\n",
    "    'Cascade Random Forest': CascadeRandomForest(\n",
    "        n_layers=4,\n",
    "        n_estimators_per_layer=80,\n",
    "        max_depth=18,\n",
    "        random_state=SEED\n",
    "    ),\n",
    "    'Hierarchical Random Forest': HierarchicalRandomForest(\n",
    "        n_clusters=5,\n",
    "        n_estimators_global=80,\n",
    "        n_estimators_local=60,\n",
    "        max_depth=18,\n",
    "        random_state=SEED\n",
    "    )\n",
    "}\n",
    "\n",
    "temporal_data_train = season_train\n",
    "temporal_data_test = season_test\n",
    "\n",
    "results = {}\n",
    "predictions = {}\n",
    "training_times = {}\n",
    "\n",
    "print(\"\\nTraining models...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if name == 'Temporal Random Forest' and temporal_data_train is not None:\n",
    "            model.fit(X_train, y_train, temporal_data=temporal_data_train)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "        train_time = time.time() - start_time\n",
    "        training_times[name] = train_time\n",
    "        \n",
    "        if name == 'Temporal Random Forest' and temporal_data_test is not None:\n",
    "            y_pred = model.predict(X_test, temporal_data=temporal_data_test)\n",
    "            y_pred_proba = model.predict_proba(X_test, temporal_data=temporal_data_test)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        predictions[name] = (y_pred, y_pred_proba)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        cv_scores = cross_val_score(model, X_scaled, y, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "        cv_accuracy_mean = cv_scores.mean()\n",
    "        cv_accuracy_std = cv_scores.std()\n",
    "        \n",
    "        results[name] = {\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Precision': precision,\n",
    "            'Test Recall': recall,\n",
    "            'Test F1': f1,\n",
    "            'CV Accuracy Mean': cv_accuracy_mean,\n",
    "            'CV Accuracy Std': cv_accuracy_std,\n",
    "            'Training Time': train_time\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ Training completed in {train_time:.2f}s\")\n",
    "        print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  CV Accuracy: {cv_accuracy_mean:.4f} ± {cv_accuracy_std:.4f}\")\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            top_features = np.argsort(model.feature_importances_)[-5:][::-1]\n",
    "            print(f\"  Top 5 features by importance: {top_features}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error training {name}: {str(e)}\")\n",
    "        results[name] = {\n",
    "            'Test Accuracy': 0,\n",
    "            'Test Precision': 0,\n",
    "            'Test Recall': 0,\n",
    "            'Test F1': 0,\n",
    "            'CV Accuracy Mean': 0,\n",
    "            'CV Accuracy Std': 0,\n",
    "            'Training Time': 0\n",
    "        }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nPERFORMANCE COMPARISON:\")\n",
    "print(\"-\" * 80)\n",
    "print(results_df.round(4).to_string())\n",
    "\n",
    "print(\"\\nSUMMARY:\")\n",
    "print(\"-\" * 80)\n",
    "best_model_name = results_df.index[0]\n",
    "best_accuracy = results_df.loc[best_model_name, 'Test Accuracy']\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best Test Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nRANKING BY METRIC:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for metric in ['Test Accuracy', 'Test F1', 'Training Time']:\n",
    "    if metric == 'Training Time':\n",
    "        sorted_df = results_df.sort_values(metric)\n",
    "        best_val = sorted_df.iloc[0][metric]\n",
    "        print(f\"{metric}:\")\n",
    "        for idx, (model, row) in enumerate(sorted_df.iterrows()):\n",
    "            print(f\"  {idx+1}. {model}: {row[metric]:.4f}\")\n",
    "    else:\n",
    "        sorted_df = results_df.sort_values(metric, ascending=False)\n",
    "        best_val = sorted_df.iloc[0][metric]\n",
    "        print(f\"{metric}:\")\n",
    "        for idx, (model, row) in enumerate(sorted_df.iterrows()):\n",
    "            print(f\"  {idx+1}. {model}: {row[metric]:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nExporting models and preprocessing artifacts...\")\n",
    "\n",
    "# Create a dictionary to store all artifacts\n",
    "model_artifacts = {\n",
    "    'scaler': scaler,\n",
    "    'label_encoders': label_encoders,\n",
    "    'best_model_name': best_model_name,\n",
    "    'models': {}\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        filename = f\"{name.replace(' ', '_').lower()}_model.joblib\"\n",
    "        joblib.dump(model, filename)\n",
    "        model_artifacts['models'][name] = filename\n",
    "        print(f\"✓ Exported {name} to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error exporting {name}: {str(e)}\")\n",
    "\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "joblib.dump(label_encoders, 'label_encoders.joblib')\n",
    "\n",
    "config = {\n",
    "    'feature_columns': list(X.columns) if hasattr(X, 'columns') else list(range(X.shape[1])),\n",
    "    'target_column': 'Crop Name',\n",
    "    'categorical_columns': categorical_cols,\n",
    "    'model_files': model_artifacts['models']\n",
    "}\n",
    "\n",
    "joblib.dump(config, 'model_config.joblib')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "\n",
    "print(f\"Primary Model for Deployment: {best_model_name}\")\n",
    "print(f\"   • Accuracy: {results_df.loc[best_model_name, 'Test Accuracy']:.4f}\")\n",
    "print(f\"   • F1-Score: {results_df.loc[best_model_name, 'Test F1']:.4f}\")\n",
    "\n",
    "print(f\"\\nAlternative Models (if primary fails):\")\n",
    "for model_name in results_df.index:\n",
    "    if model_name != best_model_name:\n",
    "        acc = results_df.loc[model_name, 'Test Accuracy']\n",
    "        f1 = results_df.loc[model_name, 'Test F1']\n",
    "        print(f\"   • {model_name}: Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "print(f\"\\nFiles available for application use:\")\n",
    "for name, filename in model_artifacts['models'].items():\n",
    "    print(f\"   • {filename} - {name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELS TRAINED AND EXPORTED SUCCESSFULLY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T21:41:19.348767Z",
     "iopub.status.busy": "2025-12-16T21:41:19.348390Z",
     "iopub.status.idle": "2025-12-16T21:41:20.156193Z",
     "shell.execute_reply": "2025-12-16T21:41:20.154917Z",
     "shell.execute_reply.started": "2025-12-16T21:41:19.348735Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/ (stored 0%)\n",
      "  adding: kaggle/working/cascade_random_forest_model.joblib (deflated 96%)\n",
      "  adding: kaggle/working/.virtual_documents/ (stored 0%)\n",
      "  adding: kaggle/working/.virtual_documents/__notebook_source__.ipynb (deflated 79%)\n",
      "  adding: kaggle/working/standard_random_forest_model.joblib (deflated 96%)\n",
      "  adding: kaggle/working/scaler.joblib (deflated 28%)\n",
      "  adding: kaggle/working/label_encoders.joblib (deflated 58%)\n",
      "  adding: kaggle/working/model_config.joblib (deflated 49%)\n",
      "  adding: kaggle/working/hierarchical_random_forest_model.joblib (deflated 95%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r exported_files.zip /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9005372,
     "sourceId": 14132591,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
